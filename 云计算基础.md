# 云计算基础

## 封面

![cover](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/cover.jpg)

## 内容简介

本书是国内销量最大、被众多高校采用的教材《云计算》的最新升级版，是中国云计算专家咨询委员会秘书长刘鹏教授团队的心血之作。在应对大数据挑战的过程中，云计算技术日趋成熟，拥有大量的成功商业应用。本书追踪最新技术，相比第二版更新了60%以上的内容，包括大数据与云计算、Google云计算、Amazon云计算、微软云计算、Hadoop 2.0及其生态圈、虚拟化技术、OpenStack开源云计算、云计算数据中心、云计算核心算法和中国云计算技术等。刘鹏教授创办的中国云计算（chinacloud.cn）、中国大数据（thebigdata.cn）网站和刘鹏微信公众号（lpoutlook）为本书学习提供技术支撑。

“让学习变得轻松”是本书的初衷。通过本书可掌握云计算的概念和原理，学习主要的云计算平台和技术，还可了解云计算核心算法和发展趋势。本书适合作为相关专业本科和研究生教材，也可作为云计算研发人员和爱好者的学习和参考资料。

未经许可，不得以任何方式复制或抄袭本书之部分或全部内容。

版权所有，侵权必究。

## 图书在版编目（CIP）数据

云计算／刘鹏主编．—3版．—北京：电子工业出版社，2015.8

ISBN 978-7-121-26386-6

Ⅰ.①云… Ⅱ.①刘… Ⅲ.①计算机网络 Ⅳ.①TP393

中国版本图书馆CIP数据核字（2015）第137325号

责任编辑：董亚峰 特约编辑：王纲

印刷：

装订：

出版发行：电子工业出版社

北京市海淀区万寿路173信箱 邮编 100036

开本：787×1092 1/16 印张：26.75 字数：688千字

版次：2010年3月第1版

2015年8月第3版

印次：2015年8月第1次印刷

定价：59.00元

凡所购买电子工业出版社图书有缺损问题，请向购买书店调换。若书店售缺，请与本社发行部联系，联系及邮购电话：（010）88254888。

质量投诉请发邮件至zlts@phei.com.cn，盗版侵权举报请发邮件至dbqq@phei.com.cn。

服务热线：（010）88258888。

**编** **写** **组**

**主** **编** ：刘鹏

**副** **主** **编** ：陈卫卫

**编** **委** ：叶晓江 慈祥 任桐炜 李志刚 鲍爱华 唐艳琴 付印金 吴海佳 李涛 余俊 王真 张晓燕 沈大为 杨震宇 张海天 宋春博 王磊

## 第三版前言

《云计算》第一版于2010年3月出版，第二版于2011年5月出版。时隔四年，在读者的翘首以待中，最受欢迎的云计算教材，终于出第三版啦！

有趣的是，2010年初，我曾经发表了《云计算开启潘多拉星球时代》一文，对2015年的云计算发展进行了预测，我们来看看准不准。

“云计算的影响将是深远的，它将彻底改变IT产业的架构和运行方式。作者在此（2010年1月）做出大胆预测，请广大读者在5年后（2015年1月）回过头来检验这些预测的正确性：在短期之内，高性能计算机、高端服务器、高端存储器、高端处理器的市场的增长率将进入拐点，这些高端硬件市场将被数量众多、低成本、低能耗、高性价比的云计算硬件市场所挤占。紧接着，成本远高于云计算的传统数据中心（IDC），将因为其过高的硬件、网络、管理和能耗成本，以及过低的资源利用率，而迅速被云计算数据中心取代，已建的数以万计的数据中心将被迫转换成云计算运行模式。很快地，绝大多数软件将以服务方式呈现，用户通过浏览器访问，数据都存储在‘云’中。甚至连大多数游戏都将在‘云’里运行，用户终端只负责玩家输入和影音输出。在不远的将来，会出现‘泛云计算化’的现象，呼叫中心、网络会议中心、智能监控中心、数据交换中心、视频监控中心、销售管理中心等，将越来越向某些超大型专业运营商集中而获取高得多的性价比。”

可以说，云计算发展到今天，与预测的结果非常吻合！现在几乎是一切皆云了！

我们唯一能做的就是跟上变化。学习最新的云计算知识，应对大数据挑战，武装自己，迎接未来！

一些同志参加了《云计算》第二版的编写工作，第三版部分地继承了他们的成果，在此记载他们的贡献。他们是：朱军、田浪军、程浩、张洁、张贞、李浩、邓鹏、刘楠、张建平、邓谦、魏家宾、王昊、李松、马少兵、冯颖聪、陈秋晓、傅雷扬等。在此一并致谢！

作者时间充分，但水平有限，欢迎大家不吝赐教。可以通过“刘鹏看未来”（lpoutlook）微信公众号或刘鹏的邮箱gloud@126.com与刘鹏取得联系。

刘鹏 教授

2015年7月1日

## 第二版前言

《云计算》第一版于2010年3月出版。承蒙大家的喜爱，一年中印刷了4次，在当当网云计算书籍中销量保持领先。由于云计算技术发展迅猛，我们的云计算研发团队封闭数月，紧密跟踪，及时推出了第二版。新版《云计算》增加了40%内容，并对原有内容进行全面改写或扩充，以确保能更准确地反映云计算技术的最新面貌。

为了使第二版能够更好满足大家的需要，本书在改版时先进行了读者调查。调查结果显示出大家已经普遍跨越了概念理解阶段，而对云计算的动手实践环节和核心技术原理有着迫切的需求。因此，本书强化了Hadoop、Eucalyptus、CloudSim等动手性强的内容，充实了Google、Amazon、微软云计算原理，增补了VMware虚拟化技术，还同步更新了对云计算理论研究热点的综述。

一些同志参加了《云计算》第一版的编写工作，第二版内容部分地继承了他们的成果。由于编写组署名空间的限制，只好在此记载他们的贡献。他们是：文艾、罗太鹏、龚传、薛志强、朱扬平、王晓璇、王晓盈、鲍爱华、伊英杰、吕良干、周游等。

虽然云计算起步于企业界，但在发展过程中有许多挑战性的技术问题需要解决，希望学术界与企业界密切协作，共同迎接挑战。本着这个思想，我们团队与华为、中兴通讯、360安全卫士、华胜天成、天威视讯、世纪鼎利等知名企业建立了紧密的联合研究关系，研究内容紧跟市场需求和技术发展，研究成果能够迅速转化成生产力。在这本书里，我们将和大家分享其中一些研究成果。

解放军理工大学 刘鹏

2011年5月18日

## 第一版前言

随着网络带宽的不断增长，通过网络访问非本地的计算服务（包括数据处理、存储和信息服务等）的条件越来越成熟，于是就有了今天我们称作“云计算”的技术。之所以称作“云”，是因为计算设施不在本地而在网络中，用户不需要关心它们所处的具体位置，于是我们就像以前画网络图那样，用“一朵云”来代替了。其实，云计算模式的形成由来已久（Google公司从诞生之初就采用了这种模式），但只有当宽带网普及到一定程度，且网格计算、虚拟化、SOA和容错技术等成熟到一定程度并融为一体，又有业界主要大公司的全力推动和吸引人的成功应用案例时，它才如同一颗新星闪亮登场。

既然云计算的服务设施不受用户端的局限，就意味着它们的规模和能力不可限量。Google、亚马逊、微软和IBM等的云计算平台已经达到几十万乃至上百万台计算机的规模。由于规模经济性和众多新技术的运用，加之拥有很高的资源利用率，云计算的性能价格比较之传统模式可以达到惊人的30倍以上——这使得云计算成为一种划时代的技术。

云计算与当今同样备受关注的3G和物联网是什么关系呢？是互为支撑、交相辉映的关系。3G为云计算带来数以亿计的宽带移动用户。移动终端的计算能力和存储空间有限，却有很强的联网能力，如果有云计算平台的支撑，移动用户将获得前所未有的服务体验；物联网使用数量惊人的传感器、RFID和视频监控单元等，采集到极其海量的数据，通过3G和宽带互联网进行传输，如果汇聚到云计算设施进行存储和处理，则可以更加迅速、准确、智能、低成本地对物理世界进行管理和控制，大幅提高社会生产力水平和生活质量。

云计算的影响将是深远的，它将彻底改变IT产业的架构和运行方式。可以预见，高性能计算机、高端服务器、高端存储器和高端处理器的市场将被数量众多、低成本、低能耗和高性价比的云计算硬件市场所挤占；传统互联网数据中心（IDC）将迅速被成本低一个数量级的云计算数据中心所取代；绝大多数软件将以服务方式呈现，甚至连大多数游戏都将在“云”里运行；呼叫中心、网络会议中心、智能监控中心、数据交换中心、视频监控中心和销售管理中心等，将越来越向某些云计算设施集中而获取高得多的性价比。放眼远眺，云计算将与网格计算融为一体，实现云计算平台之间的互操作和资源共享，实现紧耦合高性能科学计算与松耦合高吞吐量商业计算的融合，使互联网上的主要计算设施融为一个有机整体——作者称之为云格（Gloud，即Grid+Cloud）。

因为云计算如此重要，与云计算相关的书籍应运而生。但由于云计算技术起源于企业界而非学术界，各种技术文献很难寻获，目前还未见到对云计算技术进行全面、深入剖析的教科书式出版物。本书编写团队核心成员自2000年起就从事网格计算研发，并一直紧跟国际形势从事云计算领域研发，运营了中国网格（<http://www.chinagrid.net>）和中国云计算（<http://www.chinacloud.cn>）网站，并承担了知名企业的云计算技术培训工作。我们能够感受到广大读者渴望弄清云计算技术本质和细节的迫切心情，集中力量编写了这本书，希望有所裨益。

本书适合不同层次的读者阅读。根据作者的经验，读一本书，面面俱到的方法不可取——耗时过长、印象不深。建议读者带着自己的疑问，寻找感兴趣的阅读点，直奔主题而去：希望了解云计算的概念、本质和发展趋势的读者，可以重点阅读第1、11章；希望学习云计算技术原理的读者，可以将重点放在第2、3、4、5章；希望动手从事云计算开发工作的读者，可重点阅读第6、7、8章；希望从事云计算理论研究的学术界同仁，可重点阅读第9、10章。

此书非常适合作为高校教材使用。建议高校为高年级本科生和研究生开设《云计算》课程。目前解放军理工大学、南京大学等多所高校已经为本科生、研究生开设了《云计算》课程。本课程教学时数建议为60学时，其中实验教学占10～20学时为宜。建议各位老师在中国云计算网站上共享自己的教案和课件，争取依靠大家的共同努力把它做成精品课程。

感谢中国云计算专家委员会主任委员李德毅院士和林润华秘书长对我们云计算研究工作的指导和鼓励。感谢在我攻读硕、博士学位期间，我的导师谢希仁教授和李三立院士分别在计算机网络和网格计算方向对我的悉心指导。

由于云计算技术较为前沿，加之作者水平有限、时间较紧，书中难免存在谬误，恳请读者批评指正。意见和建议请发到gloud@126.com。欢迎在本书配套网站中国云计算（http：//www.chinacloud.cn）上获取更多资料，并交流与云计算相关的任何问题。我们将密切跟踪云计算技术的发展，吸收您的意见，适时编撰本书的升级版本。

解放军理工大学 刘鹏

2010年3月1日

## 第一章 大数据与云计算

图灵奖获得者杰姆·格雷（Jim Gray）曾提出著名的“新摩尔定律”：每18个月全球新增信息量是计算机有史以来全部信息量的总和。时至今日，所累积的数据量之大，已经无法用传统方法处理，因而使“大数据”这个词备受万众瞩目。而处理“大数据”的技术手段——“云计算”——早就于几年前被人们所熟知了。那么，大数据到底怎么形成的？大数据与云计算到底是什么关系？云计算到底是什么？云计算有什么样的优势？本章将沿着这个线索展开。

### 1.1 大数据时代

我们先来看看百度关于“大数据”（Big Data）的搜索指数，如图1-1所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/18_1.jpg)

图1-1 “大数据”的搜索指数

（数据来源：百度指数©baidu）

可以看出，“大数据”这个词是从 2012 年才引起关注的，之后搜索量便迅猛增长。为什么大数据这么受关注？看看图1-2 就明白了。2004年，全球数据总量是30 EB[1]。随后，2005年达到了50 EB，2006年达到了161 EB。到2015年，居然达到了惊人的7900 EB。到2020年，将达到35000 EB。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/19_1.jpg)

图1-2 全球数据总量

为什么全球数据量增长如此之快？一方面是由于数据产生方式的改变。历史上，数据基本上是通过手工产生的。随着人类步入信息社会，数据产生越来越自动化。比如在精细农业中，需要采集植物生长环境的温度、湿度、病虫害信息，对植物的生长进行精细的控制。因此我们在植物的生长环境中安装各种各样的传感器，自动地收集我们需要的信息。对环境的感知，是一种抽样的手段，抽样密度越高，越逼近真实情形。如今，人类不再满足于得到部分信息，而是倾向于收集对象的全量信息，即将我们周围的一切数据化。因为有些数据如果丢失了哪怕很小一部分，都有可能得出错误的结论，比如通过分析人的基因组判断某人可能患有某种疾病，即使丢失一小块基因片段，都有可能导致错误的结论。为了达到这个目的，传感器的使用量暴增。目前全球有30亿～50亿个传感器，到2020年将达到1000亿个之多。这些传感器24小时都在产生数据，这就导致了信息爆炸。

另一方面，人类的活动越来越依赖数据。一是人类的日常生活已经与数据密不可分。全球已经有大约30亿人连入互联网。在Web 2.0时代，每个人不仅是信息的接受者，同时也是信息的产生者，每个人都成为数据源，每个人都在用智能终端拍照、拍录像、发微博、发微信等。全球每天会有2.88万小时的视频上传到Youtube，会有5000万条信息上传到Twitter，会在亚马逊产生630万笔订单……。二是科学研究进入了“数据科学”时代。例如，在物理学领域，欧洲粒子物理研究所的大型强子对撞机，每秒产生的原始数据量高达40 TB。在天文学领域，2000年斯隆数字巡天项目启动时，位于墨西哥州的望远镜在短短几周内收集到的数据比天文学历史上的总和还要多。三是各行各业也越来越依赖大数据手段来开展工作。例如，石油部门用地震勘探的方法来探测地质构造、寻找石油，使用了大量传感器来采集地震波形数据。高铁的运行要保障安全，需要在每一段铁轨周边大量部署传感器，从而感知异物、滑坡、水淹、变形、地震等异常。在智慧城市建设中，包括平安城市、智能交通、智慧环保和智能家居等，都会产生大量的数据。目前一个普通城市的摄像头往往就有几十万个之多，每分每秒都在产生极其海量的数据。

那么，何谓大数据？参考维基百科，本书给出的定义如下：海量数据或巨量数据，其规模巨大到无法通过目前主流的计算机系统在合理时间内获取、存储、管理、处理并提炼以帮助使用者决策。

目前工业界普遍认为大数据具有4V+1C的特征。

（1）数据量大（Volume）：存储的数据量巨大，PB级别是常态，因而对其分析的计算量也大。

（2）多样（Variety）：数据的来源及格式多样，数据格式除了传统的结构化数据外，还包括半结构化或非结构化数据，比如用户上传的音频和视频内容。而随着人类活动的进一步拓宽，数据的来源更加多样。

（3）快速（Velocity）：数据增长速度快，而且越新的数据价值越大，这就要求对数据的处理速度也要快，以便能够从数据中及时地提取知识，发现价值。

（4）价值密度低（Value）：需要对大量的数据进行处理，挖掘其潜在的价值，因而，大数据对我们提出的明确要求是设计一种在成本可接受的条件下，通过快速采集、发现和分析，从大量、多种类别的数据中提取价值的体系架构。

（5）复杂度（Complexity）：对数据的处理和分析的难度大。

### 1.2 云计算——大数据的云计算

在中国大数据专家委员会成立大会上，委员会主任怀进鹏院士用一个公式描述了大数据与云计算的关系：*G*=*f*（*x*）。*x*是大数据，*f*是云计算，*G*是我们的目标。也就是说，云计算是处理大数据的手段，大数据与云计算是一杖硬币的正反面。大数据是需求，云计算是手段。没有大数据，就不需要云计算。没有云计算，就无法处理大数据。

事实上，云计算（Cloud Computing）比大数据“成名”要早。2006年8月9日，谷歌首席执行官埃里克·施密特在搜索引擎大会上首次提出了云计算的概念，并说谷歌自1998年创办以来，就一直采用这种新型的计算方式。

那么，什么是云计算？刘鹏教授对云计算给出了长、短两种定义。长定义是：“云计算是一种商业计算模型。它将计算任务分布在大量计算机构成的资源池上，使各种应用系统能够根据需要获取计算力、存储空间和信息服务。”短定义是：“云计算是通过网络按需提供可动态伸缩的廉价计算服务。[1] [2]”

这种资源池称为“云”。“云”是一些可以自我维护和管理的虚拟计算资源，通常是一些大型服务器集群，包括计算服务器、存储服务器和宽带资源等。云计算将计算资源集中起来，并通过专门软件实现自动管理，无须人为参与。用户可以动态申请部分资源，支持各种应用程序的运转，无须为烦琐的细节而烦恼，能够更加专注于自己的业务，有利于提高效率、降低成本和技术创新。云计算的核心理念是资源池，这与早在2002年就提出的网格计算池（Computing Pool）的概念非常相似[3] [4]。网格计算池将计算和存储资源虚拟成为一个可以任意组合分配的集合，池的规模可以动态扩展，分配给用户的处理能力可以动态回收重用。这种模式能够大大提高资源的利用率，提升平台的服务质量。

之所以称为“云”，是因为它在某些方面具有现实中云的特征：云一般都较大；云的规模可以动态伸缩，它的边界是模糊的；云在空中飘忽不定，无法也无须确定它的具体位置，但它确实存在于某处。之所以称为“云”，还因为云计算的鼻祖之一亚马逊公司将大家曾经称为网格计算的东西，取了一个新名称“弹性计算云”（Elastic Computing Cloud），并取得了商业上的成功。

有人将这种模式比喻为从单台发电机供电模式转向了电厂集中供电的模式。它意味着计算能力也可以作为一种商品进行流通，就像煤气、水和电一样，取用方便，费用低廉。最大的不同在于，它是通过互联网进行传输的。

云计算是并行计算（Parallel Computing）、分布式计算（Distributed Computing）和网格计算（Grid Computing）[2]的发展，或者说是这些计算科学概念的商业实现。云计算是虚拟化（Virtualization）、效用计算（Utility Computing）、将基础设施作为服务IaaS（Infrastructure as a Service）、将平台作为服务PaaS（Platform as a Service）和将软件作为服务SaaS（Software as a Service）等概念混合演进并跃升的结果。

从研究现状上看，云计算具有以下特点。

（1）超大规模。“云”具有相当的规模，谷歌云计算已经拥上百万台服务器，亚马逊、IBM、微软、Yahoo、阿里、百度和腾讯等公司的“云”均拥有几十万台服务器。“云”能赋予用户前所未有的计算能力。

（2）虚拟化。云计算支持用户在任意位置、使用各种终端获取服务。所请求的资源来自“云”，而不是固定的有形的实体。应用在“云”中某处运行，但实际上用户无须了解应用运行的具体位置，只需要一台计算机、PAD或手机，就可以通过网络服务来获取各种能力超强的服务。

（3）高可靠性。“云”使用了数据多副本容错、计算节点同构可互换等措施来保障服务的高可靠性，使用云计算比使用本地计算机更加可靠。

（4）通用性。云计算不针对特定的应用，在“云”的支撑下可以构造出千变万化的应用，同一片“云”可以同时支撑不同的应用运行。

（5）高可伸缩性。“云”的规模可以动态伸缩，满足应用和用户规模增长的需要。

（6）按需服务。“云”是一个庞大的资源池，用户按需购买，像自来水、电和煤气那样计费。

（7）极其廉价。“云”的特殊容错措施使得可以采用极其廉价的节点来构成云；“云”的自动化管理使数据中心管理成本大幅降低；“云”的公用性和通用性使资源的利用率大幅提升；“云”设施可以建在电力资源丰富的地区，从而大幅降低能源成本。因此“云”具有前所未有的性能价格比。

云计算按照服务类型大致可以分为三类：将基础设施作为服务（IaaS）、将平台作为服务（PaaS）和将软件作为服务（SaaS），如图1-3所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/22_1.jpg)

图1-3 云计算的服务类型

IaaS将硬件设备等基础资源封装成服务供用户使用，如亚马逊云计算AWS（Amazon Web Services）的弹性计算云EC2和简单存储服务S3。在IaaS环境中，用户相当于在使用裸机和磁盘，既可以让它运行Windows，也可以让它运行Linux，因而几乎可以做任何想做的事情，但用户必须考虑如何才能让多台机器协同工作。AWS提供了在节点之间互通消息的接口简单队列服务SQS（Simple Queue Service）。IaaS最大的优势在于它允许用户动态申请或释放节点，按使用量计费。运行IaaS的服务器规模达到几十万台之多，用户因而可以认为能够申请的资源几乎是无限的。同时，IaaS是由公众共享的，因而具有更高的资源使用效率。

PaaS对资源的抽象层次更进一步，它提供用户应用程序的运行环境，典型的如Google App Engine。微软的云计算操作系统Microsoft Windows Azure也可大致归入这一类。PaaS自身负责资源的动态扩展和容错管理，用户应用程序不必过多考虑节点间的配合问题。但与此同时，用户的自主权降低，必须使用特定的编程环境并遵照特定的编程模型。这有点像在高性能集群计算机里进行MPI编程，只适用于解决某些特定的计算问题。例如，Google App Engine只允许使用Python和Java语言、基于称为Django的Web应用框架、调用Google App Engine SDK来开发在线应用服务。

SaaS的针对性更强，它将某些特定应用软件功能封装成服务，如Salesforce公司提供的在线客户关系管理CRM（Client Relationship Management）服务。SaaS既不像PaaS一样提供计算或存储资源类型的服务，也不像IaaS一样提供运行用户自定义应用程序的环境，它只提供某些专门用途的服务供应用调用。

需要指出的是，随着云计算的深化发展，不同云计算解决方案之间相互渗透融合，同一种产品往往横跨两种以上类型。例如，Amazon Web Services是以IaaS发展的，但新提供的弹性MapReduce服务模仿了Google的MapReduce，简单数据库服务SimpleDB 模仿了Google Bigtable，这两者属于PaaS的范畴，而它新提供的电子商务服务FPS和DevPay以及网站访问统计服务Alexa Web服务，则属于SaaS的范畴。

在这里，还需要阐述一下云安全与云计算的关系。作为云计算技术的一个分支，云安全技术通过大量客户端的参与来采集异常代码（病毒和木马等），并汇总到云计算平台上进行大规模统计分析，从而准确识别和过滤有害代码。这种技术由中国率先提出，并取得了巨大成功，自此计算机的安全问题得到有效控制，大家才告别了被病毒木马搞得焦头烂额的日子。360安全卫士、瑞星、趋势、卡巴斯基、McAfee、Symantec、江民、Panda、金山等均推出了云安全解决方案。值得一提的是，云安全的核心思想，与刘鹏教授早在2003年提出的反垃圾邮件网格[5]完全一致。该技术被IEEE Cluster 2003国际会议评为杰出网格项目，在香港的现场演示非常轰动，并被国内代表性的电子邮件服务商大规模采用，从而使我国的垃圾邮件过滤水平居于世界领先水平。

### 1.3 云计算发展现状

由于云计算是多种技术混合演进的结果，其成熟度较高，又有大公司推动，发展极为迅速。谷歌、亚马逊和微软等大公司是云计算的先行者。云计算领域的众多成功公司还包括VMware、Salesforce、Facebook、YouTube、MySpace等。最近这几年的一个显著的变化，是以阿里云、云创存储等为代表的中国云计算的迅速崛起。

亚马逊的云计算称为Amazon Web Services（AWS），它率先在全球提供了弹性计算云EC2（Elastic Computing Cloud）和简单存储服务S3（Simple Storage Service），为企业提供计算和存储服务。收费的服务项目包括存储空间、带宽、CPU资源以及月租费。月租费与电话月租费类似，存储空间、带宽按容量收费，CPU根据运算量时长收费。目前，AWS服务的种类非常齐全，包括计算服务、存储与内容传输服务、数据库服务、联网服务、管理和安全服务、分析服务、应用程序服务、部署与管理服务、移动服务和企业应用程序服务等。亚马逊披露，其全球用户数量已经超过100万。

谷歌是最大的云计算技术的使用者。谷歌搜索引擎就建立在分布在200多个站点、超过100万台的服务器的支撑之上，而且这些设施的数量正在迅猛增长。谷歌的一系列成功应用平台，包括谷歌地球、地图、Gmail、Docs等也同样使用了这些基础设施。采用Google Docs之类的应用，用户数据会保存在互联网上的某个位置，可以通过任何一个与互联网相连的终端十分便利地访问和共享这些数据。目前，谷歌已经允许第三方在谷歌的云计算中通过Google App Engine运行大型并行应用程序。谷歌值得称颂的是它不保守，它早已以发表学术论文的形式公开其云计算三大法宝：GFS、MapReduce和Bigtable，并在美国、中国等高校开设如何进行云计算编程的课程。相应地，模仿者应运而生，Hadoop是其中最受关注的开源项目。

微软紧跟云计算步伐，于2008年10月推出了Windows Azure操作系统。Azure（译为“蓝天”）是继Windows取代DOS之后，微软的又一次颠覆性转型——通过在互联网架构上打造新云计算平台，让Windows真正由PC延伸到“蓝天”上。Azure的底层是微软全球基础服务系统，由遍布全球的第四代数据中心构成。目前，微软的云平台包括几十万台服务器。微软将Windows Azure定位为平台服务：一套全面的开发工具、服务和管理系统。它可以让开发者致力于开发可用和可扩展的应用程序。微软将为Windows Azure用户推出许多新的功能，不但能更简单地将现有的应用程序转移到云中，而且可以加强云托管应用程序的可用服务，充分体现出微软的“云”+“端”战略。在中国，微软2014年3月27日宣布由世纪互联负责运营的Microsoft Azure公有云服务正式商用，这是国内首个正式商用的国际公有云服务平台。

近几年，中国云计算的崛起是一道亮丽的风景线。阿里巴巴已经在北京、杭州、青岛、香港、深圳、硅谷等拥有云计算数据中心，并正在德国、新加坡和日本建设数据中心。阿里云提供云服务器ECS、关系型数据库服务RDS、开放存储服务OSS、内容分发网络CDN等产品服务。其用户规模已经超过140万，处于全球领先的位置，并开始在欧美市场与亚马逊等正面竞争。此外，国内代表性的公有云平台还有以游戏托管为特色的UCloud、以存储服务为特色的七牛和提供类似AWS服务的青云，以及专门支撑智能硬件大数据免费托管的万物云（wanwuyun.com）。不仅如此，中国的云计算产品公司也异军突起。中国云计算创新基地理事长单位云创存储（cstor.cn）是国际上云计算产品线最全的企业，拥有自主知识产权的cStor云存储、cProc云处理、cVideo云视频、cTrans云传输等产品线，依靠大幅的技术创新而获得独到的优势。值得一提的是，一些学术团体为推动我国云计算发展做出了不可磨灭的贡献。中国电子学会云计算专家委员会已经成功举办七届中国云计算大会。此外，代表性机构还有中国云计算专家咨询委员会、中国信息协会大数据分会、中国大数据专家委员会、中国计算机学会大数据专家委员会等。

### 1.4 云计算实现机制

由于云计算分为IaaS、PaaS和SaaS三种类型，不同的厂家又提供了不同的解决方案，目前还没有一个统一的技术体系结构，对读者了解云计算的原理构成了障碍。为此，本书综合不同厂家的方案，构造了一个供读者参考的云计算体系结构。这个体系结构如图1-4所示，它概括了不同解决方案的主要特征，每一种方案或许只实现其中部分功能，或许也还有部分相对次要功能尚未概括进来。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/24_1.jpg)

图1-4 云计算技术体系结构

云计算技术体系结构分为四层：物理资源层、资源池层、管理中间件层和SOA（Service-Oriented Architecture，面向服务的体系结构）构建层。物理资源层包括计算机、存储器、网络设施、数据库和软件等。资源池层是将大量相同类型的资源构成同构或接近同构的资源池，如计算资源池、数据资源池等。构建资源池更多的是物理资源的集成和管理工作，例如研究在一个标准集装箱的空间如何装下2000个服务器、解决散热和故障节点替换的问题并降低能耗。管理中间件层负责对云计算的资源进行管理，并对众多应用任务进行调度，使资源能够高效、安全地为应用提供服务。SOA构建层将云计算能力封装成标准的Web Services服务，并纳入SOA体系进行管理和使用，包括服务接口、服务注册、服务查找、服务访问和服务工作流等。管理中间件层和资源池层是云计算技术的最关键部分，SOA构建层的功能更多依靠外部设施提供。

云计算的管理中间件层负责资源管理、任务管理、用户管理和安全管理等工作。资源管理负责均衡地使用云资源节点，检测节点的故障并试图恢复或屏蔽它，并对资源的使用情况进行监视统计；任务管理负责执行用户或应用提交的任务，包括完成用户任务映象（Image）部署和管理、任务调度、任务执行、生命期管理等；用户管理是实现云计算商业模式的一个必不可少的环节，包括提供用户交互接口、管理和识别用户身份、创建用户程序的执行环境、对用户的使用进行计费等；安全管理保障云计算设施的整体安全，包括身份认证、访问授权、综合防护和安全审计等。

基于上述体系结构，本书以IaaS云计算为例，简述云计算的实现机制，如图1-5所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/25_1.jpg)

图1-5 简化的IaaS实现机制图

用户交互接口向应用以Web Services方式提供访问接口，获取用户需求。服务目录是用户可以访问的服务清单。系统管理模块负责管理和分配所有可用的资源，其核心是负载均衡。配置工具负责在分配的节点上准备任务运行环境。监视统计模块负责监视节点的运行状态，并完成用户使用节点情况的统计。执行过程并不复杂，用户交互接口允许用户从目录中选取并调用一个服务，该请求传递给系统管理模块后，它将为用户分配恰当的资源，然后调用配置工具为用户准备运行环境。

### 1.5 云计算压倒性的成本优势

为什么云计算拥有划时代的优势？主要原因在于它的技术特征和规模效应所带来的压倒性的性能价格比优势。

全球企业的IT开销分为三部分：硬件开销、能耗和管理成本。根据IDC在2007年做过的一个调查和预测（如图1-6所示），从1996年到2010年，全球企业IT开销中的硬件开销是基本持平的。但能耗和管理的成本上升非常迅速，以至于到2010年管理成本占了IT开销的大部分，而能耗开销越来越接近硬件开销了。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/26_1.jpg)

图1-6 全球企业IT开销发展趋势

如果使用云计算的话，系统建设和管理成本有很大的区别，如表1-1所示。根据James Hamilton的数据[1]，一个拥有5万个服务器的特大型数据中心与拥有1000个服务器中型数据中心相比，特大型数据中心的网络和存储成本只相当于中型数据中心的1/7～1/5，而每个管理员能够管理的服务器数量则扩大到7倍之多。因而，对于规模通常达到几十万乃至上百万台计算机的亚马逊和谷歌云计算而言，其网络、存储和管理成本比中型数据中心至少可以降低5～7倍。

表1-1 中型数据中心和特大型数据中心的成本比较

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/26_2.jpg)

电力和制冷成本也会有明显的差别。例如[1]，美国爱达荷州的水电资源丰富，电价很便宜。而夏威夷州是岛屿，本地没电力资源，电力价格就比较贵。二者最多相差7倍，如表1-2所示。

表1-2 美国不同地区电力价格的差异

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/27_1.jpg)

因为电价有如此显著的差异，谷歌的数据中心一般选择在人烟稀少、气候寒冷、水电资源丰富的地区，这些地点的电价、散热成本、场地成本、人力成本等都远远低于人烟稠密的大都市。剩下的挑战是要专门铺设光纤到这些数据中心。不过，由于光纤密集波分复用技术（DWDM)的应用，单根光纤的传输容量已超过10 Tbit/s，在地上开挖一条小沟埋设的光纤所能传输的信息容量几乎是无限的，远比将电力用高压输电线路引入城市要容易得多，而且没有衰减。拿谷歌的话来说，“传输光子比传输电子要容易得多”。这些数据中心采用了高度自动化的云计算软件来管理，需要的人员很少，而为了技术保密而拒绝外人进入参观，让人有一种神秘的感觉，故被人戏称为“信息时代的核电站”，如图1-7所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/27_2.jpg)

图1-7 被称为“信息时代的核电站”的谷歌数据中心

再者，云计算与传统互联网数据中心（IDC）相比，资源的利用率也有很大不同。IDC一般采用服务器托管和虚拟主机等方式对网站提供服务。每个租用IDC的网站所获得的网络带宽、处理能力和存储空间都是固定的。然而，绝大多数网站的访问流量都不是均衡的。例如，有的时间性很强，白天访问的人少，到了晚上七八点就会流量暴涨；有的季节性很强，平时访问人不多，但是到圣诞节前访问量就很大；有的一直默默无闻，但是由于某些突发事件（如迈克尔·杰克逊突然去世），使得访问量暴增而陷入瘫痪。网站拥有者为了应对这些突发流量，会按照峰值要求来配置服务器和网络资源，造成资源的平均利用率只有10%～15%，如图1-8所示。而云计算平台提供的是有弹性的服务，它根据每个租用者的需要在一个超大的资源池中动态分配和释放资源，而不需要为每个租用者预留峰值资源。而且云计算平台的规模极大，其租用者数量非常多，支撑的应用种类也是五花八门，比较容易平稳整体负载，因而云计算资源利用率可以达到80%左右，这又是传统模式的5～7倍。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/28_1.jpg)

图1-8 某典型网站的流量数据

综上所述，由于云计算有更低的硬件和网络成本、更低管理成本和电力成本，也有更高的资源利用率，两个乘起来就能够将成本节省30倍以上，如图1-9所示。这是个惊人的数字！这是云计算成为划时代技术的根本原因。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/28_2.jpg)

图1-9 云计算较之传统方式的性价比优势

从前面可以知道，云计算能够大幅节省成本，规模是极其重要的因素。那么，如果企业要建设自己的私有云，规模不大，也无法享受到电价优惠，是否就没有成本优势了呢？仍然会有数倍的优势。一方面，硬件采购成本还是会节省好几倍，这是因为云计算技术的容错能力很强，使得我们可以使用低端硬件代替高端硬件。另一方面，云计算设施的管理是高度自动化的，极少需要人工干预，可以大大减少管理人员的数量。中国移动研究院建立了1024个节点的Big Cloud云计算设施，并用它进行海量数据挖掘，大大节省了成本。

对云计算用户而言，云计算的优势也是无与伦比的。他们不用开发软件，不用安装硬件，用低得多的使用成本，就可以快速部署应用系统，而且可以动态伸缩系统的规模，可以更容易地共享数据。租用公共云的企业不再需要自建数据中心，只需申请账号并按量付费，这一点对于中小企业和刚起步的创业公司尤为重要。目前，云计算的应用领域涵盖应用托管、存储备份、内容推送、电子商务、高性能计算、媒体服务、搜索引擎、Web托管等多个领域，代表性的云计算应用企业包括Abaca、BeInSync、AF83、Giveness、纽约时报、华盛顿邮报、GigaVox、SmugMug、Alexa、Digitaria等。纽约时报使用亚马逊云计算服务在不到24小时的时间里处理了1100万篇文章，累计花费仅240美元。如果用自己的服务器，需要数月时间和多得多的费用。

**习** **题**

1.大数据现象是怎么形成的？

2.新摩尔定律的含义是什么？

3.云计算有哪些特点？

4.云计算按照服务类型可以分为哪几类？

5.云计算技术体系结构可以分为哪几层？

6.在性价比上云计算相比传统技术为什么有压倒性的优势？

**参** **考** **文** **献**

[1] Michael Armbrust,Armando Fox,and Rean Griffith,et al.Above the Clouds:A Berkeley View of Cloud Computing,mimeo,UC Berkeley,RAD Laboratory,2009.

[2] Ian Foster,Carl Kesselman,and Steve Tuecke.The Anatomy of the Grid:Enabling Scalable Virtual Organizations.International Journal of High Performance Computing Applications,15(3),2001

[3] 刘鹏.提出一种实用的网格实现方式——网格计算池模型，2002

<http://www.chinagrid.net/show.aspx?id=1672&cid=57>

[4] Peng Liu,Yao Shi,San-li Li,Computing Pool—a Simplified and Practical Computational Grid Model,the Second International Workshop on Grid and Cooperative Computing (GCC 2003),Shanghai,Dec 7-10,2003,published in Lecture Notes in Computer Science (LNCS),Vol.3032,Heidelberg:Springer-Verlag,2004

[5] Peng Liu,Yao Shi,Francis C.M.Lau,Cho-Li Wang,San-Li Li,Grid Demo:AntiSpamGrid,IEEE International Conference on Cluster Computing,Hong Kong,Dec 1-4,2003,selected as one of the excellent Grid research projects for the GridDemo session

------

[1] 1 EB = 1024 PB = $10^{18}$ 字节，1 PB = 1024 TB = $10^{15}$字节，1 TB = 1024 GB = $10^{12}$字节，1 GB = 1024 MB = $10^{9}$字节，1 MB = 1024 KB = $10^{6}$字节，1 KB = 1024字节。

## 第二章 Google云计算原理与应用

Google（谷歌）拥有全球最强大的搜索引擎。除了搜索业务，Google还有Google Maps、Google Earth、Gmail、YouTube等其他业务。这些应用的共性在于数据量巨大，且要面向全球用户提供实时服务，因此Google必须解决海量数据存储和快速处理问题。Google研发出了简单而又高效的技术，让多达百万台的廉价计算机协同工作，共同完成这些任务，这些技术在诞生几年后才被命名为Google云计算技术。Google云计算技术包括：Google文件系统GFS、分布式计算编程模型MapReduce、分布式锁服务Chubby、分布式结构化数据表Bigtable、分布式存储系统Megastore、分布式监控系统Dapper、海量数据的交互式分析工具Dremel，以及内存大数据分析系统PowerDrill等。本章详细介绍这八种核心技术和Google应用程序引擎。

### 2.1 Google文件系统GFS

Google文件系统（Google File System，GFS）是一个大型的分布式文件系统。它为Google云计算提供海量存储，并且与Chubby、MapReduce及Big-table等技术结合十分紧密，处于所有核心技术的底层。GFS不是一个开源的系统，我们仅能从Google公布的技术文档来获得相关知识。文献[1]是Google公布的关于GFS的最为详尽的技术文档，它从GFS产生的背景、特点、系统框架、性能测试等方面进行了详细的阐述。

当前主流分布式文件系统有RedHat的GFS[3]（Global File System）、IBM的GPFS[4]、Sun的Lustre[5]等。这些系统通常用于高性能计算或大型数据中心，对硬件设施条件要求较高。以Lustre文件系统为例，它只对元数据管理器MDS提供容错解决方案，而对于具体的数据存储节点OST来说，则依赖其自身来解决容错的问题。例如，Lustre推荐OST节点采用RAID技术或SAN存储区域网来容错，但由于Lustre自身不能提供数据存储的容错，一旦OST发生故障就无法恢复，因此对OST的稳定性就提出了相当高的要求，从而大大增加了存储的成本，而且成本会随着规模的扩大线性增长。

Google GFS的新颖之处在于它采用廉价的商用机器构建分布式文件系统，同时将GFS的设计与Google应用的特点紧密结合，简化实现，使之可行，最终达到创意新颖、有用、可行的完美组合。GFS将容错的任务交给文件系统完成，利用软件的方法解决系统可靠性问题，使存储的成本成倍下降。GFS将服务器故障视为正常现象，并采用多种方法，从多个角度，使用不同的容错措施，确保数据存储的安全、保证提供不间断的数据存储服务。

#### 2.1.1 系统架构

GFS的系统架构如图2-1[1]所示。GFS将整个系统的节点分为三类角色：Client（客户端）、Master（主服务器）和Chunk Server（数据块服务器）。Client是GFS提供给应用程序的访问接口，它是一组专用接口，不遵守POSIX规范，以库文件的形式提供。应用程序直接调用这些库函数，并与该库链接在一起。Master是GFS的管理节点，在逻辑上只有一个，它保存系统的元数据，负责整个文件系统的管理，是GFS文件系统中的“大脑”。Chunk Server负责具体的存储工作。数据以文件的形式存储在Chunk Server上，Chunk Server的个数可以有多个，它的数目直接决定了GFS的规模。GFS将文件按照固定大小进行分块，默认是64 MB，每一块称为一个Chunk（数据块），每个Chunk都有一个对应的索引号（Index）。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/31_1.jpg)

图2-1 GFS的系统架构

客户端在访问GFS时，首先访问Master节点，获取与之进行交互的Chunk Server信息，然后直接访问这些Chunk Server，完成数据存取工作。GFS的这种设计方法实现了控制流和数据流的分离。Client与Master之间只有控制流，而无数据流，极大地降低了Master的负载。Client与Chunk Server之间直接传输数据流，同时由于文件被分成多个Chunk进行分布式存储，Client可以同时访问多个Chunk Server，从而使得整个系统的I/O高度并行，系统整体性能得到提高。

针对多种应用的特点，Google从多个方面简化设计的GFS，在一定规模下达到了成本、可靠性和性能的最佳平衡。具体来说，它具有以下几个特点。

**1. 采用中心服务器模式：**

GFS采用中心服务器模式管理整个文件系统，简化了设计，降低了实现难度。Master管理分布式文件系统中的所有元数据。文件被划分为Chunk进行存储，对于Master来说，每个Chunk Server只是一个存储空间。Client发起的所有操作都需要先通过Master才能执行。这样做有许多好处，增加新的Chunk Server是一件十分容易的事情，Chunk Server只需要注册到Master上即可，Chunk Server之间无任何关系。如果采用完全对等的、无中心的模式，那么如何将Chunk Server的更新信息通知到每一个Chunk Server，会是设计的一个难点，而这也将在一定程度上影响系统的扩展性。Master维护了一个统一的命名空间，同时掌握整个系统内Chunk Server的情况，据此可以实现整个系统范围内数据存储的负载均衡。由于只有一个中心服务器，元数据的一致性问题自然解决。当然，中心服务器模式也带来一些固有的缺点，比如极易成为整个系统的瓶颈等。GFS采用多种机制来避免Master成为系统性能和可靠性上的瓶颈，如尽量控制元数据的规模、对Master进行远程备份、控制信息和数据分流等。

**2. 不缓存数据：**

缓存（Cache）机制是提升文件系统性能的一个重要手段，通用文件系统为了提高性能，一般需要实现复杂的缓存机制。GFS文件系统根据应用的特点，没有实现缓存，这是从必要性和可行性两方面考虑的。从必要性上讲，客户端大部分是流式顺序读写，并不存在大量的重复读写，缓存这部分数据对提高系统整体性能的作用不大；对于Chunk Server，由于GFS的数据在Chunk Server上以文件的形式存储，如果对某块数据读取频繁，本地的文件系统自然会将其缓存。从可行性上讲，如何维护缓存与实际数据之间的一致性是一个极其复杂的问题，在GFS中各个Chunk Server的稳定性都无法确保，加之网络等多种不确定因素，一致性问题尤为复杂。此外由于读取的数据量巨大，以当前的内存容量无法完全缓存。对于存储在Master中的元数据，GFS采取了缓存策略。因为一方面Master需要频繁操作元数据，把元数据直接保存在内存中，提高了操作的效率。另一方面采用相应的压缩机制降低元数据占用空间的大小，提高内存的利用率。

**3. 在用户态下实现：**

文件系统是操作系统的重要组成部分，通常位于操作系统的底层（内核态）。在内核态实现文件系统，可以更好地和操作系统本身结合，向上提供兼容的POSIX接口。然而，GFS却选择在用户态下实现，主要基于以下考虑。

（1）在用户态下实现，直接利用操作系统提供的POSIX编程接口就可以存取数据，无须了解操作系统的内部实现机制和接口，降低了实现的难度，提高了通用性。

（2）POSIX接口提供的功能更为丰富，在实现过程中可以利用更多的特性，而不像内核编程那样受限。

（3）用户态下有多种调试工具，而在内核态中调试相对比较困难。

（4）用户态下，Master和Chunk Server都以进程的方式运行，单个进程不会影响到整个操作系统，从而可以对其进行充分优化。在内核态下，如果不能很好地掌握其特性，效率不但不会高，甚至还会影响到整个系统运行的稳定性。

（5）用户态下，GFS和操作系统运行在不同的空间，两者耦合性降低，方便GFS自身和内核的单独升级。

**4. 只提供专用接口：**

通常的分布式文件系统一般都会提供一组与POSIX规范兼容的接口，使应用程序可以通过操作系统的统一接口透明地访问文件系统，而不需要重新编译程序。GFS在设计之初，是完全面向Google的应用的，采用了专用的文件系统访问接口。接口以库文件的形式提供，应用程序与库文件一起编译，Google应用程序在代码中通过调用这些库文件的API，完成对GFS文件系统的访问。采用专用接口有以下好处。

（1）降低了实现的难度。通常与POSIX兼容的接口需要在操作系统内核一级实现，而GFS是在应用层实现的。

（2）采用专用接口可以根据应用的特点对应用提供一些特殊支持，如支持多个文件并发追加的接口等。

（3）专用接口直接和Client、Master、Chunk Server交互，减少了操作系统之间上下文的切换，降低了复杂度，提高了效率。

#### 2.1.2 容错机制

**1. Master 容错：**

具体来说，Master上保存了GFS文件系统的三种元数据。

（1）命名空间（Name Space），也就是整个文件系统的目录结构。

（2）Chunk与文件名的映射表。

（3）Chunk副本的位置信息，每一个Chunk默认有三个副本。

首先就单个Master来说，对于前两种元数据，GFS通过操作日志来提供容错功能。第三种元数据信息则直接保存在各个Chunk Server上，当Master启动或Chunk Server向Master注册时自动生成。因此当Master发生故障时，在磁盘数据保存完好的情况下，可以迅速恢复以上元数据。为了防止Master彻底死机的情况，GFS还提供了Master远程的实时备份，这样在当前的GFS Master出现故障无法工作的时候，另外一台GFS Master可以迅速接替其工作。

**2. Chunk Server 容错：**

GFS采用副本的方式实现Chunk Server的容错。每一个Chunk有多个存储副本（默认为三个），分布存储在不同的Chunk Server上。副本的分布策略需要考虑多种因素，如网络的拓扑、机架的分布、磁盘的利用率等。对于每一个Chunk，必须将所有的副本全部写入成功，才视为成功写入。之后，如果相关的副本出现丢失或不可恢复等情况，Master自动将该副本复制到其他Chunk Server，从而确保副本保持一定的个数。尽管一份数据需要存储三份，好像磁盘空间的利用率不高，但综合比较多种因素，加之磁盘的成本不断下降，采用副本无疑是最简单、最可靠、最有效，而且实现的难度也最小的一种方法。

GFS中的每一个文件被划分成多个Chunk，Chunk的默认大小是64 MB，这是因为Google应用中处理的文件都比较大，以64 MB为单位进行划分，是一个较为合理的选择。Chunk Server存储的是Chunk的副本，副本以文件的形式进行存储。每一个Chunk以Block为单位进行划分，大小为64 KB，每一个Block对应一个32 bit的校验和。当读取一个Chunk副本时，Chunk Server会将读取的数据和校验和进行比较，如果不匹配，就会返回错误，使Client选择其他Chunk Server上的副本。

#### 2.1.3 系统管理技术

GFS是一个分布式文件系统，包含从硬件到软件的整套解决方案。除了上面提到的GFS的一些关键技术外，还有相应的系统管理技术来支持整个GFS的应用，这些技术可能不一定为GFS独有。

**1. 大规模集群安装技术：**

安装GFS的集群中通常有非常多的节点，文献[1]中最大的集群超过1000个节点，而现在的Google数据中心动辄有万台以上的机器在运行。因此迅速地安装、部署一个GFS的系统，以及迅速地进行节点的系统升级等，都需要相应的技术支撑。

**2. 故障检测技术：**

GFS是构建在不可靠的廉价计算机之上的文件系统，由于节点数目众多，故障发生十分频繁，如何在最短的时间内发现并确定发生故障的Chunk Server，需要相关的集群监控技术。

**3. 节点动态加入技术：**

当有新的Chunk Server加入时，如果需要事先安装好系统，那么系统扩展将是一件十分烦琐的事情。如果能够做到只需将裸机加入，就会自动获取系统并安装运行，那么将会大大减少GFS维护的工作量。

**4. 节能技术：**

有关数据表明，服务器的耗电成本大于当初的购买成本，因此Google采用了多种机制来降低服务器的能耗，例如对服务器主板进行修改，采用蓄电池代替昂贵的UPS（不间断电源系统），提高能量的利用率。Rich Miller 在一篇关于数据中心的博客文章中表示，这个设计让 Google 的 UPS 利用率达到99.9%，而一般数据中心只能达到92%～95%。

### 2.2 分布式数据处理MapReduce

MapReduce是Google提出的一个软件架构，是一种处理海量数据的并行编程模式，用于大规模数据集（通常大于1 TB）的并行运算。Map（映射）、Reduce（化简）的概念和主要思想，都是从函数式编程语言和矢量编程语言借鉴来的[5]。正是由于MapReduce有函数式和矢量编程语言的共性，使得这种编程模式特别适合于非结构化和结构化的海量数据的搜索、挖掘、分析与机器智能学习等。

#### 2.2.1 产生背景

MapReduce这种并行编程模式思想最早是在1995年提出的，文献[6]首次提出了“map”和“fold”的概念，和Google现在所使用的“Map”和“Reduce”思想相吻合。与传统的分布式程序设计相比，MapReduce封装了并行处理、容错处理、本地化计算、负载均衡等细节，还提供了一个简单而强大的接口。通过这个接口，可以把大尺度的计算自动地并发和分布执行，使编程变得非常容易。另外，MapReduce也具有较好的通用性，大量不同的问题都可以简单地通过MapReduce来解决。

MapReduce把对数据集的大规模操作，分发给一个主节点管理下的各分节点共同完成，通过这种方式实现任务的可靠执行与容错机制。在每个时间周期，主节点都会对分节点的工作状态进行标记。一旦分节点状态标记为死亡状态，则这个节点的所有任务都将分配给其他分节点重新执行。

据相关统计，每使用一次 Google 搜索引擎，Google 的后台服务器就要进行1011次运算。这么庞大的运算量，如果没有好的负载均衡机制，有些服务器的利用率会很低，有些则会负荷太重，有些甚至可能死机，这些都会影响系统对用户的服务质量。而使用MapReduce这种编程模式，就保持了服务器之间的均衡，提高了整体效率。

#### 2.2.2 编程模型

MapReduce的运行模型如图2-2所示。图中有*M*个Map操作和*R*个Reduce操作。

简单地说，一个Map函数就是对一部分原始数据进行指定的操作。每个Map操作都针对不同的原始数据，因此Map与Map之间是互相独立的，这使得它们可以充分并行化。一个Reduce操作就是对每个Map所产生的一部分中间结果进行合并操作，每个Reduce所处理的Map中间结果是互不交叉的，所有Reduce产生的最终结果经过简单连接就形成了完整的结果集，因此Reduce也可以在并行环境下执行。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/35_1.jpg)

图2-2 MapReduce的运行模型

在编程的时候，开发者需要编写两个主要函数：

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/35_2.jpg)

Map 和 Reduce 的输入参数和输出结果根据应用的不同而有所不同。Map 的输入参数是in_key和in_value，它指明了Map需要处理的原始数据是哪些。Map的输出结果是一组＜key，value＞对，这是经过 Map 操作后所产生的中间结果。在进行 Reduce 操作之前，系统已经将所有Map产生的中间结果进行了归类处理，使得相同key对应的一系列value能够集结在一起提供给一个Reduce进行归并处理，也就是说，Reduce的输入参数是（key，[value~1~，…，value~m~]）。Reduce 的工作是需要对这些对应相同key的value 值进行归并处理，最终形成（key，final_value)的结果。这样，一个 Reduce 处理了一个 key，所有Reduce的结果并在一起就是最终结果。

例如，假设我们想用MapReduce来计算一个大型文本文件中各个单词出现的次数，Map的输入参数指明了需要处理哪部分数据，以“<在文本中的起始位置，需要处理的数据长度>”表示，经过Map处理，形成一批中间结果“<单词，出现次数>”。而Reduce函数处理中间结果，将相同单词出现的次数进行累加，得到每个单词总的出现次数。

#### 2.2.3 实现机制

MapReduce操作的执行流程[7]如图2-3所示。

用户程序调用MapReduce函数后，会引起下面的操作过程（图中的数字标示和下面的数字标示相同）：

（1）MapReduce函数首先把输入文件分成*M*块，每块大概16M～64MB（可以通过参数决定），接着在集群的机器上执行分派处理程序。

（2）这些分派的执行程序中有一个程序比较特别，它是主控程序Master。剩下的执行程序都是作为Master分派工作的Worker（工作机）。总共有*M*个Map任务和*R*个Reduce任务需要分派，Master选择空闲的Worker来分配这些Map或Reduce任务。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/36_1.jpg)

图2-3 MapReduce操作的执行流程

（3）一个被分配了Map任务的Worker读取并处理相关的输入块。它处理输入的数据，并且将分析出的<key，value>对传递给用户定义的Map函数。Map函数产生的中间结果<key，value>对暂时缓冲到内存。

（4）这些缓冲到内存的中间结果将被定时写到本地硬盘，这些数据通过分区函数分成*R*个区。中间结果在本地硬盘的位置信息将被发送回Master，然后Master负责把这些位置信息传送给Reduce Worker。

（5）当Master通知执行Reduce的Worker关于中间<key，value>对的位置时，它调用远程过程，从Map Worker的本地硬盘上读取缓冲的中间数据。当Reduce Worker读到所有的中间数据，它就使用中间key进行排序，这样可使相同key的值都在一起。因为有许多不同key的Map都对应相同的Reduce任务，所以，排序是必需的。如果中间结果集过于庞大，那么就需要使用外排序。

（6）Reduce Worker根据每一个唯一中间key来遍历所有的排序后的中间数据，并且把key和相关的中间结果值集合传递给用户定义的Reduce函数。Reduce函数的结果写到一个最终的输出文件。

（7）当所有的Map任务和Reduce任务都完成的时候，Master激活用户程序。此时MapReduce返回用户程序的调用点。

由于MapReduce在成百上千台机器上处理海量数据，所以容错机制是不可或缺的。总的来说，MapReduce通过重新执行失效的地方来实现容错。

**1. Master 失效：**

Master会周期性地设置检查点（checkpoint），并导出Master的数据。一旦某个任务失效，系统就从最近的一个检查点恢复并重新执行。由于只有一个Master在运行，如果Master失效了，则只能终止整个MapReduce程序的运行并重新开始。

**2. Worker 失效：**

相对于Master失效而言，Worker失效算是一种常见的状态。Master会周期性地给Worker发送ping命令，如果没有Worker的应答，则Master认为Worker失效，终止对这个Worker的任务调度，把失效Worker的任务调度到其他Worker上重新执行。

#### 2.2.4 案例分析

排序通常用于衡量分布式数据处理框架的数据处理能力，下面介绍如何利用MapReduce进行数据排序。假设有一批海量的数据，每个数据都是由26个字母组成的字符串，原始的数据集合是完全无序的，怎样通过MapReduce完成排序工作，使其有序（字典序）呢？可通过以下三个步骤来完成。

（1）对原始的数据进行分割（Split），得到*N*个不同的数据分块，如图2-4所示。

（2）对每一个数据分块都启动一个Map进行处理。采用桶排序的方法，每个Map中按照首字母将字符串分配到26个不同的桶中，图2-5是Map的过程及其得到的中间结果。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/38_1.jpg)

图2-4 数据分块

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/38_2.jpg)

图2-5 Map的过程及其得到的中间结果

（3）对于Map之后得到的中间结果，启动26个Reduce。按照首字母将Map中不同桶中的字符串集合放置到相应的Reduce中进行处理。具体来说就是首字母为a的字符串全部放在Reduce1中处理，首字母为b的字符串全部放在Reduce2，以此类推。每个Reduce对于其中的字符串进行排序，结果直接输出。由于Map过程中已经做到了首字母有序，Reduce输出的结果就是最终的排序结果。这一过程如图2-6所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/39_1.jpg)

图2-6 Reduce过程

从上述过程中可以看出，由于能够实现处理过程的完全并行化，因此利用MapReduce处理海量数据是非常适合的。

### 2.3 分布式锁服务Chubby

Chubby是Google设计的提供粗粒度锁服务的一个文件系统，它基于松耦合分布式系统，解决了分布的一致性问题。通过使用Chubby的锁服务，用户可以确保数据操作过程中的一致性。不过值得注意的是，这种锁只是一种建议性的锁（Advisory Lock）而不是强制性的锁（Mandatory Lock），这种选择使系统具有更大的灵活性。

GFS使用Chubby选取一个GFS主服务器，Bigtable使用Chubby指定一个主服务器并发现、控制与其相关的子表服务器。除了最常用的锁服务之外，Chubby还可以作为一个稳定的存储系统存储包括元数据在内的小数据。同时Google内部还使用Chubby进行名字服务（Name Server）。本节首先简要介绍Paxos算法，因为Chubby内部一致性问题的实现用到了Paxos算法；然后围绕Chubby系统的设计和实现展开讲解。

#### 2.3.1 Paxos算法

Paxos算法[14]是Leslie Lamport最先提出的一种基于消息传递（Messages Passing）的一致性算法，用于解决分布式系统中的一致性问题。在目前所有的一致性算法中，该算法最常用且被认为是最有效的。

简单地说，分布式系统的一致性问题，就是如何保证系统中初始状态相同的各个节点在执行相同的操作序列时，看到的指令序列是完全一致的，并且最终得到完全一致的结果。怎么才能保证在一个操作序列中每个步骤仅有一个值呢？一个最简单的方案就是在分布式系统中设置一个专门节点，在每次需要进行操作之前，系统的各个部分向它发出请求，告诉该节点接下来系统要做什么。该节点接受第一个到达的请求内容作为接下来的操作，这样就能够保证系统只有一个唯一的操作序列。但是这样做也有一个很明显的缺陷，那就是一旦这个专门节点失效，整个系统就很可能出现不一致。为了避免这种情况，在系统中必然要设置多个专门节点，由这些节点来共同决定操作序列。针对这种多节点决定操作系列的情况，Lamport提出了Paxos算法。在他的算法中节点被分成了三种类型：proposers、acceptors和 learners。其中proposers提出决议（value，实际上就是告诉系统接下来该执行哪个指令），acceptors批准决议，learners获取并使用已经通过的决议。一个节点可以兼有多重类型。在这种情况下，满足以下三个条件[15]就可以保证数据的一致性。

（1）决议只有在被proposers提出后才能批准。

（2）每次只批准一个决议。

（3）只有决议确定被批准后learners才能获取这个决议。

为了满足上述三个条件（主要是第二个条件），必须对系统有一些约束条件。Lamport通过约束条件的不断加强，最后得到了一个可以实际运用到算法中的完整约束条件。那么，如何得到这个完整的约束条件呢？在决议的过程中，proposers将决议发送给accpetors，acceptors对决议进行批准，批准后的决议才能成为正式的决议。决议的批准采用少数服从多数原则，即大多数acceptors接受的决议将成为最终的正式决议。从集合论的观点来看，两组“多数派”（Majority）至少有一个公共的acceptor。如果每个acceptor只能接受一个决议，则第二个条件就能够得到保证，因此不难得到第一个约束条件[15]：

p1：每个acceptor只接受它得到的第一个决议。

p1表明一个acceptor可以收到多个决议，为了区分，对每个决议进行编号，后到的决议编号大于先到的决议编号。约束条件p1不是很完备，假设系统中一半的acceptors接受了决议1，剩下的一半接受了决议2。此时仅靠约束p1是根本无法得到一个“多数派”，从而无法得到一个正式的决议。进一步加强约束得到：

p2：一旦某个决议得到通过，之后通过的决议必须和该决议保持一致。

p1和p2能够保证第二个条件。对p2稍作加强得到：

p2a：一旦某个决议v得到通过，之后任何acceptor再批准的决议必须是v。

表面上看起来已经不存在什么问题了，但实际上p2a和p1是有矛盾的。考虑下面这种情况：假设在系统得到决议v的过程中一个proposer和一个acceptor因为出现问题并没有参与到决议的表决中。在得到决议v之后出现问题proposer和accepor恢复过来，此时这个proposer提出一个决议w（w不等于v）给这个acceptor。如果按照p1，这个acceptor应该接受这个决议w，但是按照p2a，则不应该接受这个决议。所以还需进一步加强约束条件：

p2b：一旦某个决议v得到通过，之后任何proposer再提出的决议必须是v。

满足p1和p2b就能够保证第二个条件，而且彼此之间不存在矛盾。但是p2b很难通过一种技术手段来实现它，因此提出了一个蕴含p2b的约束p2c：

p2c：如果一个编号为*n*的提案具有值v，那么存在一个“多数派”，要么它们中没有谁批准过编号小于*n*的任何提案，要么它们进行的最近一次批准具有值v。

为了保证决议的唯一性，acceptors也要满足一个约束条件：当且仅当 acceptors 没有收到编号大于*n*的请求时，acceptors 才批准编号为*n*的提案。

在这些约束条件的基础上，可以将一个决议的通过分成以下两个阶段[15]。

（1）准备阶段：proposers选择一个提案并将它的编号设为*n*，然后将它发送给acceptors中的一个“多数派”。acceptors 收到后，如果提案的编号大于它已经回复的所有消息，则acceptors将自己上次的批准回复给proposers，并不再批准小于*n*的提案。

（2）批准阶段：当proposers接收到acceptors 中的这个“多数派”的回复后，就向回复请求的acceptors发送accept请求，在符合acceptors一方的约束条件下，acceptors收到accept请求后即批准这个请求。

为了减少决议发布过程中的消息量，acceptors将这个通过的决议发送给learners 的一个子集，然后由这个子集中的learners 去通知所有其他的learners。一般情况下，以上的算法过程就可以成功地解决一致性问题，但是也有特殊情况。根据算法一个编号更大的提案会终止之前的提案过程，如果两个proposer在这种情况下都转而提出一个编号更大的提案，那么就可能陷入活锁。此时需要选举出一个president，仅允许 president提出提案。

以上简要地介绍了Paxos算法的核心内容，关于更多的实现细节读者可以参考Lamport关于Paxos算法实现的文章。

#### 2.3.2 Chubby系统设计

通常情况下Google的一个数据中心仅运行一个Chubby单元[13]（Chubby cell，下面会有详细讲解述），这个单元需要支持包括GFS、Bigtable在内的众多Google服务，因此，在设计Chubby时候，必须充分考虑系统需要实现的目标以及可能出现的各种问题。

Chubby的设计目标主要有以下几点。

（1）高可用性和高可靠性。这是系统设计的首要目标，在保证这一目标的基础上再考虑系统的吞吐量和存储能力。

（2）高扩展性。将数据存储在价格较为低廉的RAM，支持大规模用户访问文件。

（3）支持粗粒度的建议性锁服务。提供这种服务的根本目的是提高系统的性能。

（4）服务信息的直接存储。可以直接存储包括元数据、系统参数在内的有关服务信息，而不需要再维护另一个服务。

（5）支持通报机制。客户可以及时地了解到事件的发生。

（6）支持缓存机制。通过一致性缓存将常用信息保存在客户端，避免了频繁地访问主服务器。

Google没有直接实现一个包含了Paxos算法的函数库，而是在Paxos算法的基础上设计了一个全新的锁服务Chubby。Chubby中涉及的一致性问题都由Paxos解决，除此之外Chubby中还添加了一些新的功能特性。这种设计主要是考虑到以下几个问题[13]。

（1）通常情况下开发者在开发的初期很少考虑系统的一致性问题，但是随着开发的不断进行，这种问题会变得越来越严重。单独的锁服务可以保证原有系统的架构不会发生改变，而使用函数库的话很可能需要对系统的架构做出大幅度的改动。

（2）系统中很多事件的发生是需要告知其他用户和服务器的，使用一个基于文件系统的锁服务可以将这些变动写入文件中。这样其他需要了解这些变动的用户和服务器直接访问这些文件即可，避免了因大量的系统组件之间的事件通信带来的系统性能下降。

（3）基于锁的开发接口容易被开发者接受。虽然在分布式系统中锁的使用会有很大的不同，但是和一致性算法相比，锁显然被更多的开发者所熟知。

Paxos算法的实现过程中需要一个“多数派”就某个值达成一致，进而才能得到一个分布式一致性状态。这个过程本质上就是分布式系统中常见的quorum机制（quorum原意是法定人数，简单说来就是根据少数服从多数的选举原则产生一个决议）。为了保证系统的高可用性，需要若干台机器，但是使用单独的锁服务的话一台机器也能保证这种高可用性。也就是说，Chubby在自身服务的实现时利用若干台机器实现了高可用性，而外部用户利用Chubby则只需一台机器就可以保证高可用性。

正是考虑到以上几个问题，Google设计了Chubby，而不是单独地维护一个函数库（实际上，Google有这样一个独立于Chubby的函数库，不过一般情况下并不会使用）。在设计的过程中有一些细节问题也值得我们关注，比如在Chubby系统中采用了建议性的锁而没有采用强制性的锁。两者的根本区别在于用户访问某个被锁定的文件时，建议性的锁不会阻止访问，而强制性的锁则会阻止访问，实际上这是为了方便系统组件之间的信息交互。另外，Chubby还采用了粗粒度（Coarse-Grained）锁服务而没有采用细粒度（Fine-Grained）锁服务，两者的差异在于持有锁的时间。细粒度的锁持有时间很短，常常只有几秒甚至更少，而粗粒度的锁持有的时间可长达几天，选择粗粒度的锁可以减少频繁换锁带来的系统开销。

如图2-7[13]所示是Chubby的基本架构。很明显，Chubby被划分成两个部分：客户端和服务器端，客户端和服务器端之间通过远程过程调用（RPC）来连接。在客户这一端每个客户应用程序都有一个Chubby程序库（Chubby Library），客户端的所有应用都是通过调用这个库中的相关函数来完成的。服务器一端称为Chubby单元，一般是由五个称为副本（Replica）的服务器组成的，这五个副本在配置上完全一致，并且在系统刚开始时处于对等地位。

![43_1](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/43_1.jpg)

图2-7 Chubby的基本架构

#### 2.3.3 Chubby中的Paxos

一致性问题是Chubby需要解决的一个关键性问题，那么Paxos算法在Chubby中究竟是怎样起作用的呢？

为了了解Paxos算法作用，需要将单个副本的结构剖析来看，单个Chubby副本结构如图2-8[16] 所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/43_2.jpg)

图2-8 单个Chubby副本结构

从图中可以看出，单个副本主要由以下三个层次组成。

（1）最底层是一个容错的日志，该日志对于数据库的正确性提供了重要的支持。不同副本上日志的一致性正是通过Paxos算法来保证的。副本之间通过特定的Paxos协议进行通信，同时本地文件中还保存有一份同Chubby中相同的日志数据。

（2）最底层之上是一个容错的数据库，这个数据库主要包括一个快照（Snapshot）和一个记录数据库操作的重播日志（Replay-log），每一次的数据库操作最终都将提交至日志中。和容错的日志类似的是，本地文件中也保存着一份数据库数据副本。

（3）Chubby构建在这个容错的数据库之上，Chubby利用这个数据库存储所有的数据。Chubby的客户端通过特定的Chubby协议和单个的Chubby副本进行通信。

由于副本之间的一致性问题，客户端每次向容错的日志中提交新的值（value）时，Chubby就会自动调用Paxos构架保证不同副本之间数据的一致性。图2-9[16]就显示了这个过程。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/44_1.jpg)

图2-9 容错日志的API

结合图2-9来看，在Chubby中Paxos算法的实际作用为如下三个过程。

（1）选择一个副本成为协调者（Coordinator）。

（2）协调者从客户提交的值中选择一个，然后通过一种被称为accept的消息广播给所有的副本，其他的副本收到广播之后，可以选择接受或者拒绝这个值，并将决定结果反馈给协调者。

（3）一旦协调者收到大多数副本的接受信息后，就认为达到了一致性，接着协调者向相关的副本发送一个commit消息。

上述三个过程实际上跟Paxos的核心思想是完全一致的，这些过程保证提交到不同副本上容错日志中的数据是完全一致的，进而保证Chubby中数据的一致性。

由于单个的协调者可能失效，系统允许同时有多个协调者，但多个协调者可能会导致多个协调者提交了不同的值。对此Chubby的设计者借鉴了Paxos中的两种解决机制：给协调者指派序号或限制协调者可以选择的值。

针对前者，Chubby的设计者给出了如下一种指派序号的方法。

（1）在一个有 *n* 个副本的系统中，为每个副本分配一个 id $i_r$，其中 $0\leqslant i_r\leqslant n-1$。则副本的序号 $s=k×n+i_r$，其中*k*的初始值为0。

（2）某个副本想成为协调者之后，它就根据规则生成一个比它以前的序号更大的序号（实际上就是提高*k*的值），并将这个序号通过propose消息广播给其他所有的副本。

（3）如果接受到广播的副本发现该序号比它以前见过的序号都大，则向发出广播的副本返回一个promise消息，并且承诺不再接受旧的协调者发送的消息。如果大多数副本都返回了promise消息，则新的协调者就产生了。

对于后一种解决方法，Paxos强制新的协调者必须选择和前任相同的值。

为了提高系统的效率，Chubby做了一个重要的优化，那就是在选择某一个副本作为协调者之后就长期不变，此时协调者就被称为主服务器（Master）。产生一个主服务器避免了同时有多个协调者而带来的一些问题。

在Chubby中，客户端的数据请求都是由主服务器来完成，Chubby保证在一定的时间内有且仅有一个主服务器，这个时间就称为主服务器租约期（Master Lease）。如果某个服务器被连续推举为主服务器的话，这个租约期就会不断地被更新。租续期内所有的客户请求都由主服务器处理。客户端如果需要确定主服务器的位置，可以向DNS发送一个主服务器定位请求，非主服务器的副本将对该请求做出回应，通过这种方式客户端能够快速、准确地对主服务器做出定位。客户端和服务器之间的通信过程将在2.3.5节详细介绍。

需要注意的是，Chubby对于Paxos论文中未提及的一些技术细节进行了补充，所以Chubby的实现是基于Paxos，但其技术手段更加的丰富，更具有实践性。但这也导致了最终实现的Chubby不是一个完全经过理论上验证的系统。.

#### 2.3.4 Chubby文件系统

Chubby系统本质上就是一个分布式的、存储大量小文件的文件系统，它所有的操作都是在文件的基础上完成的。例如在Chubby最常用的锁服务中，每一个文件就代表了一个锁，用户通过打开、关闭和读取文件，获取共享（Shared）锁或独占（Exclusive）锁。选举主服务器的过程中，符合条件的服务器都同时申请打开某个文件并请求锁住该文件。成功获得锁的服务器自动成为主服务器并将其地址写入这个文件夹，以便其他服务器和用户可以获知主服务器的地址信息。

Chubby的文件系统[13]和UNIX类似。例如在文件名“/ls/foo/wombat/pouch”中，ls代表lock service，这是所有Chubby文件系统的共有前缀；foo是某个单元的名称；/wombat/pouch则是foo这个单元上的文件目录或者文件名。由于Chubby自身的特殊服务要求，Google对Chubby做了一些与UNIX不同的改变。例如Chubby不支持内部文件的移动；不记录文件的最后访问时间；另外在Chubby中并没有符号连接（Symbolic Link，又叫软连接，类似于Windows系统中的快捷方式）和硬连接（Hard Link，类似于别名）的概念。在具体实现时，文件系统由许多节点组成，分为永久型和临时型，每个节点就是一个文件或目录。节点中保存着包括ACL（Access Control List，访问控制列表，将在2.3.6节讲解）在内的多种系统元数据。为了用户能够及时了解元数据的变动，系统规定每个节点的元数据都应当包含以下四种单调递增的64位编号[13]。

（1）实例号（Instance Number）：新节点实例号必定大于旧节点的实例号。

（2）内容生成号（Content Generation Number）：文件内容修改时该号增加。

（3）锁生成号（Lock Generation Number）：锁被用户持有时该号增加。

（4）ACL生成号（ACL Generation Number）：ACL名被覆写时该号增加。

用户在打开某个节点的同时会获取一个类似于UNIX中文件描述符（File Descriptor）的句柄[13]（Handles），这个句柄由以下三个部分组成。

（1）校验数位（Check Digit）：防止其他用户创建或猜测这个句柄。

（2）序号（Sequence Number）：用来确定句柄是由当前还是以前的主服务器创建的。

（3）模式信息（Mode Information）：用于新的主服务器重新创建一个旧的句柄。

在实际的执行中，为了避免所有的通信都使用序号带来的系统开销增长，Chubby引入了sequencer的概念。sequencer实际上就是一个序号，只能由锁的持有者在获取锁时向系统发出请求来获得。这样一来Chubby系统中只有涉及锁的操作才需要序号，其他一概不用。在文件操作中，用户可以将句柄看做一个指向文件系统的指针。这个指针支持一系列的操作，常用的句柄函数及其作用如表2-1所示。

表2-1 常用的句柄函数及其作用

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/46_1.jpg)

#### 2.3.5 通信协议

客户端和主服务器之间的通信是通过KeepAlive握手协议来维持的，这一通信过程的简单示意图如图2-10[13]所示。

图2-10中，从左到右的水平方向表示时间在增加，斜向上的箭头表示一次KeepAlive请求，斜向下的箭头则是主服务器的一次回应。M1、M2、M3表示不同的主服务器租约期。C1、C2、C3则是客户端对主服务器租约期时长做出的一个估计。KeepAlive是周期发送的一种信息，它主要有两方面的功能：延迟租约的有效期和携带事件信息告诉用户更新。主要的事件包括文件内容被修改、子节点的增加、删除和修改、主服务器出错、句柄失效等。正常情况下，通过KeepAlive握手协议租约期会得到延长，事件也会及时地通知给用户。但是由于系统有一定的失效概率，引入故障处理措施是很有必要的。通常情况下系统可能会出现两种故障：客户端租约期过期和主服务器故障，对于这两种情况系统有着不同的应对方式。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/47_1.jpg)

图2-10 Chubby客户端与服务器端的通信过程

**1. 客户端租约过期：**

刚开始时，客户端向主服务器发出一个KeepAlive请求（见图2-10中的1），如果有需要通知的事件时则主服务器会立刻做出回应，否则主服务器并不立刻对这个请求做出回应，而是等到客户端的租约期C1快结束的时候才做出回应（见图2-10中的2），并更新主服务器租约期为M2。客户端在接到这个回应后认为该主服务器仍处于活跃状态，于是将租约期更新为C2并立刻发出新的KeepAlive请求（见图2-10中的3）。同样地，主服务器可能不是立刻回应而是等待C2接近结束，但是在这个过程中主服务器出现故障停止使用。在等待了一段时间后C2到期，由于并没有收到主服务器的回应，系统向客户端发出一个危险（Jeopardy）事件，客户端清空并暂时停用自己的缓存，从而进入一个称为宽限期（Grace Period）的危险状态。这个宽限期默认是45秒。在宽限期内，客户端不会立刻断开其与服务器端的联系，而是不断地做探询。图2-10中新的主服务器很快被重新选出，当它接到客户端的第一个KeepAlive请求（见图2-10中的4）时会拒绝（见图2-10中的5），因为这个请求的纪元号（Epoch Number）错误。不同主服务器的纪元号不相同，客户端的每次请求都需要这个号来保证处理的请求是针对当前的主服务器。客户端在主服务器拒绝之后会使用新的纪元号来发送KeepAlive请求（见图2-10中的6）。新的主服务器接受这个请求并立刻做出回应（见图2-10中的7）。如果客户端接收到这个回应的时间仍处于宽限期内，系统会恢复到安全状态，租约期更新为C3。如果在宽限期未接到主服务器的相关回应，客户端终止当前的会话。

**2. 主服务器出错：**

在客户端和主服务器端进行通信时可能会遇到主服务器故障，图2-10就出现了这种情况。正常情况下旧的主服务器出现故障后系统会很快地选举出新的主服务器，新选举的主服务器在完全运行前需要经历以下九个步骤[13]。

（1）产生一个新的纪元号以便今后客户端通信时使用，这能保证当前的主服务器不必处理针对旧的主服务器的请求。

（2）只处理主服务器位置相关的信息，不处理会话相关的信息。

（3）构建处理会话和锁所需的内部数据结构。

（4）允许客户端发送KeepAlive请求，不处理其他会话相关的信息。

（5）向每个会话发送一个故障事件，促使所有的客户端清空缓存。

（6）等待直到所有的会话都收到故障事件或会话终止。

（7）开始允许执行所有的操作。

（8）如果客户端使用了旧的句柄则需要为其重新构建新的句柄。

（9）一定时间段后（1分钟），删除没有被打开过的临时文件夹。

如果这一过程在宽限期内顺利完成，则用户不会感觉到任何故障的发生，也就是说新旧主服务器的替换对于用户来说是透明的，用户感觉到的仅仅是一个延迟。使用宽限期的好处正是如此。

在系统实现时，Chubby还使用了一致性客户端缓存（Consistent Client-Side Caching）技术，这样做的目的是减少通信压力，降低通信频率。在客户端保存一个和单元上数据一致的本地缓存，需要时客户可以直接从缓存中取出数据而不用再和主服务器通信。当某个文件数据或者元数据需要修改时，主服务器首先将这个修改阻塞；然后通过查询主服务器自身维护的一个缓存表，向对修改的数据进行了缓存的所有客户端发送一个无效标志（Invalidation）；客户端收到这个无效标志后会返回一个确认（Acknowledge），主服务器在收到所有的确认后才解除阻塞并完成这次修改。这个过程的执行效率非常高，仅仅需要发送一次无效标志即可，因为对于没有返回确认的节点，主服务器直接认为其是未缓存的。

#### 2.3.6 正确性与性能

**1. 一致性：**

前面提到过每个Chubby单元是由五个副本组成的，这五个副本中需要选举产生一个主服务器，这种选举本质上就是一个一致性问题。在实际的执行过程中，Chubby使用Paxos算法来解决这个问题。

主服务器产生后客户端的所有读写操作都是由主服务器来完成的。读操作很简单，客户直接从主服务器上读取所需数据即可，但是写操作就会涉及数据一致性的问题。为了保证客户的写操作能够同步到所有的服务器上，系统再次利用了Paxos算法。因此，可以看出Paxos算法在分布式一致性问题中的作用是巨大的。

**2. 安全性：**

Chubby采用的是ACL形式的安全保障措施。系统中有三种ACL名[13]，分别是写ACL名（Write ACL Name）、读ACL名（Read ACL Name）和变更ACL名（Change ACL Name）。只要不被覆写，子节点都是直接继承父节点的ACL名。ACL同样被保存在文件中，它是节点元数据的一部分，用户在进行相关操作时首先需要通过ACL来获取相应的授权。图2-11是一个用户成功写文件所需经历的过程。

用户chinacloud提出向文件CLOUD中写入内容的请求。CLOUD首先读取自身的写ACL名fun，接着在fun中查到了chinacloud这一行记录，于是返回信息允许chinacloud对文件进行写操作，此时chinacloud才被允许向CLOUD写入内容。其他的操作和写操作类似。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/49_1.jpg)

图2-11 Chubby的ACL机制

**3. 性能优化：**

为了满足系统的高可扩展性，Chubby目前已经采取了一些措施[13]。比如提高主服务器默认的租约期、使用协议转换服务将Chubby协议转换成较简单的协议、客户端一致性缓存等。除此之外，Google的工程师们还考虑使用代理（Proxy）和分区（Partition）技术，虽然目前这两种技术并没有实际使用，但是在设计时还是被包含进系统，不排除将来使用的可能。代理可以减少主服务器处理KeepAlive以及读请求带来的服务器负载，但是它并不能减少写操作带来的通信量。Google自己的数据统计表明，在所有的请求中，写请求仅占极少的一部分，几乎可以忽略不计。使用分区技术的话可以将一个单元的命名空间（Name Space）划分成*N*份。除了少量的跨分区通信外，大部分的分区都可以独自地处理服务请求。通过分区可以减少各个分区上的读写通信量，但不能减少KeepAlive请求的通信量。因此，如果需要的话，将代理和分区技术结合起来使用才可以明显提高系统同时处理的服务请求量。

### 2.4 分布式结构化数据表Bigtable

Bigtable是Google开发的基于GFS和Chubby的分布式存储系统。Google的很多数据，包括Web索引、卫星图像数据等在内的海量结构化和半结构化数据，都存储在Bigtable中。从实现上看，Bigtable并没有什么全新的技术，但是如何选择合适的技术并将这些技术高效、巧妙地结合在一起恰恰是最大的难点。Bigtable在很多方面和数据库类似，但它并不是真正意义上的数据库。通过本节的学习，读者将会对Bigtable的数据模型、系统架构、实现以及它使用的一些数据库技术有一个全面的认识。

#### 2.4.1 设计动机与目标

Google设计Bigtable的动机主要有如下三个方面。

（1）需要存储的数据种类繁多。Google目前向公众开放的服务很多，需要处理的数据类型也非常多。包括URL、网页内容、用户的个性化设置在内的数据都是Google需要经常处理的。

（2）海量的服务请求。Google运行着目前世界上最繁忙的系统，它每时每刻处理的客户服务请求数量是普通的系统根本无法承受的。

（3）商用数据库无法满足Google的需求。一方面现有商用数据库的设计着眼点在于其通用性，根本无法满足Google的苛刻服务要求，而且在数量庞大的服务器上根本无法成功部署普通的商用数据库。另一方面对于底层系统的完全掌控会给后期的系统维护、升级带来极大的便利。

在仔细考察了Google的日常需求后，Bigtable开发团队确定Bigtable设计应达到如下几个基本目标。

（1）广泛的适用性。Bigtable是为了满足一系列Google产品而并非特定产品的存储要求。

（2）很强的可扩展性。根据需要随时可以加入或撤销服务器。

（3）高可用性。对于客户来说，有时候即使短暂的服务中断也是不能忍受的。Bigtable设计的重要目标之一就是确保几乎所有的情况下系统都可用。

（4）简单性。底层系统的简单性既可以减少系统出错的概率，也为上层应用的开发带来便利。

在目标确定之后，Google希望巧妙地结合各种数据库技术，扬长避短。最终实现的系统也确实达到了原定的目标。下面详细讲解Bigtable。

#### 2.4.2 数据模型

Bigtable是一个分布式多维映射表，表中的数据通过一个行关键字（Row Key）、一个列关键字（Column Key）以及一个时间戳（Time Stamp）进行索引。Bigtable对存储在其中的数据不做任何解析，一律看做字符串，具体数据结构的实现需要用户自行处理。Bigtable的存储逻辑可以表示为：

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/50_1.jpg)

Bigtable数据的存储格式如图2-12所示[8]。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/50_2.jpg)

图2-12 Bigtable数据的存储格式

**1. 行：**

Bigtable的行关键字可以是任意的字符串，但是大小不能够超过64KB。Bigtable和传统的关系型数据库有很大不同，它不支持一般意义上的事务，但能保证对于行的读写操作具有原子性（Atomic）。表中数据都是根据行关键字进行排序的，排序使用的是词典序。图2-12是Bigtable数据模型的一个典型实例，其中com.cnn.www就是一个行关键字。不直接存储网页地址而将其倒排是Bigtable的一个巧妙设计。这样做至少会带来以下两个好处。

（1）同一地址域的网页会被存储在表中的连续位置，有利于用户查找和分析。

（2）倒排便于数据压缩，可以大幅提高压缩率。

由于规模问题，单个的大表不利于数据的处理，因此Bigtable将一个表分成了很多子表（Tablet），每个子表包含多个行。子表是Bigtable中数据划分和负载均衡的基本单位。有关子表的内容在2.4.5节详细讲解。

**2. 列：**

Bigtable并不是简单地存储所有的列关键字，而是将其组织成所谓的列族（Column Family），每个族中的数据都属于同一个类型，并且同族的数据会被压缩在一起保存。引入了列族的概念之后，列关键字就采用下述的语法规则来定义：

族名：限定词（family：qualifier）

族名必须有意义，限定词则可以任意选定。在图2-12中，内容（Contents）、锚点（Anchor，就是HTML中的链接）都是不同的族。而cnnsi.com和my.look.ca则是锚点族中不同的限定词。通过这种方式组织的数据结构清晰明了，含义也很清楚。族同时也是Bigtable中访问控制（Access Control）的基本单元，也就是说访问权限的设置是在族这一级别上进行的。

**3. 时间戳：**

Google的很多服务比如网页检索和用户的个性化设置等都需要保存不同时间的数据，这些不同的数据版本必须通过时间戳来区分。图2-12中内容列的t3、t5和t6表明其中保存了在t3、t5和t6这三个时间获取的网页。Bigtable中的时间戳是64位整型数，具体的赋值方式可以采取系统默认的方式，也可以用户自行定义。

为了简化不同版本的数据管理，Bigtable目前提供了两种设置：一种是保留最近的*N*个不同版本，图2-12中数据模型采取的就是这种方法，它保存最新的三个版本数据。另一种是保留限定时间内的所有不同版本，比如可以保存最近10天的所有不同版本数据。失效的版本将会由Bigtable的垃圾回收机制自动处理。

#### 2.4.3 系统架构

Bigtable是在Google的另外三个云计算组件基础之上构建的，其基本架构如图2-13所示[11]。

图中WorkQueue是一个分布式的任务调度器，它主要被用来处理分布式系统队列分组和任务调度，关于其实现Google并没有公开。在前面已经讲过，GFS[9]是Google的分布式文件系统，在Bigtable中GFS主要用来存储子表数据以及一些日志文件。Bigtable还需要一个锁服务的支持，Bigtable选用了Google自己开发的分布式锁服务Chubby。在Bigtable中Chubby主要有以下几个作用[10]。

（1）选取并保证同一时间内只有一个主服务器（Master Server）。

（2）获取子表的位置信息。

（3）保存Bigtable的模式信息及访问控制列表。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/52_1.jpg)

图2-13 Bigtable基本架构

另外在Bigtable的实际执行过程中，Google的MapReduce和Sawzall也被用来改善其性能，不过需要注意的是这两个组件并不是实现Bigtable所必需的。

Bigtable主要由三个部分组成：客户端程序库（Client Library）、一个主服务器（Master Server）和多个子表服务器（Tablet Server），这三个部分在图2-13中都有相应的表示。从图2-13可以看出，客户访问Bigtable服务时，首先要利用其库函数执行Open（）操作来打开一个锁（实际上就是获取了文件目录），锁打开以后客户端就可以和子表服务器进行通信了。和许多具有单个主节点的分布式系统一样，客户端主要与子表服务器通信，几乎不和主服务器进行通信，这使得主服务器的负载大大降低。主服务主要进行一些元数据的操作以及子表服务器之间的负载调度问题，实际的数据是存储在子表服务器上的。

#### 2.4.4 主服务器

主服务器的主要作用如图2-14所示。

当一个新的子表产生时，主服务器通过一个加载命令将其分配给一个空间足够的子表服务器。创建新表、表合并以及较大子表的分裂都会产生一个或多个新子表。对于前面两种，主服务器会自动检测到，因为这两个操作是由主服务器发起的，而较大子表的分裂是由子服务发起并完成的，所以主服务器并不能自动检测到，因此在分割完成之后子服务器需要向主服务发出一个通知。由于系统设计之初就要求能达到良好的扩展性，所以主服务器必须对子表服务器的状态进行监控，以便及时检测到服务器的加入或撤销。Bigtable中主服务器对子表服务器的监控是通过Chubby完成的，子表服务器在初始化时都会从Chubby中得到一个独占锁。通过这种方式所有的子表服务器基本信息被保存在Chubby中一个称为服务器目录（Server Directory）的特殊目录之中。主服务器通过检测这个目录可以随时获取最新的子表服务器信息，包括目前活跃的子表服务器，以及每个子表服务器上现已分配的子表。对于每个具体的子表服务器，主服务器会定期向其询问独占锁的状态。如果子表服务器的锁丢失或没有回应，则此时可能有两种情况，要么是Chubby出现了问题（虽然这种概率很小，但的确存在，Google自己也做过相关测试），要么是子表服务器自身出现了问题。对此主服务器首先自己尝试获取这个独占锁，如果失败说明Chubby服务出现问题，需等待Chubby服务的恢复。如果成功则说明Chubby服务良好而子表服务器本身出现了问题。这种情况下主服务器会中止这个子表服务器并将其上的子表全部移至其他子表服务器。当在状态监测时发现某个子表服务器上负载过重时，主服务器会自动对其进行负载均衡操作。

基于系统出现故障是一种常态的设计理念（Google几乎所有的产品都是基于这个设计理念），每个主服务器被设定了一个会话时间的限制。当某个主服务器到时退出后，管理系统就会指定一个新的主服务器，这个主服务器的启动需要经历以下四个步骤[8]。

（1）从Chubby中获取一个独占锁，确保同一时间只有一个主服务器。

（2）扫描服务器目录，发现目前活跃的子表服务器。

（3）与所有的活跃子表服务器取得联系以便了解所有子表的分配情况。

（4）通过扫描元数据表（Metadata Table），发现未分配的子表并将其分配到合适的子表服务器。如果元数据表未分配，则首先需要将根子表（Root Tablet）加入未分配的子表中。由于根子表保存了其他所有元数据子表的信息，确保了扫描能够发现所有未分配的子表。

在成功完成以上四个步骤后主服务器就可以正常运行了。

#### 2.4.5 子表服务器

Bigtable中实际的数据都是以子表的形式保存在子表服务器上的，客户一般也只和子表服务器进行通信，所以子表以及子表服务器是我们重点讲解的概念。子表服务器上的操作主要涉及子表的定位、分配以及子表数据的最终存储问题。其中子表分配在前面已经有了详细介绍，这里略过不讲。在讲解其他问题之前我们首先介绍一下SSTable的概念以及子表的基本结构。

**1. SSTable及子表基本结构：**

SSTable是Google为Bigtable设计的内部数据存储格式。所有的SSTable文件都存储在GFS上，用户可以通过键来查询相应的值，图2-15是SSTable格式的基本示意。

SSTable中的数据被划分成一个个的块（Block），每个块的大小是可以设置的，一般来说设置为64 KB。在SSTable的结尾有一个索引（Index），这个索引保存了SSTable中块的位置信息，在SSTable打开时这个索引会被加载进内存，这样用户在查找某个块时首先在内存中查找块的位置信息，然后在硬盘上直接找到这个块，这种查找方法速度非常快。由于每个SSTable一般都不是很大，用户还可以选择将其整体加载进内存，这样查找起来会更快。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/54_1.jpg)

图2-15 SSTable格式的基本示意

从概念上讲子表是表中一系列行的集合，它在系统中的实际组成如图2-16所示。

每个子表都是由多个SSTable以及日志（Log）文件构成。有一点需要注意，那就是不同子表的SSTable可以共享，也就是说某些SSTable会参与多个子表的构成，而由子表构成的表则不存在子表重叠的现象。Bigtable中的日志文件是一种共享日志，也就是说系统并不是对子表服务器上每个子表都单独地建立一个日志文件，每个子表服务器上仅保存一个日志文件，某个子表日志只是这个共享日志的一个片段。这样会节省大量的空间，但在恢复时却有一定的难度，因为不同的子表可能会被分配到不同的子表服务器上，一般情况下每个子表服务器都需要读取整个共享日志来获取其对应的子表日志。Google为了避免这种情况出现，对日志做了一些改进。Bigtable规定将日志的内容按照键值进行排序，这样不同的子表服务器都可以连续读取日志文件了。一般来说每个子表的大小在100MB到200MB之间。每个子表服务器上保存的子表数量可以从几十到上千不等，通常情况下是100个左右。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/54_2.jpg)

图2-16 子表实际组成

**2. 子表地址：**

子表地址的查询是经常碰到的操作。在Bigtable系统的内部采用的是一种类似B+树的三层查询体系。子表地址结构如图2-17所示[8]。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/55_1.jpg)

图2-17 子表地址结构

所有的子表地址都被记录在元数据表中，元数据表也是由一个个的元数据子表（Metadata Tablet）组成的。根子表是元数据表中一个比较特殊的子表，它既是元数据表的第一条记录，也包含了其他元数据子表的地址，同时Chubby中的一个文件也存储了这个根子表的信息。这样在查询时，首先从Chubby中提取这个根子表的地址，进而读取所需的元数据子表的位置，最后就可以从元数据子表中找到待查询的子表。除了这些子表的元数据之外，元数据表中还保存了其他一些有利于调试和分析的信息，比如事件日志等。

为了减少访问开销，提高客户访问效率，Bigtable使用了缓存（Cache）和预取（Prefetch）技术，这两种技术手段在体系结构设计中是很常用的。子表的地址信息被缓存在客户端，客户在寻址时直接根据缓存信息进行查找。一旦出现缓存为空或缓存信息过时的情况，客户端就需要按照图2-17所示方式进行网络的来回通信（Network Round-trips）进行寻址，在缓存为空的情况下需要三个网络来回通信。如果缓存的信息是过时的，则需要六个网络来回通信。其中三个用来确定信息是过时的，另外三个获取新的地址。预取则是在每次访问元数据表时不仅仅读取所需的子表元数据，而是读取多个子表的元数据，这样下次需要时就不用再次访问元数据表。

**3. 子表数据存储及读/写操作：**

在数据的存储方面Bigtable做出了一个非常重要的选择，那就是将数据存储划分成两块。较新的数据存储在内存中一个称为内存表（Memtable）的有序缓冲里，较早的数据则以SSTable格式保存在GFS中。这种技术在数据库中不是很常用，但Google还是做出了这种选择，实际运行的效果也证明Google的选择虽然大胆却是正确的。

从图2-18[8]中可以看出读和写操作有很大的差异性。做写操作（Write Op）时，首先查询Chubby中保存的访问控制列表确定用户具有相应的写权限，通过认证之后写入的数据首先被保存在提交日志（Commit Log）中。提交日志中以重做记录（Redo Record）的形式保存着最近的一系列数据更改，这些重做记录在子表进行恢复时可以向系统提供已完成的更改信息。数据成功提交之后就被写入内存表中。在做读操作（Read Op）时，首先还是要通过认证，之后读操作就要结合内存表和SSTable文件来进行，因为内存表和SSTable中都保存了数据。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/56_1.jpg)

图2-18 Bigtable数据存储及读/写操作

在数据存储中还有一个重要问题，就是数据压缩的问题。内存表的空间毕竟是很有限的，当其容量达到一个阈值时，旧的内存表就会被停止使用并压缩成SSTable格式的文件。在Bigtable中有三种形式的数据压缩，分别是次压缩（Minor Compaction）、合并压缩（Merging Compaction）和主压缩（Major Compaction）。三者之间的关系如图2-19所示。

每一次旧的内存表停止使用时都会进行一个次压缩操作，这会产生一个SSTable。但如果系统中只有这种压缩的话，SSTable的数量就会无限制地增加下去。由于读操作要使用SSTable，数量过多的SSTable显然会影响读的速度。而在Bigtable中，读操作实际上比写操作更重要，因此Bigtable会定期地执行一次合并压缩的操作，将一些已有的SSTable和现有的内存表一并进行一次压缩。主压缩其实是合并压缩的一种，只不过它将所有的SSTable一次性压缩成一个大的SSTable文件。主压缩也是定期执行的，执行一次主压缩之后可以保证将所有的被压缩数据彻底删除，如此一来，既回收了空间又能保证敏感数据的安全性（因为这些敏感数据被彻底删除了）。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/56_2.jpg)

图2-19 三种形式压缩之间的关系

#### 2.4.6 性能优化

**1. 局部性群组（Locality groups）：**

Bigtable允许用户将原本并不存储在一起的数据以列族为单位，根据需要组织在一个单独的SSTable中，以构成一个局部性群组。这实际上就是数据库中垂直分区技术的一个应用。结合图2-13的实例来看，在被Bigtable保存的网页列关键字中，有的用户可能只对网页内容感兴趣，那么它可以通过设置局部性群组只看内容这一列。有的则会对诸如网页语言、网站排名等可以用于分析的信息比较感兴趣，他也可以将这些列设置到一个群组中。局部性群组如图2-20所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/57_1.jpg)

图2-20 局部性群组

通过设置局部性群组用户可以只看自己感兴趣的内容，对某个用户来说的大量无用信息无须读取。对于一些较小的且会被经常读取的局部性群组，用户可以将其SSTable文件直接加载进内存，这可以明显地改善读取效率。

**2. 压缩：**

压缩可以有效地节省空间，Bigtable中的压缩被应用于很多场合。首先压缩可以被用在构成局部性群组的SSTable中，可以选择是否对个人的局部性群组的SSTable进行压缩。Bigtable中这种压缩是对每个局部性群组独立进行的，虽然这样会浪费一些空间，但是在需要读时解压速度非常快。通常情况下，用户可以采用两步压缩的方式[8]：第一步利用Bentley&McIlroy方式（BMDiff）在大的扫描窗口将常见的长串进行压缩；第二步采取Zippy技术进行快速压缩，它在一个16KB大小的扫描窗口内寻找重复数据，这个过程非常快。压缩技术还可以提高子表的恢复速度，当某个子表服务器停止使用后，需要将上面所有的子表移至另一个子表服务器来恢复服务。在转移之前要进行两次压缩，第一次压缩减少了提交日志中的未压缩状态，从而减少了恢复时间。在文件正式转移之前还要进行一次压缩，这次压缩主要是将第一次压缩后遗留的未压缩空间进行压缩。完成这两步之后压缩的文件就会被转移至另一个子表服务器。

**3. 布隆过滤器：**

Bigtable向用户提供了一种称为布隆过滤器[12]的数学工具。布隆过滤器是巴顿·布隆在1970年提出的，实际上它是一个很长的二进制向量和一系列随机映射函数，在读操作中确定子表的位置时非常有用。布隆过滤器的速度快，省空间。而且它有一个最大的好处是它绝不会将一个存在的子表判定为不存在。不过布隆过滤器也有一个缺点，那就是在某些情况下它会将不存在的子表判断为存在。不过这种情况出现的概率非常小，跟它带来的巨大好处相比这个缺点是可以忍受的。

目前包括Google Analytics、Google Earth、个性化搜索、Orkut和RRS阅读器在内的几十个项目都使用了Bigtable。这些应用对Bigtable的要求以及使用的集群机器数量都是各不相同的，但是从实际运行来看，Bigtable完全可以满足这些不同需求的应用，而这一切都得益于其优良的构架以及恰当的技术选择。与此同时Google还在不断地对Bigtable进行一系列的改进，通过技术改良和新特性的加入提高系统运行效率及稳定性。

### 2.5 分布式存储系统Megastore

互联网的迅速发展带来了新的数据应用场景，和传统的数据存储有别的是，互联网上的应用对于数据的可用性和系统的扩展性具有很高的要求。一般的互联网应用都要求能够做到7天×24小时的不间断服务，达不到的话则会带来较差的用户体验。热门的应用往往会在短时间内经历急剧的用户数量增长，这就要求系统具有良好的可扩展性。在互联网的应用中，为了达到好的可扩展性，常常会采用NoSQL存储方式。但是从应用程序的构建方面来看，传统的关系型数据库又有着NoSQL所不具备的优势。Google设计和构建了用于互联网中交互式服务的分布式存储系统Megastore，该系统成功的将关系型数据库和NoSQL的特点与优势进行了融合。本节将向大家介绍该系统，着重突出Megastore设计与构建过程中的核心思想和技术。

#### 2.5.1 设计目标及方案选择

Megastore的设计目标很明确，那就是设计一种介于传统的关系型数据库和NoSQL之间的存储技术，尽可能达到高可用性和高可扩展性的统一。为了达到这一目标，设计团队采用了如下的两种方法：

（1）针对可用性的要求，实现了一个同步的、容错的、适合远距离传输的复制机制。在方案的选择和实现过程中Megastore团队研究和比较了一些传统的远距离复制技术，最终确定了引入Paxos算法并对其做出一定的改进以满足远距离同步复制的要求。具体的实现将在2.5.5节介绍。

（2）针对可扩展性的要求，设计团队借鉴了数据库中数据分区的思想，将整个大的数据分割成很多小的数据分区，每个数据分区连同它自身的日志存放在NoSQL数据库中，具体来说就是存放在Bigtable中。

图2-21[17]显示了数据的分区和复制。在Megastore中，这些小的数据分区被称为实体组集（Entity Groups）。每个实体组集包含若干的实体组（Entity Group，相当于分区中表的概念），而一个实体组中又包含很多的实体（Entity，相当于表中记录的概念）。从图中还可以看出单个实体组支持ACID语义，以上这些都体现了关系型数据库的特征。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/59_1.jpg)

图2-21 数据的分区和复制

实体组集之间只具有比较松散的一致性。每个实体组都通过复制技术在数据中心中保存若干数据副本，这些实体组及其副本都存储在NoSQL数据库（Bigtable）中。

#### 2.5.2 Megastore数据模型

传统的关系型数据库是通过连接（join）来满足用户的需求的，但是就Megastore而言，这种数据模型是不合适的，主要有以下三个原因。

（1）对于高负载的交互式应用来说，可预期的性能提升要比使用一种代价高昂的查询语言所带来的好处多。

（2）Megastore所面对的应用是读远多于写的，因此好的选择是将读操作所需要做的工作尽可能地转移到写操作上。

（3）在Bigtable这样的键/值存储系统中存储和查询级联数据（Hierarchical Data）是很方便的。

基于上述三点考虑，Google团队设计了一种能够提供细粒度控制的数据模型和模式语言。Megastore中关系型数据库的特征就集中体现在这种数据模型。同关系型数据库一样，Megastore的数据模型是在模式（schema）中定义的且是强类型的（strongly typed）。每个模式都由一系列的表（tables）构成，表又包含有一系列的实体（entities），每个实体中又包含一系列的属性（properties）。属性是命名的且具有类型，这些类型包括字符型（strings）、数字类型（numbers）或者Google的Protocol Buffers。这些属性可以被设置成必需的（required）、可选的（optional）或者可重复的（repeated，即允许单个属性上有多个值）。图2-22[17]是Megastore中一个照片共享服务的数据模型实例。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/60_1.jpg)

图2-22 照片共享服务的数据模型实例

从图中可以很容易地发现，这种模式定义的方式和关系型数据库中的定义方法非常的类似。在Megastore中，所有的表要么是实体组根表（Entity Group Root Table），要么是子表（Child Table）。所有的子表必须有一个参照根表的外键，这个外键是通过ENTITY GROUP KEY来声明的。图2-22[17]中表Photo就是一个子表，因为它声明了一个外键，User则是一个根表。一个Megastore实例中可以有若干个不同的根表，表示不同类型的实体组集。

图2-22中的实例还可以看到三种不同的属性设置，既有必需的（如user_id），也有可选的（如thumbnail_url）。值得注意的是Photo中的可重复类型的tag属性，这也就意味着一个Photo中允许同时出现多个tag属性。

Megastore数据模型中另一个非常重要的概念——索引（Index）也在图2-22中得到体现。Megastore将索引分成了两大类：局部索引（Local Index）和全局索引（Global Index）。局部索引定义在单个实体组中，它的作用域仅限于单个实体组。全局索引则可以横跨多个实体组集进行数据读取操作。图2-22中PhotosByTime就是一个局部索引，而PhotosByTag则是一个全局索引。除了这两大类的索引外，Megastore还提供了一些额外的索引特性，主要包括以下几个。

（1）STORING子句（STORING Clause）：通过在索引中增加STORING子句，应用程序可以存储一些额外的属性，这样在读取数据时可以更快地从基本表中得到所需内容。PhotosByTag这样一个索引中就对thumbnail_url使用了STORING子句。

（2）可重复的索引（Repeated Indexes）：Megastore提供了对可重复属性建立索引的能力，这种可重复的索引对于子表来说常常是很有效的。

（3）内联索引（Inline Indexes）：任何一个有外键的表都能够创建一个内联索引。内联索引能够有效的从子实体中提取出信息片段并将这些片段存储在父实体中，以此加快读取速度。

最后简单地了解在这种数据模型下数据是如何存储在Bigtable中的。Megastore中的实体组都存储在Bigtable中，表2-2[17]列出了上面照片共享服务实例的数据在Bigtable中的存储情况。

表2-2 在数据Bigtable中的存储情况

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/61_1.jpg)

从表中不难看出，Bigtable的列名实际上是表名和属性名结合在一起得到的。不同表中的实体可以存储在同一个Bigtable行中。

#### 2.5.3 Megastore中的事务及并发控制

每个实体组实际上就像一个小的数据库，在实体组内部提供了完整的序列化ACID语义（Serializable ACID Semantics）支持。

Megastore提供了三种方式的读，分别是current、snapshot和inconsistent。其中current读和snapshot读总是在单个实体组中完成的。在开始某次current读之前，需要确保所有已提交的写操作已经全部生效，然后应用程序再从最后一个成功提交的事务时间戳位置读取数据。对于snapshot读，系统取出已知的最后一个完整提交的事务的时间戳，接着从这个位置读数据。和current读不同的是，snapshot读的时候可能还有部分事务提交了但未生效。inconsistent读忽略日志的状态直接读取最新的值。这对于那些要求低延迟并能容忍数据过期或不完整的读操作是非常有用的。

Megastore事务中的写操作采用了预写式日志（Write-ahead Log），也就是说只有当所有的操作都在日志中记录下后写操作才会对数据执行修改。一个写事务总是开始于一个current读以便确认下一个可用的日志位置。提交操作将数据变更聚集到日志，接着分配一个比之前任意一个都高的时间戳，然后使用Paxos将数据变更加入到日志中。这个协议使用了乐观并发（Optimistic Concurrency）：尽管可能有多个写操作同时试图写同一个日志位置，但只会有1个成功。所有失败的写都会观察到成功的写操作，然后中止并重试它们的操作。

一个完整的事务周期要经过如下几个阶段。

（1）读：获取最后一次提交的事务的时间戳和日志位置。

（2）应用逻辑：从Bigtable读取并且聚集数据到日志入口。

（3）提交：使用Paxos达到一致，将这个入口追加到日志。

（4）生效：将数据更新到Bigtable中的实体和索引。

（5）清除：清理不再需要的数据。

Megastore中事务间的消息传递是通过队列（Queue）实现的，图2-23[17]显示了这一过程。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/62_1.jpg)

图2-23 Megastore中的事务机制

Megastore中的消息能够横跨实体组，在一个事务中分批执行多个更新或者延缓作业（Defer Work）。在单个实体组上执行的事务除了更新它自己的实体外，还能够发送或收到多个信息。每个消息都有一个发送和接收的实体组；如果这两个实体组是不同的，那么传输将会是异步的。虽然这种消息队列机制在关系型数据库中已经有了很长的应用历史，Megastore实现的这种消息机制的最大特点在于其规模：声明一个队列后可以在其他所有的实体组上创建一个收件箱。

除了队列机制之外，Megastore还支持两阶段提交（Two-phase Commit）。但是这会产生比较高的延迟并且增加了竞争的风险，一般情况下不鼓励使用。

#### 2.5.4 Megastore基本架构

图2-24[17]是Megastore的基本架构，最底层的数据是存储在Bigtable中的。不同类型的副本存储不同的数据。在Megastore中共有三种副本，分别是完整副本（Full Replica）、见证者副本（Witness Replica）和只读副本（Read-only Replica）。图2-24中出现了两种副本，分别是完整副本A和副本B，以及见证者副本C。对于完整副本，Bigtable中存储完整的日志和数据。见证者副本的作用是在Paxos算法执行过程中无法产生一个决议时参与投票，因此对于这种副本，Bigtable只存储其日志而不存储具体数据。最后一种只读副本和见证者副本恰恰相反，它们无法参与投票。它们的作用只是读取到最近过去某一个时间点的一致性数据。如果读操作能够容忍这些过期数据，只读副本能够在不加剧写延迟的情况下将数据在较大的地理空间上进行传输。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/63_1.jpg)

图2-24 Megastore的基本架构

Megastore的部署需要通过一个客户端函数库和若干的服务器。应用程序连接到这个客户端函数库，这个函数库执行Paxos算法。图2-24中还有一个称为协调者的服务，要想理解这个服务的作用，首先来了解下Megastore中提供的快速读（Fast Reads）和快速写（Fast Writes）机制。

**1. 快速读：**

如果读操作不需要副本之间进行通信即可完成，那么读取的效率必然相对较高。由于写操作基本上能在所有的副本上成功，一旦成功认为该副本上的数据都是相同的且是最新的，就能利用本地读取（Local Reads）实现快速读，能够带来更好的用户体验及更低的延迟。确保快速读成功的关键是保证选择的副本上数据是最新的。为了达到这一目标，设计团队引入了协调者的概念。协调者是一个服务，该服务分布在每个副本的数据中心里面。它的主要作用就是跟踪一个实体组集合，集合中的实体组需要具备的条件就是它们的副本已经观察到了所有的Paxos写。只要出现在这个集合中的实体组，它们的副本就都能够进行本地读取，也就是说能够实现快速读。协调者的状态是由写算法来保证，关于这点将在2.5.5节中再次介绍。

**2. 快速写：**

为了达到快速的单次交互的写操作，Megastore采用了一种在主/从式系统中常用的优化方法。如果一次写成功，那么下一次写的时候就跳过准备过程，直接进入接受阶段。因为一次成功的写意味着也准确地获知了下一个日志的位置，所以不再需要准备阶段。Megastore没有使用专门的主服务器，而是使用leaders。系统在每一个日志位置都运行一个Paxos算法实例。leader主要是来裁决哪个写入的值可以获取0号提议。第一个将值提交给leader的可以获得一个向所有副本请求接收这个值作为0号提议最终值的机会。其他的值就需要重新使用Paxos算法。

由于写入者在提交值给其他副本之前必须要和leader通信，为了尽可能地减少延迟，Megastore做了一个简单的优化，即在提交值最多的位置附近选择一个副本作为leader。

客户端、网络及Bigtable的故障都会导致一个写操作处于不确定的状态。图2-24中的复制服务器会定期扫描未完成的写入并且通过Paxos算法提议没有操作的值（No-op Values）来让写入完成。

#### 2.5.5 核心机制——复制

复制可以说是Megastore最核心的技术，如何实现一个高效、实时的复制方案对于整个系统的性能起着决定性的作用。通过复制保证所有最新的数据都保存有一定数量副本，能够很好地提高系统的可用性。

**1. 复制的日志：**

每个副本都存有记录所有更新的数据。即使是它正从一个之前的故障中恢复数据，副本也要保证其能够参与到写操作中的Paxos算法，因此Megastore允许副本不按顺序接受日志，这些日志将独立的存储在Bigtable中。图2-25[17]是Megastore中预写式日志的一个典型应用场景。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/64_1.jpg)

图2-25 预写式日志

当日志有不完整的前缀时我们就称一个日志副本有“缺失”（Holes）。在图2-25中0～99的日志位置已经被全部清除，100的日志位置被部分清除，因为每个副本都会被通知到其他副本已经不再需要这个日志。101的日志位置被全部副本接受。102的日志位置被γ获得，这是一种有争议的一致性。103的日志位置被副本A和副本C接受，副本B则留下了一个“缺失”。104的日志位置则未达到一致性，因为副本A和副本B存在争议。

**2. 数据读取：**

在一次Current读之前，要保证至少有一个副本上的数据是最新的，也就是说所有之前提交到日志中的更新必须复制到该副本上并确保在该副本上生效。这个过程称为追赶（Catchup）。

图2-26[17]是一次数据读取过程，总的来看，该过程要经过以下几个步骤。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/65_1.jpg)

图2-26 数据读取

1）本地查询（Query Local）

查询本地副本的协调者来决定这个实体组上数据是否已经是最新的。

2）发现位置（Find Position）

确定一个最高的已经提交的日志位置，选择一个已经在该位置上生效的副本。

（1）本地读取（Local Read）：如果本地查询确定当前的本地副本已经是最新的，则从副本中的最高日志位置和时间戳读取数据。这实际上就是前面提到的快速读。

（2）多数派读取（Majority Read）：如果本地副本不是最新的（或者本地查询或本地读取超时），从一个副本的多数派中发现最大的日志位置，然后从中选取一个读取。选择一个响应最快或者最新的副本，并不一定就是本地副本。

3）追赶

一旦某个副本被选中，就采取如下方式使其追赶到已知的最大日志位置处。

（1）对于所选副本中所有不知道共识值（Consensus Value）的日志位置，从其他的副本中读取值。对于任意的没有任何可用的已提交的值的日志位置，将会利用Paxos算法发起一次无操作的写。Paxos将会促使绝大多数副本达成一个共识值——可能是无操作的写也可能是以前的一次写操作。

（2）接下来就所有未生效的日志位置生效成上面达成的共识值，以此来达到一种分布式一致状态。

4）验证（Validate）

如果本地副本被选中且数据不是最新，发送一个验证消息到协调者断定（entity group，replica）对（（entity group，replica）pair）能够反馈所有提交的写操作。无须等待回应，如果请求失败，下一个读操作会重试。

5）查询数据（Query Data）

在所选的副本中利用日志位置的时间戳读取数据。如果所选的副本不可用了，重新选中一个替代副本，执行追赶操作，然后从中读取数据。单个的较大查询结果可能是从多个副本中汇聚而来。

需要指出的是，本地查询和本地读取是并行执行的。

**3. 数据写入：**

执行完一次完整的读操作之后，下一个可用的日志位置、最后一次写操作的时间戳，以及下一次的leader副本都知道了。在提交时刻所有的更新都被打包（Packaged）和提议（Proposed），同时还包含一个时间戳、下一次leader提名及下一个日志位置的共识值。如果该值赢得了分布式共识，它将应用到所有的副本中。否则整个事务将中止且从读操作重新开始。

在2.5.4节中介绍快速读时曾经提到协调者的状态是由写算法来保证的。这实际上描述了这样的一个过程：如果一次写操作不是被所有的副本所接受，必须要将这些未接受写操作的副本中相关的实体组从协调者中移去，这个过程称为失效（Invalidation）。失效的过程可以保证协调者所看到的副本上数据都是接受了写操作的最新数据。在一次写操作被提交并准备生效之前，所有的副本必须选择接受或者在协调者中将有关的实体组进行失效。

图2-27[17]是数据写入的完整过程，具体包括以下几个步骤。

（1）接受leader：请求leader接受值作为0号提议。这实际上就是前面介绍的快速写方法。如果成功，跳至步骤（3）。

（2）准备：在所有的副本上使用一个比其当前所见的日志位置更高的提议号进行Paxos准备阶段。将值替换成拥有最高提议号的那个值。

（3）接受：请求剩余的副本接受该值，如果大多数副本拒绝这个值，返回步骤（2）。

（4）失效：将不接受值的副本上的协调者进行失效操作。

（5）生效：将值的更新在尽可能多的副本上生效。如果选择的值和原来提议的有冲突，返回一个冲突错误。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/67_1.jpg)

图2-27 数据写入

**4. 协调者的可用性：**

从上面的介绍中可以发现协调者在系统中是比较重要的，协调者的进程运行在每个数据中心。每次的写操作中都要涉及协调者，因此协调者的故障将会导致系统的不可用。虽然在实践中由协调者导致的系统不可用的情况很少出现，但是网络和主机故障还是有可能导致协调者出现暂时的不可用。

Megastore使用了Chubby锁服务，协调者在启动的时候从数据中心获取指定的Chubby锁。为了处理请求，一个协调者必须持有其多数锁。一旦因为出现问题导致它丢失了大部分锁，协调者就会恢复到一个默认保守状态——认为所有它所能看见的实体组都是失效的。

写入者通过测试一个协调者是否丢失了锁从而让其在协调者不可用的过程中得到保护。写入者知道在恢复之前协调者会认为自己是失效的。当一个协调者突然不可用时，这个算法需要面对一个短暂（几十秒）的写停顿风险——所有的写入者必须等待协调者的Chubby锁过期。

除了可用性问题，对于协调者的读写协议必须满足一系列的竞争条件。失效的信息总是安全的，但是生效的信息必须谨慎处理。在协调者中较早的写操作生效和较晚的写操作失效之间的竞争通过带有日志位置而被保护起来。较高位置的失效操作总是胜过较低位置的生效操作。一个位置*n*的失效操作和一个位置*m* < *n*的生效操作之间的竞争常常和一个冲突联系在一起。Megastore通过一个唯一的代表协调者的序号来检测冲突：生效操作只允许在最近一次对协调者进行的读取操作以来序号没有发生变化的情况下修改协调者的状态。

在实际的应用中，以下因素能够减轻使用协调者所带来的问题。

（1）协调者比任何的Bigtable 服务器都简单，基本上没有依赖，所以可用性更高。

（2）协调者简单、均匀的工作负载让它们能够低成本地进行预防措施。

（3）协调者轻量的网络传输允许使用高可用连接进行服务质量监控。

（4）操作者能够在维护期或者故障期集中地让一批协调者失效。当出现某些系统默认的监控信号时这一过程会自动进行。

（5）Chubby 锁的quorum机制能够监测到大多数网络问题和节点的不可用。

#### 2.5.6 产品性能及控制措施

本节将介绍Megastore在Google中实际应用的情况及系统出现错误时的一些控制措施。

Megastore在Google中已经部署和使用了若干年，有超过100个产品使用Megastore作为其存储系统。图2-28[17]显示了这些产品可用性的分布情况，从图中可以看出，绝大多数产品具有极高的可用性（>99.999%）。这表明Megastore系统的设计是非常成功的，基本达到了预期目标。

图2-29[17]是产品延迟情况的分布，根据数据中心的距离和写入数据的大小，应用程序的平均读取延迟在万分之一毫秒之内，平均写入延迟在100～400毫秒。

当某个完整副本忽然变得不可用或失去连接时，为了避免Megastore的性能下降，可采取以下三种应对方法。

（1）通过重新选择路由使客户端绕开出现问题的副本，这是最重要的一种错误处理 机制。

（2）将出现问题副本上的协调者禁用，确保问题的影响降至最小。

（3）禁用整个副本，这是最严厉的一种手段，但是这种方法比较少使用。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/68_1.jpg)

图2-28 可用性的分布情况

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/69_1.jpg)

图2-29 产品延迟情况的分布

一般来说，出现错误之后，上述三种方法可能会结合起来使用，而不是仅仅使用某种方法。

本节主要介绍了Megastore的设计思想以及核心的技术手段，其中很多内容对于设计NoSQL存储系统是有借鉴意义的。但需要跟读者指出的是：Megastore已经是Google相对过时的存储技术。Google目前正在使用的存储系统是Spanner架构，Spanner的设计目标是能够控制一百万到一千万台服务器，Spanner最强大之处在于能够在50 ms之内为数据传递提供通道——即使这两个数据中心分布于地球的两端。

### 2.6 大规模分布式系统的监控基础架构Dapper

Google认为系统出现故障是一种常态，基于这种设计理念，Google的工程师们结合Google的实际开发出了Dapper。这是目前所知的第一种公开其实现的大规模分布式系统的监控基础架构。

#### 2.6.1 基本设计目标

Google使用最多的服务就是它的搜索引擎，以此为例，有资料表明，用户的平均每一次前台搜索会导致Google的后台发生1011次的处理。用户将一个关键字通过Google的输入框传到Google的后台，系统再将具体的查询任务分配到很多子系统中，这些子系统有些是用来处理涉及关键字的广告，有些是用来处理图像、视频等搜索的，最后所有这些子系统的搜索结果被汇总在一起返回给用户。在我们看来很简单的一次搜索实际上涉及了众多Google后台子系统，这些子系统的运行状态都需要进行监控，而且随着时间的推移Google的服务越来越多，新的子系统也在不断被加入，因此在设计时需要考虑到的第一个问题就是设计出的监控系统应当能够对尽可能多的Google服务进行监控，即广泛可部署性（Ubiquitous Deployment）。另一方面，Google的服务是全天候的，如果不能对Google 的后台同样进行全天候的监控很可能会错过某些无法再现的关键性故障，因此需要进行不间断的监控。这两个基本要求导致了以下三个基本设计目标。

（1）低开销：这个是广泛可部署性的必然要求。监控系统的开销越低，对于原系统的影响就越小，系统的开发人员也就越愿意接受这个监控系统。

（2）对应用层透明：监控系统对程序员应当是不可见的。如果监控系统的使用需要程序开发人员对其底层的一些细节进行调整才能正常工作的话，这个监控系统肯定不是一个完善的监控系统。

（3）可扩展性：Google的服务增长速度是惊人的，设计出的系统至少在未来几年里要能够满足Google服务和集群的需求。

#### 2.6.3 Dapper监控系统简介

**1. 基本概念：**

对系统行为进行监控的过程非常的复杂，特别是在分布式系统中。为了理解这种复杂性，首先来看如图2-30[18]所示的一个过程。

在图中，用户发出一个请求X，它期待得到系统对它做出的应答X。但是接收到该请求的前端A发现该请求的处理需要涉及服务器B和服务器C，因此A又向B和C发出两个RPC（远程过程调用）。B收到后立刻做出响应，但是C在接到后发现它还需要调用服务器D和E才能完成请求X，因此C对D和E分别发出了RPC，D和E接到后分别做出了应答，收到D和E的应答之后C才向A做出响应，在接收到B和C的应答之后A才对用户请求X做出一个应答X。在监控系统中记录下所有这些消息不难，如何将这些消息记录同特定的请求（本例中的X）关联起来才是分布式监控系统设计中需要解决的关键性问题之一。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/70_1.jpg)

图2-30 典型分布式系统的请求及应答过程

一般来说，有两种方案可供选择：黑盒（Black Box）方案及基于注释的监控（Annotation-based Monitoring）方案。二者比较而言，黑盒方案比较轻便，但是在消息关系判断的过程中，黑盒方案主要是利用一些统计学的知识来进行推断，有时不是很准确。基于注释的方案利用应用程序或中间件给每条记录赋予一个全局性的标示符，借此将相关消息串联起来。考虑到实际的需求，Google的工程师最终选择了基于注释的方案，为了尽可能消除监控系统的应用程序对被监控系统的性能产生的不良影响，Google的工程师设计并实现了一套轻量级的核心功能库，这将在后面进行介绍。

Dapper监控系统中有三个基本概念：监控树（Trace Tree）、区间（Span）和注释（Annotation）。如图2-31[18]所示是一个典型的监控树，从中可以看到所谓的监控树实际上就是一个同特定事件相关的所有消息，只不过这些消息是按照一定的规律以树的形式组织起来。树中的每一个节点称为一个区间，区间实际上就是一条记录，所有这些记录联系在一起就构成了对某个事件的完整监控。从图2-31不难看出，每个区间包括以下内容：区间名（Span Name）、区间id（Span id）、父id（Parent id）和监控id（Trace id）。区间名主要是为了方便人们记忆和理解，因此要求这个区间名是人们可以读懂的。区间id是为了在一棵监控树中区分不同的区间。父id是区间中非常重要的一个内容，正是通过父id才能够对树中不同区间的关系进行重建，没有父id的区间称为根区间（Root Span）。图2-31中的Frontend Request就是一个根区间。在图中还能看出，区间的长度实际上包括了区间的开始及结束时间信息。

监控id在图2-31中并没有列出，一棵监控树中所有区间的监控id是相同的，这个监控id是随机分配的，且在整个Dapper监控系统中是唯一的。正如区间id是用来在某个监控树中区分不同的区间一样，监控id是用来在整个Dapper监控系统中区分不同的监控。注释主要用来辅助推断区间关系，也可以包含一些自定义的内容。图2-32[18]展示了图2-31中区间Helper.Call的详细信息。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/71_1.jpg)

图2-31 监控树

在图2-32中可以清楚地看到这个区间的区间名是“Helper.Call”，监控id是100，区间id是5，父id是3。一个区间既可以只有一台主机的信息，也可以包含来源于多个主机的信息；事实上，每个RPC区间都包含来自客户端（Client）和服务器端（Server）的注释，这使得双主机区间（Two-host Span）成为最常见的一种。图2-32中的区间就包含了来自客户端的注释信息：“\<Start\>”、“Client Send”、“Client Recv”和“\<End\>”，也包含了来自服务器端的注释信息：“Server Recv”、“foo”和“Server Send”。除了“foo”是用户自定义的注释外，其他的注释信息都是和时间相关的信息。Dapper不但支持用户进行简单的文本方式的注释，还支持键—值对方式的注释，这赋予了开发者更多的自由。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/72_1.jpg)

图2-32 区间Helper.Call的详细信息

**2. 监控信息的汇总:**

Dapper对几乎所有的Google后台服务器进行监控。海量的消息记录必须通过一定的方式汇集在一起才能产生有效的监控信息。在实际中，Dapper监控信息的汇总需要经过三个步骤，如图2-33[18]所示。

（1）将区间的数据被写入到本地的日志文件。

（2）利用Dapper守护进程（Dapper daemon）和Dapper收集器（Dapper Collectors）将所有机器上的本地日志文件汇集在一起。

（3）将汇集后的数据写入到Bigtable存储库中。

从图中也很容易地看出，（1）和（2）是一个读的过程，而（3）是一个写的过程。选择Bigtable主要是因为区间的数目非常多，而且各个区间的长度变化很大，Bigtable对于这种很松散的表结构能够很好地进行支持。写入数据后的Bigtable中，单独的一行表示一个记录，而一列则相当于一个区间。这些监控数据的汇总是单独进行的，而不是伴随系统对用户的应答一起返回的。如此选择主要有如下的两个原因：首先，一个内置的汇总方案（监控数据随RPC应答头返回）会影响网络动态。一般来说，RPC应答数据规模比较的小，通常不超过  10 KB。而区间数据往往非常的庞大，如果将二者放在一起传输，会使这些RPC应答数据相对“矮化”进而影响后期的分析。另一方面，内置的汇总方案需要保证所有的RPC都是完全嵌套的，但有许多的中间件系统在其所有的后台返回最终结果之前就对调用者返回结果，这样有些监控信息就无法被收集。基于这两个考虑，Google选择将监控数据和应答信息分开传输。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/73_1.jpg)

图2-33 监控信息的汇总

安全问题是所有系统都必须考虑的问题，为了防止未授权用户对于RPC信息的访问，信息汇总过程中Dapper只存储RPC方法的名称却不存储任何RPC负载数据，取而代之的是，应用层注释提供了一种方便的选择机制（Opt-in Mechanism）：应用程序开发者可以将任何对后期分析有益的数据和区间关联起来。

#### 2.6.3 关键性技术

前面提到了Dapper的三个基本设计目标，在这三个目标中，实现难度最大的是对应用层透明。为了达到既定设计目标，Google不断进行创新，最终采用了一些关键性技术解决了存在的问题。这些关键性技术概括起来主要包括以下两个方面。

**1. 轻量级的核心功能库:**

这主要是为了实现对应用层透明，设计人员通过将Dapper的核心监控实现限制在一个由通用线程（Ubiquitous Threading）、控制流（Control Flow）和RPC代码库（RPC Library Code）组成的小规模库基础上实现了这个目标。其中最关键的代码基础是基本RPC、线程和控制流函数库的实现，主要功能是实现区间创建、抽样和在本地磁盘上记录日志。用C++的话Dapper核心功能的实现不超过1000行代码，而用Java则不到800行。键/值对方式注释功能的实现需要额外增加500行代码。将复杂的功能实现限制在一个轻量级的核心功能库中保证了Dapper的监控过程基本对应用层透明。

**2. 二次抽样技术:**

监控开销的大小直接决定Dapper的成败，为了尽可能地减小开销，进而将Dapper广泛部署在Google中，设计人员设计了一种非常巧妙的二次抽样方案。二次抽样顾名思义包括两次抽样过程。Google每天需要处理的请求量惊人，如果对所有的请求都进行监控的话所产生的监控数据将会十分的庞大，也不利于数据分析，因此Dapper对这些请求进行了抽样，只有被抽中的请求才会被监控。在实践中，Dapper的设计人员发现了一个非常有意思的现象，那就是当抽样率低至1/1024时也能够产生足够多的有效监控数据，即在1024个请求中抽取1个进行监控也是可行的，这种低抽样率有效的原因在于巨大的事件数量使关注的事件可能出现的足够多，从而可以捕获有效数据。这就是Dapper的第一次抽样。最初Dapper设计的是统一的抽样率，但是慢慢地发现对于一些流量较低的服务，低抽样率很可能会导致一些关键性事件被忽略，因此Dapper的设计团队正在设计一种具有适应性抽样率的方案。尽管采取抽样监控，所产生的数据量也是惊人的。根据Dapper团队的统计，Dapper每天得到的监控数据量已经超过1T，如果将这些数据全部写入Bigtable中效率较低，而且Bigtable的数据存储量有限，必须定期处理，较少的数据能够保存更长的时间。对此，Dapper团队设计了第二次的抽样。这次抽样发生在数据写入Bigtable之前，具体方法是将监控id散列成一个标量*z*，其中0≤*z*≤1。如果某个区间的*z*小于事先定义好的汇总抽样系数（Collection Sampling Coeficient），则保留这个区间并将它写入Bigtable。否则丢弃不用。也就是说在采样决策中利用*z*值来决定某个监控树是整棵予以保留还是整棵弃用。这种方法非常的巧妙，因为在必要时只需改动*z*值就可以改变整个系统的写入数据量。利用二次抽样技术成功地解决了低开销及广泛可部署性的问题。

上面的两种技术手段解决了主要设计问题，这使得Dapper在Google内部得到了广泛的应用。Dapper守护进程已成为Google镜像的一部分，因此Google所有的服务器上都有运行Dapper。

#### 2.6.4 常用Dapper工具

**1. Dapper存储API:**

Dapper的“存储API”简称为 DAPI，提供了对分散在区域Dapper存储库（DEPOTS）的监控记录的直接访问。一般来说，有以下三种方式可以对这些记录进行访问。

（1）通过监控id访问（Access by Trace id）：利用全局唯一的监控id直接访问所需的监控数据。

（2）块访问（Bulk Access）：DAPI可以借助MapReduce来提供对数以十亿计的Dapper监控数据的并行访问。用户覆写一个将Dapper监控作为其唯一参数的虚函数（Virtual Function），在每次获取用户定义的时间窗口内的监控数据时架构都将引用该函数。

（3）索引访问（Indexed Access）：Dapper存储库支持单索引（Single Index），因为监控id的分配是伪随机的，这是快速访问同特定服务或主机相关监控的最好方式。

根据不完全的统计，目前大约有三个基于DAPI的持久应用程序，八个额外的基于DAPI的按需分析工具及15～20个使用DAPI框架构建的一次性分析工具。

**2. Dapper用户界面:**

大部分的用户在使用Dapper时都是通过基于Web的交互式用户界面，图2-34～图2-38显示其一般性的使用流程。

（1）首先用户需要选择监控对象，包括监控的起止时间、区分监控模式的信息（图2-34[18]中是区间名）及一个衡量开销的标准（图2-34中是服务延迟）。

（2）如图2-35[18]所示，一个大的性能表给出了所有同指定监控对象有关的分布式执行模式的简要情况。用户可以按其意愿对这些执行模式进行排序并选择某一个查看更多的细节。

（3）图2-36[18]是某个选中的分布式执行模式，该执行模式以图形化描述呈现给用户。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/75_1.jpg)

图2-35 监控对象相关的执行模型

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/75_2.jpg)

图2-36 特定的执行模式

（4）根据最初选择的开销度量标准，Dapper会以频度直方图的形式将步骤（3）中选中的执行模式的开销分布展示出来，如图2-37[18]所示，同时呈现给用户的还有一系列特殊的监控样例信息，这些信息落在直方图的不同部分。用户可以进一步的选择这些监控样例。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/76_1.jpg)

图2-37 执行模式开销的频度直方图

（5）在用户选择了某个监控样例后，就会进入所谓的监控审查视图（Trace Inspection View）。图 2-38[18]是部分的监控审查视图，在这个视图中，最顶端是一条全局的时间线（Global Time Line）。每一行是一个监控树，选择“+”或“−”能够展开或折叠监控树。每个监控树用嵌套的彩色长方形表示的。每个 RPC 区间又被进一步的分成花在服务器处理上的时间和花在网络通信上的时间。用户注释并未在图中显示出来，但是它们可以按照逐个区间被选择包含在全局时间线上。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/76_2.jpg)

图2-38 监控审查视图

根据统计，一个普通的工作日内大概有200个不同的Google工程师在使用Dapper用户界面。因此，在一周的时间里，有750～1000个不同的用户。

#### 2.6.5 Dapper使用经验

本节介绍Dapper在Google中的一些使用经验，通过这些经验可以看出在哪些场景中Dapper是最适用的。

**1. 新服务部署中的Dapper的使用：**

Google的AdWords系统的构建围绕着一个由关键字命中准则和相关的文字广告组成的大型数据库。在这个系统进行重新开发时，开发团队从原型系统直到最终版本的发布过程中，反复的使用了Dapper。开发团队利用Dapper对系统的延迟情况进行一系列的跟踪，进而发现存在的问题，最终证明Dapper对于AdWords系统的开发起到了至关重要的作用。

**2. 定位长尾延迟（Addressing Long Tail Latency）：**

Google最重要的产品就是搜索引擎，由于规模庞大，对其进行调试是非常复杂的。当用户请求的延迟过长，即延迟时间处于延迟分布的长尾时，即使最有经验的工程师对这种端到端性能表现不好的根本原因也常常判断错误。通过图2-39[18]不难发现，端到端性能和关键路径上的网络延迟有着极大的关系，因此发现关键路径上的网络延迟常常就能够发现端到端性能表现不佳的原因。利用Dapper恰恰能够比较准确的发现关键路径。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/77_1.jpg)

图2-39 关键路径网络延迟对于端到端性能表现的影响

**3. 推断服务间的依存关系（Inferring Service Dependencies）：**

Google的后台服务之间经常需要互相的调用，当出现问题时需要确定该时刻哪些服务是相互依存的，因为这样有利于发现导致问题的真正原因。Google的“服务依存关系”项目使用监控注释和DPAI的MapReduce接口实现了服务依存关系确定的自动化。

**4. 确定不同服务的网络使用情况：**

在Dapper出现之前，Google的网管人员在网络出现故障时几乎没有工具能够确定到底是哪个部分的网络出现的故障。而现在Google利用Dapper平台构建了一个连续不断更新的控制台，用来显示内部集群网络通信中最活跃的应用层终端。这样在出现问题时可以最快的定位占用网络资源最多的几个服务。

**5. 分层的共享式存储系统：**

Google中的许多存储系统都是由多个相对独立且具有复杂层次的分布式基础架构组成。例如，Google App Engine是构建在一个可扩展的实体存储系统之上的。而该实体存储系统则是构建在底层的Bigtable之上，展现出一些RDBMS（关系型数据库管理系统）的功能。而Bigtable又依次用到了Chubby和GFS。在这样的层次式系统中决定端用户的资源消耗模式并不总是那么简单。例如，由Bigtable的单元引起的GFS高流量可能主要由一个用户或几个用户产生，但是在GFS的层次上这两种不同的使用模式是没法分开的。更进一步，在没有Dapper之类工具的情况下对于这种共享式服务资源的争用也同样难以调试。

**6. 利用Dapper进行“火拼”（Firefighting with Dapper）：**

这里所谓的“火拼”是指处于危险状态的分布式系统的代表性活动。正在“火拼”中的Dapper用户需要访问最新的数据却没有时间来编写新的DAPI代码或者等待周期性的报告，此时可以通过和Dapper守护进程的直接通信，将所需的最新数据汇总在一起。

Dapper在Google内部取得了巨大的成功，虽然这种成功在一定程度上得益于Google内部系统的同构性，但是Dapper团队的创新性设计才是系统取得成功的根本性因素。Google的后台系统可以说是目前全球最大的一个云平台，读者借鉴Dapper的设计思想一定能够为不同规模的云平台设计出合适的监控系统。

### 2.7 海量分析的交互式分析工具Dremel

数据本身不会产生价值，只有经过分析才有可能产生价值。Google公开了MapReduce计算框架之后，由于其强大的数据分析和处理能力，很快就被视为数据分析的一个实际标准，各种围绕着MapReduce框架开发的软件和系统层出不穷。但是随着互联网尤其是移动互联网的发展，数据种类和应用需求呈现出爆炸式的增长。MapReduce作为一种面向批处理的框架，在很多应用领域已经不太适用。对此出现了两种应对思路，一种是将MapReduce进行改造，使其除了能进行批处理之外，还能进行其他类型的数据处理，比如处理流数据。另一种思路就是完全抛开MapReduce，根据具体的应用重新进行架构。后一种思路显然对问题的解决更彻底，但是很长时间里也没有出现能够弥补MapReduce缺失功能的框架，直到Google公开了Dremel系统。

Dremel早在2006年就开始在Google内部使用，但是直到2010年的VLDB（数据库国际三大顶级会议之一）Google才公开了Dremel的实现。本节将详细讲解Dremel的实现，对其产生的背景和具体的设计细节进行阐述。

#### 2.7.1 产生背景

MapReduce在处理数据时的确有其便捷性，但是假设我们有这样的一个应用场景：数据分析师在编写完一段代码之后，想立刻验证下这段代码是否可以有效地从海量的数据集中提取出有效的特征，于是他运行这段代码。如果是利用MapReduce的话，由于数据量巨大，很可能需要等待几个小时甚至更长时间才能出结果。假如发现代码的算法有问题，无法有效地提取特征，因此又重新修改了代码，并再次运行。这样的过程可能要反复好几次，总的耗时可能多达数天。这种情况下MapReduce的效率显然无法让人接受。类似这样的数据探索（Data Exploration）的应用在实际中是非常普遍的，很多时候用户更期望的是实时的数据交互，就像传统的SQL查询一样。用户希望提交完自己的请求之后，在一个相对可以接受的合理时间内系统就会返回结果，而不是像MapReduce这样，需要耗费很长的时间。

随着上述需求在Google内部越来越强烈，Google的团队结合其自身的实际需求，借鉴搜索引擎和并行数据库的一些技术，开发出了实时的交互式查询系统Dremel。Dremel支撑了Google内部的很多系统，典型的应用包括：

●Android市场的应用安装数据的跟踪；

●Google产品的错误报告；

●Google图书的光学字符识别；

●Bigtable实例上的tablet迁移；

●Google分布式构建系统的测试结果分析；

●Google数据中心上运行任务的资源监控；

●Google代码库的符号和依赖关系分析。

Dremel并不开源，但是Google利用Dremel向外界用户提供BigQuery服务，读者可以通过体验BigQuery服务来感受Dremel的强大功能。正如Google的MapReduce被Hadoop复制一样，目前也有一些开源项目尝试复制Dremel系统，比如Apache的Drill等。

#### 2.7.2 数据模型

Google的数据平台常常需要满足通用性，也就是不同平台之间能够很好地实现数据的交互处理。在这种情况下想要实现上述场景中所说的实时交互查询，需要两方面的技术支撑：首先需要有一个统一的存储平台，能够实现高效的数据存储，Dremel使用的底层数据存储平台是GFS。另一方面还需要有一个统一的数据存储格式，这样存储的数据才可以被不同的平台所使用。而数据存储格式的实现又包含两个方面的内容：数据模型以及模型的具体实现。首先我们来看Dremel的数据模型。

关系数据库中用关系模型对其存储的数据进行建模，统一的模型一定程度上简化了数据的存储和查询。但是在现实的世界中，很多数据之间并没有严格的关系，它们更多的是一种松散和扁平的结构。很显然此时关系模型就无法很好地进行建模。Google在对其存储的数据进行仔细的分析之后，发现嵌套数据模型（Nested Data Model）很适合Google的数据存储。在关系数据库中，数据的存储方式一般有两种，较早期基本使用的都是行［关系数据库中也称其为元组（tuple）］存储，或者说是面向记录的存储。这种存储方式以行为单位，一行一行地存储数据。列存储则是以属性为单位，每次存储一个属性，在应用时再将需要的属性重新组装成原始的记录。在关系数据库中列存储的研究已经相对成熟，但是在嵌套数据模型中，尚未有大规模的应用实例。Google的Dremel是第一个在嵌套数据模型基础上实现列存储的系统。列存储的主要好处在于处理时只需要使用涉及的列数据，且列存储更利于数据的压缩。图2-40[19]是面向记录和面向列的存储。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/80_1.jpg)

图2-40 面向记录和面向列的存储

式（2-1）是嵌套模型的形式化定义，其中*τ*可以是原子类型（Atomic Type），也可以是记录类型（Record Type）。原子类型允许的取值类型包括整型、浮点型、字符串等，记录类型则可以包含多个域。A*i*代表该记录型数据的名称。记录型数据包括三种类型：必须的（Required）、可重复的（Repeated）以及可选的（Optional）。其中 Required类型必须出现且仅能出现一次。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/80_2.jpg)

图2-41[19]是一个记录类型文档的实例。其中，右上角的虚线框内是该文档的模式（Schema）定义。而r1和r2则是符合该模式的两条记录。r1和r2中的属性是通过完整的路径来表示的，比如Name.Language.Code。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/80_3.jpg)

图2-41 嵌套结构的模式和实例

图2-41中所定义的这种嵌套式数据模型对于Google而言非常重要，因为这种定义方式跟具体的语言和处理平台无关。利用该数据模型，可以使用Java语言，也可以使用C++语言来处理数据，甚至可以用Java编写的MapReduce程序直接处理C++语言产生的数据集。这种跨平台的优良特性正是Google所需要的。

#### 2.7.3 嵌套式的列存储

关系型数据库中采用列存储有其便利之处，因为在不同列中相同位置的数据必然属于原数据库中的同一行，因此我们可以直接将每一列的值按顺序排列下来，不用引入其他的概念，也不会丢失数据信息。但是针对Google的这种嵌套式数据结构的列存储，数据本身之间的关系比关系数据库要复杂。存储后的数据本身反映不出任何结构上的信息，因此存储中除了记录值，还要记录结构。另外，所有的列存储在应用时往往要涉及多个列，如何按照正确的顺序快速地进行数据重组也是列存储需要解决的。Dremel从数据结构的表示等方面解决了上述问题。

**1. 数据结构的无损表示：**

在嵌套式的数据模型中，对于某个值，譬如图2-41中r1的'en-us'和'en'，如果仅仅是单纯地记录下来，根本无法判断这两个值对应的是r1中哪个位置，因为在r1中Name.Language.Code出现了三次。为了准确地在存储中反映出嵌套的结构，Dremel定义了两个变量：*r*（Repetition Level，重复深度）和*d*（Definition Level，定义深度）。重复深度和定义深度的直接定义比较晦涩，下面以图2-41中记录r1和r2为例来解释这两个概念。从r1中可以发现，Name.Language.Code一共出现了三次，值分别为'en-us'、'en'和'en-gb'。对照模式定义，不难发现在Name.Language.Code中，可重复的（repeated）字段有Name和Language，因此Code的可重复深度的取值只可能为0、1、2，其中0表示一个新记录的开始。沿着r1的记录从上到下读取，当我们第一个读取到'en-us'时，我们尚没有看见Name和Language重复出现（这两个字段都是第一次出现），所以'en-us'的*r*值取0。接着往下读取，'en'出现的时候，Language出现了第二次，也就是说重复了。因为Language在Name.Language.Code路径中的位置排在第二，所以'en'的*r*值取2。当读取到'en-gb'时，Name重复（此Name后Language只出现过一次，没有重复），Name在Name.Language.Code中排第一位，所以重复深度是1。因此，r1中Code的值的重复深度分别是0、2、1。简单来说，重复深度记录的是该列的值是在哪一个级别上重复的。

要注意第二个Name在r1中没有包含任何Code值。为了确定'en-gb'出现在第三个Name中而不是第二个，系统会添加一个NULL值在'en'和'en-gb'之间（图2-42[19]）。由于在模式定义中，Language字段中的Code字段是必需的，所以它的缺失意味着Language也没有定义。

从上述过程可以看出，Dremel的重复深度跟树的深度概念不太一样。树每向下延伸一层，深度就增加一层。但是Dremel中重复深度的定义并不考虑非repeated类型的字段，因为required和optional类型的值不可能重复，所以只考虑repeated类型字段出现的情况就可以完整表达出嵌套结构中字段的重复情况。

重复深度主要关注的是可重复类型，而定义深度同时关注可重复类型和可选类型（optional）。定义深度表示“值的路径中有多少可以不被定义（因为是可重复类型或可选类型）的字段实际是有定义的”。以Name.Language.Country路径为例，按照模式定义，Name、Language和Country三个字段均属于可有可无型，所以其重复深度的可能取值为0、1、2、3。Name.Language.Country在r1中一共有4个定义，值分别为'us'、NULL、NULL和'gb'。对于'us'，Name、Language、Country都是有定义的，所以'us'的*d*值为3。同理，第一个NULL的*d*值为2，第二个NULL的*d*值为1，'gb'的*d*值为3。图2-42列出了记录r1和r2的完整*r*值与*d*值。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/82_1.jpg)

图2-42 带有重复深度和定义深度的r1与r2的列存储

每一列最终会被存储为块（Block）的集合，每个块包含重复深度和定义深度且包含字段值。NULLs没有明确存储因为它们可以根据定义深度确定：任何定义深度小于可重复和可选字段数量之和就意味着这是一个NULL。必需字段的值不需要存储定义深度。类似地，重复深度只在必要时存储；比如，定义深度0意味着重复深度0，所以后者可省略。

**2. 高效的数据编码：**

虽然从理论上已经完成了嵌套结构的无损表示，但是实际中如何将一行行的记录表示成图2-42所示的结构仍是一个挑战。计算重复和定义深度的基础算法如图2-43[19]所示。

算法遍历记录结构然后计算每个列值的深度（包括重复和定义深度，下同），即使是NULL时也不例外。在Google，经常会出现模式包含成千上万的字段，却仅有几百个在记录中被使用的情况。因此需要尽可能廉价地处理缺失字段。Dremel利用图2-43的算法创建一个树状结构，树的节点为字段的writer，它的结构与模式中的字段层级匹配。核心的想法是只在字段writer有自己的数据时执行更新，非绝对必要时不尝试往下传递父节点状态。子节点writer继承父节点的深度值。当任意值被添加时，子writer将深度值同步到父节点。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/83_1.jpg)

图2-43 计算重复和定义深度的基础算法

**3. 数据重组：**

无论何种模型，只要采用列存储，在使用时就需要考虑数据重组问题。简单来说，就是将查询涉及的列取出，然后将其按照原始记录的顺序组装起来，让用户感觉好像数据库中仅存在这些查询涉及的列一样。Dremel数据重组方法的核心思想是为每个字段创建一个有限状态机（FSM），读取字段值和重复深度，然后顺序地将值添加到输出结果上。一个字段的FSM状态对应这个字段的reader。重复深度驱动状态变迁。一旦一个reader获取了一个值，就去查看下一个值的重复深度来决定状态如何变化、跳转到哪个reader。一个FSM状态变化的始终就是一条记录装配的全过程。以图2-41中的r1为例，其所对应的有限状态机如图2-44[19]所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/83_2.jpg)

图2-44 r1的有限状态机

结合图2-42中相关字段的内容，可以构建出r1的完整数据重组过程，见表2-3。

表2-3 r1的数据重组过程

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/84_1.jpg)

如果具体的查询中不是涉及所有列，而是仅涉及很少的列的话，上述数据重组的过程会更加便利，比如图2-45[19]中仅仅涉及DocId和Name.Language.Country的有限状态机。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/84_2.jpg)

图2-45 DocId和Name.Language.Country的有限状态机

图2-46[19]是有限状态机的构造算法，核心的思想如下：设置*t*为当前字段读取器的当前值*f*所返回的下一个重复深度。在模式树中，找到它在深度*t*的祖先，然后选择该祖先节点的第一个叶子字段*n*。由此得到一个FSM状态变化（*f*，*t*）->*n*。比如，如果*t*=1为*f*=Name.Language.Country读取的下一个重复深度。它的祖先重复深度为1的是Name，而Name的第一个叶子字段是*n* = Name.Url，这就形成了图2-44中Name.Language.Country遇到1跳转到Name.Url的过程。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/85_1.jpg)

图2-46 有限状态机的构造算法

#### 2.7.3 查询语言与执行

Dremel的查询语言是基于SQL的，这点跟Hive很像。这种抽象大大降低了用户的使用门槛，提高了系统的易用性。Dremel的SQL查询输入的是一个或多个嵌套结构的表以及相应的模式，而输出的结果是一个嵌套结构的表以及相应的模式。

图2-47[19]是一个典型的查询实例，图中显示了查询语言、查询结果模式以及最终输出的数据模式。图2-47中的查询执行在图2-41中的*t*={r1，r2}上，实际执行了投影（projection）、选择（selection）和记录内聚合（within-record aggregation）等操作。查询会最终根据某种规则产出一个嵌套结构的数据，不需要用户在SQL中自己来指明该规则。

为了具体解释图2-47中的查询，考虑其SQL语句中的选择和投影两个操作。在选择操作（where子句）中。可以将一个嵌套记录想象为一个树结构，树中每个节点的标签对应字段的名字。选择操作就是对不满足指定条件的分支进行剪枝。因此在图2-47的例子中，只有当Name.Url非空且满足正则表达式“^http”才被保留。接下来的投影操作中，SELECT子句中的每个标量表达式都会投影为一个值，此值的嵌套深度和表达式中重复字段最多的保持一致。所以，Str值的嵌套深度与Name.Language.Code相同。COUNT部分完成记录内聚合，每个Name子记录都会执行此聚合。将Name.Language.Code出现的COUNT投影为每个Name下的Cnt值，它是一个非负的64位整型。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/86_1.jpg)

图2-47 Dremel的SQL查询语句实例

Dremel的类SQL语言支持嵌套子查询、记录内聚合、top-k、joins、用户自定义函数（user-defined）等操作类型。

在了解Dremel的SQL查询语言之后，我们来看看具体的查询在系统中是如何执行的。如图2-48[19]所示，Dremel利用多层级服务树（multi-level service tree）的概念来执行查询操作。Dremel的查询流程非常清晰，根服务器接受客户端发出的请求，读取相应的元数据，然后将请求转发至中间服务器。中间服务器负责查询中间结果的聚集，而执行的数据来源则主要由叶子服务器负责。叶子服务器既可以读取其本地的数据，也可以向统一的存储层发出数据请求来获取数据。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/86_2.jpg)

图2-48 查询执行的基本架构

大多数的Dremel查询都是简单的聚集操作（不涉及join等复杂操作），具体的执行过程比较容易理解。从图2-48来看，向下的箭头主要执行的是查询重写（rewrite）过程，而向上的箭头主要是查询执行和聚集的过程。以如下的一个典型的查询为例：

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/87_1.jpg)

当系统接收到这个查询请求之后，沿着多层级服务树从上到下不断地执行查询重写。首先将式（2-2）重写成式（2-3)的形式：

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/87_2.jpg)

其中$R^1_i$是树中第1层的节点1～*n*返回的子查询结果（根节点是第0层，下一层是第1层）。$R^1_i$的查询表达式如式（2-4)所示：

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/87_3.jpg)

Dremel中的数据都是分布式存储的，因此每一层查询涉及的数据实际都被水平划分后存储在多个服务器上。式（2-4)中的T1 *i*表示T的第*i*个分区的数据。接下来的第2层直到叶子节点都将进行类似的查询重写。在每个部分得到子查询结果之后，会依次从下到上执行一个查询结果聚集的过程，直到得到最后的结果并返回给用户。

Dremel是一个多用户系统，因此同一时刻往往会有多个用户进行查询。为了维护系统的性能稳定，需要查询分发器（query dispatcher）来进行负载均衡。同时查询分发器还负责系统的容错，当某个节点的执行时间过长，会分配其他节点来执行该任务。执行时间信息由查询分发器维护一个直方图来体现。

查询分发器有一个很重要参数，它表示在返回结果之前一定要扫描百分之多少的tablet，Google的使用经验表明设置这个参数到较小的值（比如98%而不是100%，也就是说不需要扫描完所有的数据）通常能显著地提升执行速度。

#### 2.7.5 性能分析

由于Dremel并不开源，我们只能通过Google论文中的分析大致了解其性能。Google的实验数据集规模见表2-4。

表2-4 实验数据集

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/87_4.jpg)

图2-49[19]展现了两个MR任务和Dremel的执行耗时。所有的任务均运行在3000个工作节点上。在实验中，Dremel和MR-列状存储，实际仅读取大约0.5 TB的压缩列状数据，而MR-面向记录存储则读取87 TB数据（也就是读取了全部数据）。如图2-49所示，MR从面向记录转换到列状存储后性能提升了一个数量级（从小时到分钟），而使用Dremel则又提升了一个数量级（从分钟到秒)。更多的性能分析此处不再赘述，有兴趣的读者可以查阅原论文。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/88_1.jpg)

图2-49 面向记录和面向列存储的MR与Dremel性能对比

#### 2.7.6 小结

本节给读者介绍了Dremel产生的背景，对其设计的核心思想和关键性实现技术进行了详细的介绍。Dremel和MapReduce并不是互相替代，而是相互补充的技术。在不同的应用场景下各有其用武之地。Drill的设计目标就是复制一个开源的Dremel，但是从目前来看，该项目无论是进展还是影响力都达不到Hadoop的高度。希望未来能出现一个真正有影响力的开源系统实现Dremel的主要功能并被广泛采用。

### 2.8 内存大数据分析系统PowerDrill

Dremel的推出，在一定程度上减轻了Google内部对于MapReduce系统的依赖。但是随着数据量的增加和对数据分析时效性的要求越来越高，Dremel在有些场景下也显得不太适用，为此Google研发了内部代号为PowerDrill的系统。该系统从2008年年底开始部署，2009年中旬开始在Google内部公开使用。到了2012年的时候，Google在VLDB会议上公开了PowerDrill的部分实现。根据Google自己的说法，该系统在针对相对特定的数据集上，可以比Dremel处理得更快。由于Google主要公开的是PowerDrill的存储部分，因此本节主要介绍PowerDrill为了达到更快的处理速度所采用的存储方案及相关的优化等。

#### 2.8.1 产生背景与设计目标

用户对于实时的交互式数据查询和分析一直都有很高的要求，尤其是在一些数据探索（data exploration）的场景，完成一项任务之前需要先向系统发出请求，根据得到的结果来修正查询内容，并再次向系统发出新的查询，如此反复的过程可能要进行很多次。很显然MapReduce无法实现这种程度的交互式查询。Dremel可以在一定程度上实现实时的交互式查询，但是随着数据规模的增大和Google内部对ad hoc查询（即席查询）需求的增多，Google设计和开发了新的交互式查询系统PowerDrill。ad hoc查询是用户根据自己的需求，灵活地选择查询条件，系统根据用户的选择生成相应的统计结果。通常的查询在系统设计和实施时是已知的，所以我们可以在系统构建时通过建立索引、分区等技术来优化这些查询，达到提升查询效率的目的。但ad hoc查询是用户在使用时临时产生的，系统无法预先优化这些查询，因此需要通过其他技术手段来实现高效的ad hoc查询。

通过对内部数据和查询类型的分析，PowerDrill开发团队得出了以下两个假设结论，整个系统的开发和优化都基于这两个假设：

（1）绝大多数的查询是类似和一致的；

（2）存储系统中的表只有一小部分是经常被使用的，绝大部分的表使用频率不高。

在这两个假设的指导下，为了实现高效的查询，PowerDrill开发团队主要考虑如下两方面的内容：

（1）如何尽可能在查询中略去不需要的数据分块；

（2）如何尽可能地减少数据在内存中的占用，占用越少意味着越多的数据可以被加载进内存中处理。

与Dremel数据处理方式不同的是，PowerDrill需要尽可能地将数据加载至内存，在某种程度上其计算方式更接近内存计算。PowerDrill整个系统实际分为三个部分：

（1）Web UI，用于和用户进行交互；

（2）一个抽象层，用户的命令会被转换成SQL，然后根据不同的需求，这些命令会被发送至不同的终端，比如Dremel，Bigtable等；

（3）列式存储。

在论文中仅涉及列式存储部分，因此本节主要介绍列式存储的基本数据结构以及为了实现高效查询所采取的优化策略。

#### 2.8.2 基本数据结构

列式存储主要有两个好处：减少查询涉及的数据量以及便于数据压缩。传统的关系数据库一般通过索引来减少查询中用到的数据量，但是在ad hoc查询这样的场景下，索引基本不起作用。PowerDrill采用的方式是对数据进行分块，对块数据设计巧妙的数据结构，使得在查询时可以确定哪些块不需要，可以直接被略去，这样就大大减少了所需的数据量。关于分块的内容会在2.8.3节介绍，下面首先介绍块数据的数据结构。

图2-50[20]是一张存储搜索关键字（search string）的表，清楚阐述了PowerDrill采用的数据结构，简单来说就是一个双层数据字典结构。图2-50中假设表的数据已经被分成3个块，分别是chunk 0、chunk 1和chunk 2。图2-50最左侧是一个全局字典表，存储的是全局id（global-id）和搜索关键字的对应关系。全局字典表的右侧是3个块的数据，对于每个块，主要由两部分组成：块字典和块元素。块字典记录的是块id（chunk-id）和全局id的映射关系，而块元素记录的是块中存储数据的块id（注意不是全局id）。在具体查询时需要完成两层的数据映射才能得到真实的数据值。比如chunk 0中第一个块id为3，查块字典得到其全局id为5，再查询全局字典得到其真实值为ebay。反过来从真实值查询其对应的块id也是类似的过程。需要说明的是图2-50仅仅说明了数据结构，具体实现时根据不同的应用场景可以采用前缀树（trie）或其他合适的方法。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/90_1.jpg)

图2-50 基本数据结构

#### 2.8.3 性能优化

PowerDrill系统更关心查询的时效性，如果对于任何的查询，都需要从磁盘上将数据加载至内存，势必会严重影响查询效率。因此如何在基本的数据结构之上进行全面的优化，使得尽可能多的数据能够常驻内存才是性能得到显著提升的关键因素。在实际的开发中，PowerDrill团队采用了多种优化技术，虽然不是全新的方法，但是组合起来对PowerDrill系统性能的提升还是很显著的。下面分别对这些优化进行简要的讲解。

**1. 数据分块：**

由于传统的索引对于PowerDrill的查询场景作用不是很大，因此一个很自然的考虑就是对数据进行分块，然后通过一些方法过滤查询中不需要的数据块来减少数据量。数据分块的另一个好处就是防止单个数据表过大，导致性能下降。基本上现代的数据库系统都是支持数据分区的，常见的分区方法有范围分区（range partitioning）、散列分区（hash partitioning）等。PowerDrill实际采用的是一种组合范围分区 （composite range partitioning）方法。大致的方法是由领域专家确定若干个划分的域（需要是有序的，一般是3个到5个域），然后依次利用这几个域（要求该域至少有两个以上的不同属性值）对数据进行划分，可以设定一个阈值，比如50000。当每个块的行数达到这个阈值时就停止划分，否则可以进一步划分。PowerDrill采用的数据分块方法简单实用，但是由于域的确定需要领域专家，因此这种方法在实际使用中还有一定的局限性。

**2. 数据编码的优化：**

在实际存储中选择合适的编码方式也很重要。最简单的自然是统一编码，比如对所有的块id都采用32位的整型来记录。但是这种方法会带来空间上的浪费。比如一个块中仅有1个不同值，那我们实际只需要记录块中的记录数即可。如果有两个不同值，1比特位也就足够了。依次类推，对于不同的块，如果我们可以确定块中不同值的数量，那么就可以根据这个数量值来选择可变的比特位来记录块id。统计一组数中不同值的个数有一个专有名词，称为“基数估计”。对于小规模的数据集，可以比较容易地统计出精确的基数。但是在大数据的环境下，精确的基数统计非常耗时，因此能保证一定精度的基数估计就可以满足实际的需求。基数估计的方法很多，大多利用了散列函数的一些特性，Google内部使用的是一种称为Hyperloglog的基数估计方法的变种。

**3. 全局字典的优化：**

对于Google的生产环境来说，全局字典是非常庞大的，因此有必要对全局字典进行优化。在优化中主要利用了两个特性：

（1）全局字典是有序的；

（2）排序后的数据常常有共同的前缀。

基于这两个特性，前缀树是一个不错的选择。实际使用中为了进一步减少查询中需要加载到内存的全局字典，对全局字典又进行了分块，因为不同的值使用的频度是不一样的，将最常用的值组织成一个块，全局字典中剩余值再组织成其他若干个块，这样就能保证每次涉及的块较少。除此之外，对每个全局字典块还会维护一个布隆过滤器（bloom filter）来快速确定某个值是否在字典中。

**4. 压缩算法：**

上面提到过列存储的最大优势之一就是便于数据的压缩，当然不同的压缩算法在执行效率上是有差别的。Google曾经对一些主流的压缩算法做过简单的测试，性能见表2-5[21]。

表2-5 常见压缩算法性能对比

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/91_1.jpg)

虽然Zippy是Google自己开发的压缩算法，但是PowerDrill的开发团队在经过测试之后，最终选择了LZO算法的一个变种作为实际生产环境中的压缩算法，可能是因为LZO算法整体表现比较均衡。但是对LZO算法具体做了何种改动，Google的论文中并没有提及。

不管压缩算法的解压速度多快，总会消耗一定的物理资源与时间。对此PowerDrill采用了一种冷热数据分别对待的策略。即在内存中保有压缩和未压缩的数据，根据需要对数据进行压缩和解压缩。在冷热数据切换策略中，比较常用的是LRU算法。LRU是Least Recently Used的缩写，即最近最少使用页面置换算法。但是PowerDrill开发团队认为直接的LRU算法效果还不是很理想，为此他们采用了一种启发式的缓存策略来代替原始的LRU算法。

**5. 行的重排：**

有研究表明，在列存储中，对行进行适当的重排不会影响结果且会提升压缩效率。这个是比较好理解的，以图2-51[20]为例。

假设图2-51中左侧是原始的数据顺序，而右侧是重排后的，从直观上感觉肯定是右侧的更利于数据压缩。下面解释为什么这种重排会提升压缩效率以及如何实现这种重排。

数据压缩的算法有很多，比较常用的一种称为游程编码（Run-Length Encoding，RLE），又称行程长度编码，其好处是压缩和解压缩都非常快。RLE算法将数据压缩成三元组的形式。比如AAAABBBBBBCCCCCC 这样一个包含16条数据的某列数值，RLE算法会将它压缩成形如“[实际值，起始位置，个数]”的三元数组，所以上面的数值会压缩成[A，1，4]、[B，5，6]、[C，11，5]的格式，从而比原始的数据占用更少的空间。由于RLE算法的特点，相邻数据之间越相似，压缩效果越好，因此数据往往需要重新排序从而得到更好的结果。这就是为什么图2-51中右侧的数据会有更好压缩效果的原因。

虽然知道重排可以提升压缩效果，但是实际的重排过程却并不是那么容易。数据重排的过程等效于著名的TSP（旅行商）问题，也就是说这是一个NP困难问题。为什么数据重排的过程等效于TSP问题呢？首先我们来看汉明距离（hamming distance）的定义。在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。

例如图2-52[20]中左侧的字符串010和011的汉明距离就为1，因为它们只是第3个位置上的字符不同。假设图2-52中左侧每一行的字符串看做汉明空间的一个点，则图2-52中右侧表示这3行字符串（3个点）的访问顺序。即从010到011（距离为1），再到100（距离为3），总的距离为4。这也是使得这3个点在汉明空间取得最小总距离的访问顺序。根据RLE算法，保证相邻两行之间的距离之和最小就能够保证高效的压缩。这实际上相当于寻找一种重排方案，使得重排后的数据从第一行一直遍历到最后一行，每次计算相邻两行的距离，最后累加值最小，这显然就是一个TSP问题。但是由于TSP问题是一个NP困难问题，PowerDrill在实际生产环境中采用了一个比较简单的启发式方法来进行数据的重排，即对数据分块时选定的那几个域按照字典序进行排序来得到重排的结果。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/92_1.jpg)

图2-52 字符串的汉明距离

#### 2.8.4 性能分析与对比

PowerDrill在Google内部已经使用了一段时间，令我们比较关注的是两组数字：

（1）在查询过程中，平均92.41%的数据被略去，5.02%的数据会直接被缓存命中，一般仅须扫描2.66%的数据即可得到查询结果。这说明PowerDrill的数据分块策略是比较成功的。

（2）虽然PowerDrill的设计目标是将尽可能多的数据加载进内存，使得查询能够直接利用内存中的数据完成，但实际使用过程中不可避免地会有查询需要访问磁盘。根据Google自己的统计，超过70%的查询是不需要从磁盘访问任何数据的，这些查询的平均访问延迟大约是25秒。96.5%的查询需要访问的磁盘量不超过1 GB。图2-53[20]是查询延迟和访问磁盘数据量的关系，其中横轴表示访问数据量（GB），纵轴表示延迟时间（s）。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/93_1.jpg)

图2-53 查询延迟和访问磁盘数据量的关系

由于均采用列存储，因此PowerDrill不可避免地被人们拿来和Dremel做对比，但实际上除了都采用列存储之外，二者并没有什么特别共同的地方，相反却有不少区别。

（1）两者的设计目标不同，Dremel用来处理非常大量的数据集（指数据集的数量和每个数据集的规模都大），而PowerDrill设计用来分析少量的核心数据集（指每个数据集的规模大，但数据集的数量不多）。

（2）基本设计理念路不同，主要有：

① Dremel处理的数据来自外存，PowerDrill处理的数据尽可能地存于内存。

② Dremel未进行数据分区，分析时要扫描所有需要的列；PowerDrill使用了组合范围分区，分析时可以跳过很多不需要的分区。

③ Dremel数据通常不需要加载，增加数据很方便；PowerDrill数据需要加载，增加数据相对不便。

总的来看，PowerDrill涉及的技术手段并无特别新颖之处，大都是对已有技术手段直接采用或进行改进。

### 2.9 Google应用程序引擎

如果说Amazon给开发人员配置了一台可以在上面安装许多软件的虚拟机的话（参见第3章），Google App Engine[22]可以说是给开发人员提供了一个基于Python语言的Django框架。由于Google App Engine与Google自身的操作环境联系比较紧密，涉及底层的操作很少，用户比较容易上手。并且Python语言相对而言简单易学，开发人员可以很容易地开发出自己的程序。但是Google App Engine简单方便的同时，却在提供的解决方案上有着自己的局限性。

#### 2.9.1 Google App Engine简介

Google公司发展迅速，不断推出自己的新产品，比如Google搜索、Google Maps、Google Earth、Google AdSense、Google Reader等。在推出自己产品的同时，Google倾力打造了一个平台，来集成自己的服务并供开发者使用，这就是Google App Engine平台。

简单地说，Google App Engine是一个由Python应用服务器群、Bigtable数据库及GFS数据存储服务组成的平台，它能为开发者提供一体化的可自动升级的在线应用服务。

从云计算平台的分类来看，Amazon提供的是IaaS平台，而Google提供的Google App Engine是一个PaaS平台，用户可以在上面开发应用软件，并在Google的基础设施上运行此软件。其定位是易于实施和扩展，无须服务器维护。

Google App Engine可以让开发人员在Google的基础架构上运行网络应用程序。在Google App Engine之上易构建和维护应用程序，并且应用程序可根据访问量和数据存储需要的增长轻松进行扩展。使用Google App Engine，开发人员将不再需要维护服务器，只需要上传应用程序，它便可立即为用户提供服务。

在Google App Engine中，用户可以使用appspot.com域上的免费域名为应用程序提供服务，也可以使用Google企业应用套件从自己的域为它提供服务。开发人员可以与全世界的人共享自己的应用程序，也可以限制为只有自己组织内的成员可以访问。

除此之外，还可以免费使用Google App Engine。注册一个免费账户即可开发和发布应用程序，而且不需要承担任何费用和责任。免费账户可以使用多达500MB的持久存储空间，以及可支持每月约500万页面浏览量的超大CPU和带宽。

Google App Engine作为一个开发平台，有其自身的特点。

Google App Engine的整体架构如图2-54[23]所示。Google App Engine的架构可以分成四部分：前端和静态文件负责将请求转发给应用服务器并进行负载均衡和静态文件的传输；应用服务器则能同时运行多个应用的运行时（Runtime）；服务器群提供了一些服务，主要有Memcache、Images、URLfetch、E-mail和Data Store等；Google App Engine还有一个应用管理节点，主要负责应用的启停和计费。

关于Google App Engine的一些基本概念，比如应用程序环境、沙盒、Python运行时环境、数据库、Google账户、App Engine服务、开发流程、配额和限制等，总体而言，每个开发程序都将涉及这些概念。每个开发程序有自身的应用程序环境（这个环境由Google App Engine提供），该环境对应用程序提供了一些基本的支持，使应用程序可以在Google App Engine上正常运行。除此之外，Google App Engine为每个应用程序提供了一个安全运行环境（沙盒），该沙盒可以保证每个应用程序能够安全地隔离运行。现阶段，Google App Engine支持Java和Python语言，通过Google App Engine 的 Java 运行时环境，可以使用标准 Java 技术构建应用程序。开发程序时还可能要使用到Python运行时环境，该环境包括Python运行库等模块，并且Google App Engine还提供了一个由Python语言编写的网络应用程序框架Webapp。Google App Engine上开发的应用程序使用的是Data Store数据库，该数据库不同于日常使用的Oracle、SQL Server等数据库，它是一个分布式存储数据库，可以随着应用程序访问量的增加而增加。使用Google App Engine开发应用程序必须拥有一个Google账户，有了该账户之后才可以在Google App Engine上运行开发的程序。为了简化开发流程，Google App Engine提供了一些服务，这些服务统称为App Engine服务，使用Google App Engine开发应用程序必须遵守一定的开发流程。Google App Engine为每个Google账户用户提供了一些免费的空间与流量支持，但是免费的空间和流量有一定的配额和限制。

通过对这些概念的了解，可深入理解Google App Engine。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/95_1.jpg)

图2-54 Google App Engine的整体架构

#### 2.9.2 应用程序环境

Google App Engine有着自身的应用程序环境，这个应用程序环境包括以下特性。

（1）动态网络服务功能。能够完全支持常用的网络技术。

（2）具有持久存储的空间。在这个空间里平台可以支持一些基本操作，如查询、分类和事务的操作。

（3）具有自主平衡网络和系统的负载、自动进行扩展的功能。

（4）可以对用户的身份进行验证，并且支持使用Google账户发送邮件。

（5）有一个功能完整的本地开发环境，可以在自身的计算机上模拟Google App Engine环境。

（6）支持在指定时间或定期触发事件的计划任务。

基于这样的环境支持，Google App Engine可以在负载很重和数据量极大的情况下轻松构建安全运行的应用程序。

最开始Google App Engine只支持Python开发语言，现阶段开始支持Java语言。本书案例中，Google App Engine应用程序使用Python编程语言实现。该运行时环境包括完整的Python语言和绝大多数的Python标准库。在Python运行时环境中使用的是Python 2.5.2版本。这里先详细介绍一下Python运行时环境。

Python运行时环境包括Python标准库，开发人员可以调用库中的方法来实现程序功能，但是不能使用沙盒限制的库方法。这些受限制的库方法包括尝试打开套接字、对文件进行写入操作等。为了便于编程，Google App Engine设计人员将一些模块禁用了，被禁用的这些模块的主要功能是不受运行时环境的标准库支持的，因而，开发者在导入这些模块的代码时程序将给出错误提示。

在Python运行时环境中，应用程序只能以Python语言编写，扩展代码中若有C语言，则应用程序将不受系统支持。Python环境为开发平台中的数据库、Google账户、网址抓取和电子邮件服务等提供了丰富的Python API。此外，Google App Engine还提供了一个简单的Python网络应用程序框架，这个框架称为Webapp。借助于这个框架，开发人员可以轻松构建自己的应用程序。为了方便开发，Google App Engine还包括了Django网络应用程序框架，在开发过程中，可以将Django与Google App Engine配合使用。

沙盒是Google App Engine虚拟出的一个环境，类似于PC所使用的虚拟机。在这个环境中，用户可以开发使用自己的应用程序，沙盒将用户应用程序隔离在自身的安全可靠的环境中，该环境和网络服务器的硬件、系统及物理位置完全无关，并且沙盒仅提供对基础操作系统的有限访问权限。

沙盒还可以对用户进行如下限制。

（1）用户的应用程序只能通过Google App Engine提供的网址抓取API和电子邮件服务API来访问互联网中其他的计算机，并且其他计算机如请求与该应用程序相连接，只能在标准接口上通过HTTP或HTTPS进行。

（2）应用程序无法对Google App Engine的文件系统进行写入操作，只能读取应用程序代码上的文件，并且该应用程序必须使用Google App Engine的Data Store数据库来存储应用程序运行期间持续存在的数据。

（3）应用程序只有在响应网络请求时才运行，并且这个响应时间必须极短，在几秒之内必须完成。与此同时，请求处理的程序不能在自己的响应发送后产生子进程或执行代码。

简言之，沙盒给开发人员提供了一个虚拟的环境，这个环境使应用程序与其他开发者开发使用的程序相隔离，从而保证每个使用者可以安全地开发自己的应用程序。

开发人员开发程序必须使用Google App Engine SDK，即Google App Engine软件开发套件。可以先下载这个套件到自己的本地计算机上，然后进行开发和运行。使用SDK时，可以在本地计算机上模拟包括所有Google App Engine服务的网络服务器应用程序，该SDK包括Google App Engine中的所有API和库。该网络服务器还可以模拟沙盒环境，这些沙盒环境用来检查是否存在禁用的模块被导入的情况，以及对不允许访问的系统资源的尝试访问等情况的发生。

Google App Engine SDK完全使用Python实现，这个开发套件可以在装有Python 2.5的任何平台上面运行，包括Windows、Mac OS X和Linux等，开发人员可以在Python网站上获得适合自己系统的Python。

该开发套件还包括将应用程序上传到Google App Engine之上的工具。用户创建自己应用程序的代码、静态文件和配置文件之后，就可以运行这个工具将数据上传到平台上面。在上传过程中，该工具还将提示开发者输入Google账户和电子邮件地址及密码等信息。

系统中有一个管理控制台，这个管理控制台有一个网络接口，用于管理在Google App Engine上运行的应用程序。开发人员可以使用管理控制台来创建应用程序、配置域名、更改应用程序当前的版本、检查访问权限和错误日志以及浏览应用程序数据库等。

#### 2.9.3 Google App Engine服务

Google App Engine提供了多种服务。这些服务可以帮助开发人员在管理应用程序的同时执行常规操作，可以通过以下API来使用Google App Engine提供的服务。

**1. 图像操作API：**

开发的应用程序可以使用Google App Engine提供的图像操作API对图像进行操作，使用该API可以对JPEG和PNG格式的图像进行缩放、裁剪、旋转和翻转等操作。

1）Image类

Image类来自google.appengine.api.images模块，该类可以用来封装图像信息及转换该图像，转换时可以使用execute_transforms（）方法；可以使用classImage（image_data）来构造函数，参数image_data表示字节字符串（str）格式的图像数据；可以采用PNG、JPEG、TIFF或ICO等格式对图像数据进行编码。

Image类中主要有如下实例方法。

（1）resize（width=0，height=0）：该方法用来缩放图像，可以将图像缩小或放大到参数指定的宽度或者高度。参数width和height都以像素数量来表示，并且必须是int型或long型。

（2）crop（left_x，top_y，right_x，bottom_y）：该方法可以将图像裁剪到指定边界框的大小，并且裁剪后以相同的格式返回转换的图像。参数left_x表示边界框的左边界，top_y表示边界框的上边界，right_x表示边界框的右边界，bottom_y表示边界框的下边界。以上四个参数均采用指定为float类型值的从0.0到1.0的图像宽度的比例（其中float值包括了0.0 和1.0）。

（3）rotate（image_data，degrees，output_encoding=images.PNG）：该方法用来旋转图像。参数degrees表示图像旋转的量，采用的形式是度数，且这个度数必须是90°的倍数，数据格式必须为int型或long型，使用该函数对图像进行旋转是沿顺时针方向执行的。image_data是指要旋转的图像，是JPEG、GIF、BMP、TIFF或者ICO等格式的字节字符串（str）。output_encoding指转换的图像所需的格式，可以是images.PNG或images.JPEG格式，默认的格式是images.PNG格式。

（4）horizontal_flip（image_data，output_encoding=images.PNG）：该函数表示对图像进行水平翻转。参数image_data表示要翻转的图像是JPEG、PNG、TIFF或ICO格式的字节字符串（str）。output_encoding参数表示要转换的图像所需要的格式，可以是images.PNG或是images.JPEG，默认的格式是images.PNG格式。

（5）vertical_flip（image_data，output_encoding=images.PNG）：该函数表示垂直地翻转图像，并且转换后的图像与以前的格式一样。

2）exception类

google.appengine.api.images 包主要为用户提供了以下exception类。

（1）exception Error（）：这是该包中所有异常的基类。

（2）exception TransformationError（）：表示尝试转换图像时发生错误。

（3）exception BadRequestError（）：表示转换参数无效。

**2. 邮件API：**

Google App Engine为开发的应用程序提供了电子邮件服务。邮件API为用户提供了两种方式来发送电子邮件，分别是mail.send_mail（）函数和EmailMessage类。发送电子邮件时可以发送附件，为了安全考虑，用户发送的附件必须是所允许的文件类型。

1）允许的附件类型

允许作为电子邮件附件的MIME类型以及相对应的文件扩展名主要有：图像格式包括BMP、GIF、JPEG、JPG、JPE、PNG、TIFF、TIF、WBMP，文本格式包括CSS、CSV、HTM、HTML、TEXT、TXT、ASC、DIFF、POT，应用程序格式包括PDF、RSS。

2）EmailMessage类

邮件API中的EmailMessage类由google.appengine.api.mail包提供。EmailMessage 实例代表那些要使用Google App Engine邮件服务来发送的电子邮件，电子邮件中有一组字段，这组字段可以使用构造函数进行初始化。

（1）构造函数。在构造函数classEmailMessage（**kw）中，邮件的字段可以使用传递到构造函数的关键字参数进行初始化，并且字段还可以在构造之后对实例的属性进行设置，也可以通过initialize（）方法设置。

（2）实例方法。check_initialized（）方法用来检查EmailMessage类是否已经进行了正确的初始化，以便对邮件进行发送。若邮件成功发送，则该方法不会返回错误，否则会抛出与其找到的第一个问题对应的错误。

initialize（**kw）方法只是对EmailMessage是否进行了正确的初始化进行判断。如果是则返回True，与check_initialized（）一样执行同样的操作，区别只是不抛出错误。

send（）方法用来发送电子邮件。

（3）函数。google.appengine.api.mail包为邮件API主要提供了以下函数。

① Is_email_valid（email_address）：如果参数email_address是有效的电子邮件地址，则函数返回True。该函数会执行与check_email_valid相同的检查，但是不会抛出异常。

② send_mail（sender，to，subject，body，**kw）：创建并且发送一封电子邮件。sender、to、subject和body参数是邮件必填的字段。其他的字段也可以指定为关键字参数。

（4）异常。google.appengine.api.mail 包为邮件API主要提供了以下 exception 类。① exception Error（）：该包中所有异常的基类。

② exception BadRequestError（）：邮件服务以无效为理由拒绝EmailMessage。

③ exception InvalidEmailError（）：表示该电子邮件的地址无效。电子邮件地址字段仅接受有效的电子邮件地址，例如 sender 或 to。

**3. Memcache API：**

高性能的网络应用程序一般在运行之前需要使用分布式内存数据缓存（Memcache），或用分布式内存数据缓存来代替某些任务的稳定持久存储，Google App Engine为用户提供了这样一个高性能的内存键值缓存，可以使用应用程序的实例来访问这个缓存。Memcache适合存储永久性功能和事务性功能的数据，例如，可以将临时数据或数据库数据复制到缓存以进行高速访问。

Memcache API提供了一个基于类的接口，以便和其他Memcache API相兼容。这里Client类由google.appengine.api.memcache包提供。

1）构造函数

class Client（）产生与Memcache服务通信的客户端。

2）实例方法

构造的Client 实例主要有以下几种方法。

（1）set（key，value，time=0，min_compress_len=0）：该方法用来设置键的值，与先前缓存中的内容无关。其中参数key表示要设置的键，key可以是字符串或（哈希值，字符串）格式的元组；参数value表示要设置的值；参数time指可选的过期时间，可以是相对当前时间的秒数（最多1个月），也可以是绝对UNIX时间戳的时间；min_compress_len是为了兼容性而忽略的选项。

（2）get（key）：该方法用来在Memcache中查找一个键。参数key指明要在Memcache中查找的键，key可以是字符串或（哈希值，字符串）格式的元组。如果在Memcache中找到键，则返回值为该键的值，否则返回None。

（3）delete（key，seconds=0）：该方法用来从 Memcache 删除键。参数key指要删除的键，可以是字符串或（哈希值，字符串）格式的元组，参数seconds指定删除的项目对添加操作锁定的可选秒数，值可以是从当前时间开始的增量，也可以是绝对UNIX时间戳时间，默认情况下值为0。

（4）add（key，value，time=0，min_compress_len=0）：该方法用来设置值，但是只在项目没有处于Memcache时设置。参数key指明要设置的键，它可以是字符串或（哈希值，字符串）格式的元组；参数value是指要设置的值；参数time指明可选的过期时间，可以是相对当前时间的秒数，也可以是绝对UNIX时间戳时间；参数min_compress_len是为了兼容性而忽略的选项。

（5）replace（key，value，time=0，min_compress_len=0）：该方法用来替换键的值。参数key指要设置的键，Key可以是字符串或（哈希值，字符串）格式的元组；参数value指明要设置的值；参数time是指可选的过期时间，可以是相对当前时间的秒数，也可以是绝对UNIX时间戳时间；参数min_compress_len是为了兼容性而忽略的选项。

（6）incr（key，delta=1）：该方法可以自动增加键的值。在内部，值是无符号64bit整数，同时Memcache不会检查64bit溢出，如果值过大则会换行。这里的键必须已存在于缓存中才能增加值。初始化计数器时可以使用set（）进行初始值的设置。参数key是指要增加的键，key可以是字符串或（哈希值，字符串）格式的元组；参数delta值作为键的增加量的非负整数值（int型或long型），默认值为1。

（7）decr（key，delta=1）：该方法可以自动减少键的值。内部而言，值是无符号的64bit数，并且Memcache不检查64bit溢出，若值过大则会换行。初始化计数器时可以使用set（）进行初始值设置。参数key指要减少的键，key可以是字符串或（哈希值，字符串）格式的元组；参数delta是键的减少量的非负整数值（int型或long型），默认值为1。

（8）flush_all（）：该方法用来删除Memcache中的所有内容。若成功则返回True，若是RPC或服务器错误，则返回False。

（9）get_stats（）：该方法可获取该应用程序的Memcache统计信息。函数的返回值是将统计信息名称映射到相关值的参照表。

**4. 用户API：**

Google App Engine的功能和账号是集成的，因此应用程序可以让用户使用他们自身的Google账户登录。

1）User对象

用户API主要是通过User类来实现其功能的，每个User类的对象代表着一个用户。User对象是唯一的且可比较，若两个对象相同，则这两个对象代表着同一个用户。开发的应用程序可通过调用users.get_current_user（）函数来访问当前用户的User对象，也可以利用电子邮件地址来构造User对象。

2）登录网址

用户API提供了函数来构建到Google账户的网址，这样Google账户允许用户登录或退出，并重新定向到用户的应用程序。登录或退出目标网址可以使用users.create_login_url（）和users.create_logout_url（）。

3）User类

User类的一个对象代表具有Google账户的一个用户，User类是由google.appengine.api.users模块提供的。

（1）构造函数。classUser（email=None）这个函数代表具有Google账户的用户。函数中参数email表示用户的电子邮件地址，默认为当前用户。若系统没有指定电子邮件地址，并且当前用户没有登录，那么系统将抛出 UserNotFoundError错误。

系统在创建Use 对象时，不检查这个电子邮件地址是否有效。若该User对象的邮件地址不是有效的，则该User对象仍然可能存储在数据库中，但是不会与真正的用户相匹配。

（2）实例方法。User实例主要提供以下方法。

① nickname（）：用来返回用户的“昵称”。

② email（）：用于返回用户的电子邮件地址。

（3）函数。google.appengine.api.users 包主要提供以下函数。

① create_login_url（dest_url）：用于返回一个网址。当用户访问这个网址时，它将提示用户使用自己的Google账户登录，并将用户重新定向到指定的dest_url网址。其中dest_url 可以是完整的网址，也可以是相对于应用程序的域的路径。

② create_logout_url（dest_url）：用来返回一个网址。当用户访问这个网址时会注销这个用户，然后将用户重新定位到指定的dest_url网址。其中参数dest_url 可以是完整的网址，或者是相对于应用程序的域的路径。

③ get_current_user（）：若用户已登录，则该函数返回当前用户的User对象；若用户未登录，返回None。

（4）异常。google.appengine.api.users 包主要提供以下exception类。

① exceptionError（）：这个包中所有异常的基类。

② exceptionUserNotFoundError（）：若用户没有提供电子邮件地址，且当前用户未登录，则系统将由 User 构造函数抛出异常。

③ exception RedirectTooLongError（）：表示create_login_url（） 或create_logout_url（）函数的重定向网址的长度超过了所允许的最大长度。

**5. 数据库API：**

Google App Engine提供了一个强大的分布式数据存储服务。该服务包含查询引擎、事务功能等功能，并且该数据库规模可以随着访问量的上升而扩大。Google App Engine数据库和传统的关系数据库不同，该数据库中的数据对象有一个类和一组属性。数据库中的查询可以检索按照属性值过滤的实体，也可以检索按照分类指定种类的实体，其中属性值可以是任何一种受系统数据库支持的属性值类型。

Google App Engine的数据库使用了简单的API来为用户提供查询引擎和事务存储服务，并且这些服务都运行在Google的可扩展结构上。在Google App Engine中，Python接口包含了数据建模API和类似于SQL的一种查询语言（称为GQL）。通过这些API和GQL查询语言，可以极大地方便用户开发可扩展数据库的应用程序。

Google App Engine的数据库API拥有一个用于定义数据模型的机制。这里Model用来描述实体的类型（包括其属性的类型和配置）。数据库 API 提供两种查询接口：查询对象接口和GQL查询语言。查询的结果以Model类的实例形式返回实体，这些Model类可以被修改并放到数据库中。

1）Model类

Model类是数据模型定义的超类，由google.appengine.ext.db包提供。

Model 类的构造函数定义如下。

class Model（parent=None，key_name=None，** kw）。其中参数parent用来作为新实体的父实体的Mode 实例或key实例；参数key_name是新实体的名称，并且key_name 的值不得以数字开头，也不能采用“__ * __”的形式，存储为Unicode字符串，str值转换为ASCII文本；参数“**kw”表示实例的属性的初始值，作为关键字参数。

（1）类方法。Model类主要提供以下类方法。

① Model.get（keys）：用来获取指定Key对象的Model实例，键值必须代表Model类的实例。若程序提供的键类型不符合，则系统抛出KindError。参数keys是指Key对象或Key对象的列表，还可以是Key对象的字符串版本或字符串列表。

② Model.all（）：返回代表与该Model对应类型的所有实体的Query对象。在执行Query对象上的方法之前，可以对查询进行过滤和排序。

③ Model.gql（query_string，*args，**kwds）：用来对该Model的实例执行GQL查询。其中参数query_string指明GQL查询中“SELECT*FROM model”后的部分；参数“*args”用于位置参数绑定，类似于GqlQuery构造函数；参数“**kwds”表示关键字参数绑定，类似于GqlQuery构造函数。

（2）实例方法。Model实例主要有以下方法。

① key（）：返回该 Model 实例的数据库Key。在put（）入数据库之前，Model实例没有键。在实例拥有键之前调用key（）会抛出NotSavedError错误。

② put（）：将Model实例存储在数据库中。如果Model实例是新创建的并且之前从未存储过，则该方法会在数据库中创建新的数据实体，否则，该方法会用当前属性值更新数据实体。该方法会返回存储的实体的Key。

③ delete（）：用来从数据库中删除Model实例。如果实例从未被put（）到数据库，则删除不会起任何作用。

2）Property类

Property类也是一个超类，用来对数据模型的属性进行定义。它可以定义属性值的类型、值的验证方式以及在数据库中的存储方式等，Property由google.appengine.ext.db包提供。

（1）类构造函数。Property 基类的构造函数定义如下。

classProperty（verbose_name=None，name=None，default=None，required=False，validator=None，choices=None）。这是Model属性定义的超类。其中参数verbose_name表示用户友好的属性名称，属性构造函数的第一个参数必须始终是这个参数。参数name表示的是属性的存储名称，默认情况下，该名称表示属性的属性名称。参数default是指属性的默认值，若属性值从未被指定或值是None，则该属性值被视为默认值。参数required若是True，则属性值不能为None，Model实例必须要利用构造函数来初始化所有必需的属性，这样创建实例时才不会缺少值。参数validator表示分配属性值的时候应该调用以便用来验证值的函数，函数使用该属性值为唯一的参数。参数choices表示可接受的属性值的列表，如果设置了该参数，则不能给属性分配该列表以外的其他值。

（2）类属性。Property类下面的子类可以定义下面的属性。

data_type属性用来接受作为Python自有值的Python数据类型或类。

（3）实例方法。Property类实例主要具有以下方法。

① default_value（）：返回属性的默认值。其中基础的实施方案使用的是传递到构造函数default参数的值。

② validate（value）：表示属性的完整验证程序。若value值有效，则函数返回该值。程序的基础实施方案将会检查以下的内容：value值是否为None；若已经根据选择的内容对属性进行了设置，那么该值是否是一个有效的选择（choices参数）；若这个值存在，那么该值是否已经通过自定义的程序的验证（validator参数）。

③ empty（value）：若这个属性类型的value使用的是空值，那么该应用程序将返回True。

3）Query类

Query类是一个数据库查询的接口，程序可以使用对象和方法来准备这个查询。Query类由google.appengine.ext.db包提供。

（1）构造函数。Query类的构造函数的定义如下。

class Query（model_class）。函数主要表示使用对象和方法来准备数据查询的接口，由构造函数返回的Query实例表示的是对该类型的所有实体的查询。函数中参数model_class代表了查询的数据库实体类型的Model（或者Expando）类。

（2）实例方法。Query类主要有以下几种实例方法。

① filter（property_operator，value）：对属性的条件进行过滤，并加到该查询中，因而该查询只会返回满足所有条件的属性的实体。参数property_operator包含了属性名称和比较运算符的字符串，并且支持下列的比较运算符：<、<=、=、>=、>。参数value用来代表比较过程中所用的置于表达式右侧的值。

② order（property）：用来给结果添加排序，并且结果将根据首先添加的顺序进行排列。参数property表示的是一个字符串，是要为其排序的属性的名称，若要将排列顺序改为降序，可以在名称前加上一个连字符（-），若不加表示进行升序排列。

③ ancestor（ancestor）：对祖先条件进行过滤，并且将它加入查询，该查询只会返回以这个祖先条件为过滤器的那些实体。参数ancestor代表的是该祖先的Model实例或Key实例。

④ get（）：执行查询，然后返回第一个结果。若这个查询没有返回任何结果，则会返回None。

⑤ fetch（limit，offset=0）：执行查询，然后返回结果。参数limit是必须有的一个参数，表示要返回的结果的数量。若满足条件的结果数量不够，则返回的结果或许会少于limit个。参数offset表示要跳过的结果的数量，返回值是一个Model实例列表，也可能是一个空的列表。

⑥ count（limit）：返回这个查询所抓取的结果的数量。参数limit表示的是要计数的结果的最大数量。

4）GqlQuery类

GqlQuery类是一种使用了Google App Engine查询语言（即GQL）的数据库查询接口。GqlQuery类由google.appengine.ext.db包提供。

（1）构造函数。GqlQuery类的构造函数定义如下。

classGqlQuery（query_string，*args，**kwds），函数使用的是GQL的Query对象。参数query_string表示的是以“SELECT*FROM model-name”开头的完整GQL语句，参数“*args”表示位置参数绑定，参数“**kwds”表示关键字参数绑定。

（2）实例方法。GqlQuery实例主要有以下方法。

① bind（*args，**kwds）：重新绑定参数进行查询。新的查询将会在重新绑定参数之后第一次访问结束时执行。参数“*args”表示新位置参数绑定，参数“**kwds”表示新关键字参数绑定。

② get（）：执行查询，并且返回第一个结果。若查询之后没有返回结果就返回None。

③ fetch（limit，offset=0）：执行查询，然后返回结果。参数limit表达的是程序将要返回的结果的数量，是必需的参数。当结果数未知时，可以迭代地使用GqlQuery对象而不是使用fetch（）方法来从查询结果中获取每个结果。参数offset是指要跳过的结果的数量，返回的值是一个Model 实例列表，也可能是一个空的列表。

④ count（limit）：返回该查询抓取的结果的数量。count（）比那些通过常量系数来进行检索的速度要快一些。参数limit表示的是要计数的结果的最大值。

5）Key类

Key类的实例代表的是数据库实体唯一键，Key类由google.appengine.ext.db包提供。

（1）构造函数。classKey（encoded=None）。函数表示的是数据库对象的唯一键。用户可以通过将 Key 对象传递到str（）（或调用对象的__str__（）方法），也可以把键编码成字符串。参数encoded表示的是Key实例的str形式。

（2）类方法。Key 类提供以下类方法。

Key.from_path（*args，** kwds）方法表示从一个或者多个实体键的祖先路径来构建新的Key对象。这里的路径代表的是实体中父子之间关系的层次结构，每一个实体都由实体的类型以及其数字ID或键名来代表。参数“ * args”是从根实体到主题的路径，参数“** kwds”是关键字参数。

（3）实例方法。Key实例主要有以下方法。

① app（）：返回存储数据实体的应用程序的名称。

② kind（）：以字符串形式返回数据实体的类型。

③ id（）：以整数形式返回数据实体的数字ID，若实体没有数字ID，则函数返回None。

④ name（）：返回数据实体的名称，若实体没有名称则返回None。

（4）函数。google.appengine.ext.db包主要提供以下函数。

① get（keys）：用于获取任何 Model 的指定键的实体。其中参数keys表示的是Key对象或Key对象的列表。

② put（models）：将一个或多个Model实例放置到数据库中。参数models表示的是要存储的Model实例或Model实例的列表。

③ delete（models）：从数据库中删除一个或多个Model实例。参数models表示要删除的Model实例、实体的Key，或Model实例列表，也可以是实体的Key列表。

④ run_in_transaction（function，*args，** kwargs）：用于在一个事务中运行包含数据库更新的函数。若代码在事务处理过程之中抛出异常，则事务中进行的所有数据库更新都将回滚。参数function指的是要在数据库事务中运行的函数，参数“ *args”是指传递到函数的位置参数，参数“**kwargs”是指传递到函数的关键字参数。

**习题：**

1. Google云计算技术包括哪些内容？

2. 当前主流分布式文件系统有哪些？各有什么优缺点？

3. GFS采用了哪些容错措施来确保整个系统的可靠性？

4. MapReduce与传统的分布式程序设计相比有何优点？

5. Chubby的设计目标是什么？Paxos算法在Chubby中起什么作用？

6. 阐述Bigtable的数据模型和系统架构。

7. 分布式存储系统Megastore的核心技术是什么？

8. 大规模分布式系统的监控基础架构Dapper关键技术是什么？

9. 相比于行存储，列存储有哪些优点？

10. 为什么MapReduce不适合实时数据处理？

11. 简单阐述Dremel如何实现数据的无损表示。

12. PowerDrill能实现高效的数据处理，在存储部分主要依赖哪两方面的技术？

13. Google App Engine提供了哪些服务？

14. Google App Engine的沙盒对开发人员有哪些限制？

**参考文献：**

[1] Sanjay Ghemawat, Howard Gobioff, Shun-Tak Leung. The Google File System, Proceedings of 19th ACM Symposium on Operating Systems Principles,2003,20-43.

[2] Sun "Lustre Networking: High-Performance Features and Flexible Support for a Wide Array of Networks"

 <https://www.sun.com/offers/details/lustre_networking.xml>.

[3] Soltis, Steven R; Erickson, Grant M; Preslan, Kenneth W (1997), "The Global File System: A File System for Shared Disk Storage", IEEE Transactions on Parallel and Distributed Systems.

[4] Schmuck,Frank;Roger Haskin (January 2002)." GPFS: A Shared-Disk File System for Large Computing Clusters". Proceedings of the FAST' 02 Conference on File and Storage Technologies. Monterey,California,USA.

[5] Wikipedia.<http://zh.wikipedia.org/wiki/MapReduce>.

[6] John Darlington, Yi-ke Guo, Hing Wing To. Structured parallel programming: theory meets practice. Computing tomorrow: future research directions in computer science book contents Pages: 49-65.

[7] Jeffrey Dean,Sanjay Ghemawant. MapReduce: Simpli_ed Data Processing on Large Clusters.

[8] Chang F,Dean J,Ghemawat S, Hsieh WC, Wallach DA,Burrows M,Chandra T, Fikes A, Gruber RE. Bigtable: A distributed storage system for structured data. In: Proc. of the 7th USENIX Symp. on Operating Systems Design and Implementation. Berkeley: USENIX Association,2006.205-218.

[9] Ghemawat S,Gobioff H,Leung ST. The Google file system. In: Proc. of the 19th ACM Symp. on Operating Systems Principles. New York: ACM Press, 2003. 29-43.

[10] Burrows M. The chubby lock service for loosely-coupled distributed systems. In: Proc. of the 7th USENIX Symp. on Operating Systems Design and Implementation. Berkeley: USENIX Association,2006.335-350.

[11] 陈康，郑纬民.云计算：系统实例与研究现状。软件学报，2009，20（5）：1337-1348.

[12] BLOOM, B. H. Space/time trade-offs in hash coding with allowable errors. CACM 13,7 (1970),422-426.

[13] BURROWS,M. The Chubby lock service for loosely-coupled distributed systems. In Proc. of the 7th OSDI, Nov,2006.

[14] LAMPORT, L.Paxos made simple. ACM SIGACT News 32,4 (2001),18-25.

[15] Paxos算法.维基百科.<http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95>.

[16] T.D.Chandra, R.Griesemer, andJ.Redstone.Paxos made live: an engineering perspective. In PODC,2007.

[17] Jason Baker,Chris Bond,James C.Corbett, JJ Furman,Andrey Khorlin, James Larson, Jean-Michel Leon, Yawei Li,Alexander Lloyd, and Vadim Yushprakh. Megastore: Providing scalable,highly available storage for Interactive services. In Proc. CIDR,2011.

[18] Benjamin H.Sigelman,Luiz André Barroso,Mike Burrows,Pat Stephenson,Manoj Plakal,Donald Beaver,Saul Jaspan,Chandan Shanbhag.Dapper,a Large-Scale Distributed Systems Tracing Infrastructure.Google Technical Report,2010.

[19] Melnik S,Gubarev A,Long J J,et al.Dremel:interactive analysis of web-scale datasets.Proceedings of the VLDB Endowment,2010,3(1-2):330-339.

[20] Hall A,Bachmann O,Büssow R,et al.Processing a trillion cells per mouse click.Proceedings of the VLDB Endowment,2012,5(11):1436-1446.

[21] George L.HBase:the definitive guide.O'Reilly Media,Inc.,2011.

[22] <http://code.google.com/intl/zh-CN/appengine/docs/>.

[23] <http://q.sohu.com/forum/5/topic/21388686>.

## 第三章 Amazon云计算AWS

Amazon（亚马逊）凭借在电子商务中积累的大量基础性设施和各类先进技术，很早地进入了云计算领域，并在提供计算、存储等服务方面处于领先地位。在此基础上，Amazon还不断地进行技术创新，开发并提供了一系列新颖且实用的云计算服务，赢得了巨大的用户群体。这些云计算服务共同构成了Amazon的云计算服务平台Amazon Web Service（AWS）。目前，AWS提供的服务主要包括：弹性计算云EC2[13]、简单存储服务S3[15]、简单数据库服务Simple DB[16]、简单队列服务SQS[17]、弹性MapReduce服务[19]、内容推送服务CloudFront[21]、电子商务服务DevPay[24]和FPS[25]等。这些服务涵盖了云计算的各个方面，用户可以根据需要从中选取一个或多个云计算服务来构建自己的应用程序，并能够按需获取资源且具有很高的可扩展性及灵活性。
本章详细介绍AWS的基础存储架构Dynamo[1]及各项主要服务，重点剖析其中所涉及的核心概念、重要技术和基本原理。本章不介绍AWS中各类服务的具体使用方法，感兴趣的读者可以参考Amazon的相关技术文档。

### 3.1 基础存储架构Dynamo

当Web服务刚刚兴起时，各种平台大多采用关系型数据库进行数据存储。但由于Web数据中大部分为半结构化数据且数据量巨大，关系型数据库无法满足其存储要求。为此，很多服务商都设计并开发了自己的存储系统。其中，Amazon的Dynamo是非常具有代表性的一种存储架构，被作为状态管理组件用于AWS的很多系统中。2007年，Amazon将Dynamo以论文形式发表，引起了广泛的关注，并被作为其他云存储架构的基础和参照，例如最初由Facebook开发的开源分布式数据库Cassandra[39]。

#### 3.1.1 Dynamo概况

Amazon作为目前世界上最主要的电子商务提供商之一，其系统每天要接受全球数以百万计的服务请求。图3-1[1]展示了面向服务的Amazon平台基本架构。
为了保证其稳定性，Amazon的系统采用完全的分布式、去中心化的架构。其中，作为底层存储架构的Dynamo也同样采用了无中心的模式。
Dynamo只支持简单的键/值（key/value）方式的数据存储，不支持复杂的查询，适用于Amazon的购物车、S3等服务。Dynamo中存储的是数据值的原始形式，即按位存储，并不解析数据的具体内容，这也使得Dynamo几乎可以存储所有类型的数据。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/108_1.jpg)

图3-1 面向服务的Amazon平台架构

#### 3.1.2 Dynamo架构的主要架构

Dynamo在设计时被定位为一个基于分布式存储架构的，高可靠、高可用且具有良好容错性的系统。表3-1[1]列举了Dynamo设计时面临的主要问题及所采取的解决方案。
表3-1 Dynamo需要解决的主要问题及解决方案

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/108_2.jpg)

如图3-1所示，Dynamo中的存储节点呈无中心的环状分布。其中包含两个基本概念：preference list和coordinator[1]。preference list是存储与某个特定键值相对应的数据的节点列表，coordinator是执行一次读或写操作的节点。通常，coordinator是preference list上的第一个节点。
**1. 数据均衡分布的问题：**
Dynamo采用了分布式的数据存储架构，均衡的数据分布可以保证负载平衡和系统良好的扩展性。因此，如何在各个节点上数据的均衡性是影响Dynamo性能的关键问题。Dynamo中使用改进后的一致性哈希算法，并在此基础上进行数据备份，以提高系统的可用性。
1）一致性哈希算法
一致性哈希算法[2]是目前主流的分布式哈希表（Distributed Hash Table，DHT）协议之一，于1997年由麻省理工学院提出。一致性哈希算法通过修正简单哈希算法，解决了网络中的热点问题，使得DHT可以真正地应用于P2P环境中。一致性哈希算法的基本过程为：对于系统中的每个设备节点，为其分配一个随机的标记，这些标记可以构成一个哈希环。在存储数据时，计算出数据中键的哈希值，将其存放到哈希环顺时针方向上第一个标记大于或等于键的哈希值的设备节点上。图3-2展示了基于一致性哈希算法进行数据存储的示例。对于给定的数据对象，计算出其键的哈希值，顺时针方向找到的第一个设备节点为节点2，因此将该数据对象存放在节点2上。
一致性哈希算法除了能够保证哈希运算结果充分分散到整个环上外，还能保证在添加或删除设备节点时只会影响到其在哈希环中的前驱设备节点，而不会对其他设备节点产生影响。如图3-2和图3-3所示，当在节点2和节点3之间增加节点5之后，原来存储在节点3上的部分数据会被迁移到节点5中，但对节点1、节点2和节点4没有影响。同样，如果从图3-3中删除节点5，原来存储在节点5上的数据会迁移到节点3上，对节点1、节点2和节点4也没有影响。这表明一致性哈希算法可以大大降低在添加或删除节点时引起的节点间的数据传输开销。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/109_1.jpg)

图3-2 一致性哈希算法

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/109_2.jpg)

图3-3 添加节点示意图

2）改进的一致性哈希算法
一致性哈希算法在设备节点数量较少的情况下，有可能造成环上节点分布的不均匀；并且没有考虑哈希环上不同设备节点的性能差异。为了解决这些问题，Dynamo中引入了虚拟节点[1]的概念。每个虚拟节点都隶属于某一个实际的物理节点，一个物理节点根据其性能的差异被分为一个或多个虚拟节点。各个虚拟节点的能力基本相当，并随机分布在哈希环上。如图3-4所示，在存储数据时，数据对象先按照其键的哈希值被分配到某个虚拟节点上，并存储在该虚拟节点所对应的物理节点中。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/110_1.jpg)

图3-4 Dynamo中的虚拟节点和物理节点

为了进一步提高数据分布的均衡性，如图3-5所示，Dynamo将整个哈希环划分成Q等份，每个等份称为一个数据分区（Partition）[1]。假设系统中共有S个虚拟节点，且满足Q＞＞S，则每个虚拟节点负责的数据分区数为V=Q/S。在存储数据时，每个数据会被先分配到某个数据分区，再根据负责该数据分区的虚拟节点，最终确定其所存储的物理节点。采用数据分区的好处有两点。首先，在虚拟节点数量较少时，随机将数据分配到虚拟节点上可能会引起数据分布的不均衡。由于数据分区的数量远大于虚拟节点的数量，可以减小数据分布不均衡的可能性。其次，采用数据分区后，在添加或删除设备节点时，会引起较小的数据传输。以添加设备节点为例，为简化讨论，假设该设备节点包含N个虚拟节点。虽然在添加该设备节点时会使得每个原有虚拟节点负责的数据分区数量从Q/S变为Q/（S+N），但实际上每个原有的虚拟节点只需要将Q/S-Q/（S+N）个数据分区移到新的虚拟节点上，总的数据传输量为QN/（S+N）。因此，在添加节点的过程中，只是改变了少量数据分区所属的虚拟节点，而对大多数数据分区并不需要改变。这样就可以在很小的数据传输代价下，保证整个系统中数据分布的均衡性。需要注意的是，随着节点的增加，特别是S接近Q后，Dynamo的性能会急剧下降，因此需要选择好Q的取值。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/111_1.jpg)

图3-5 Dynamo节点划分方式示意图

**2. 数据备份：**
为了提高数据的可用性，Dynamo中在存储每个数据对象时，保存了其多个副本作为冗余备份。假设每个数据对象保存在系统中的副本数为N（通常为3），考虑到存在节点失效的情况，preference list中节点的个数大于N，并且为实际的物理节点。在Dynamo中，每个数据的副本备份存储在哈希环顺时针方向上该数据所在虚拟节点的后继节点中。如图3-5所示，某个数据对象的键为k，其数据存储在虚拟节点A中，则其数据副本将按顺时针方向存储在虚拟节点B、C上。数据备份在存储数据的同时进行，会使每次写操作的时延变长。因此，Dynamo中对写操作进行了优化，保证一个副本必须写入硬盘，其他副本只要写入节点的内存即返回写成功。这样即保证了副本的数量，又减少了时延。根据这个规定，每个虚拟节点上实际存储了分配给它以及分配它的前N-1个前驱虚拟节点的数据。Amazon中保证相邻的虚拟节点分别位于不同的区域，这样即便某个数据中心由于自然灾害或断电的原因整体瘫痪，仍然可以保证其他数据中心中保存有数据的备份。
Dynamo在产生N个数据副本时采用了参数可调的弱quorum（Sloppy quorum）的机制[1]。该机制中涉及三个参数W、R、N，其中W表示一次成功的写操作至少需要写入的副本数，R表示一次成功读操作须由服务器返回给用户的最小副本数，N表示每个数据存储的副本数。只要满足R+W>N的要求，Dynamo保证当存在故障的节点数量不超过1台时，用户至少可以获得一份最新的数据副本。通过配置R和W，可以调节系统的性能。如果应用要求较高的读效率，则可以设置R=1，W=N；如果要求较高的写效率，则可以设置R=N，W=1；如果希望在读/写效率中取得平衡，以N=3为例，则可以设置R=2，W=2。此外，通过配置R和W，还可以在系统的可用性和容错性之间取得平衡。例如，当N=3时，如果丢失一些最后的更新并不会造成太大影响，则可以R=1，W=1。
**3. 数据冲突问题：**
分布式系统架构中通常需要考虑三个因素：可靠性（Reliability）、可用性（Availability）和一致性（Consistency）。但这三者不能同时满足，最多只能实现其中的两个。Dynamo系统根据其业务特点，选择通过牺牲一致性来保证系统的可靠性和可用性。Dynamo中没有采用强一致性模型（Strong Consistency），而采用了最终一致性模型（Eventual Consistency）。后者不要求各个数据副本在更新过程中始终保持一致，只需要最终时刻所有数据副本能够保证一致性。由于Dynamo中可能出现同一个数据被多个节点同时更新的情况，且无法保证数据副本的更新顺序，这有可能会导致数据冲突。为了解决该问题，Dynamo中采用了向量时钟技术（Vector Clock）。
图3-6展示了向量时钟的一个示例。Dynamo中的向量时钟通过[node，counter]对来表示。其中，node表示操作节点；counter是其对应的计数器，初始值为0，节点每进行一次更新操作则计数器加1。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/112_1.jpg)

图3-6 向量时钟原理图

在图3-6中，节点Sx首先对数据对象D进行一次更新操作，产生第一个版本D1（[Sx，1]），其中操作节点为Sx，计数器的值为1。接着，Sx进行第二次操作，产生第二个版本D2（[Sx，2]），其中操作节点不变，计数器加1。之后，节点Sy和Sz分别同时对该数据对象进行更新操作，Sy将自身的信息加入时钟向量产生了新的版本D3（[Sx，2]，[Sy，1]）；同样，Sz产生了新的版本D4（[Sx，2]，[Sz，1]）。此时，对于同一个数据对象D，Dynamo系统中会存在两个相互冲突的版本。如果节点Sx需要再次更新数据对象D，它将会同时获得两个数据版本，并通过解决数据冲突来获得最终版本，如D5（[Sx，2]，[Sy，1]，[Sz，1]）。常用的解决冲突的方案有两种：一种是通过客户端由用户来解决，例如购物车应用；另一种是系统自动选择时间戳最近的版本，但由于集群内的各个节点并不能严格保证时钟同步，所以不能完全保证最终版本的准确性。需要注意的是，向量时钟的数量是有限制的，当超过限制时将会根据时间戳删除最早的向量时钟。
**4. 成员资格及错误检测:**
由于Dynamo采用了无中心的架构，每个成员节点都需要保存其他节点的路由信息，以保障系统中各个节点之间数据转发顺畅。但由于机器或人为的因素，Dynamo中添加或删除节点的情况时常发生。为了保证每个节点都能拥有最新的成员节点信息，Dynamo中采用了一种类似于Gossip（闲聊）协议[1]的技术，要求每个节点相隔固定时间（1秒）从其他节点中任意选择一个与之通信。
如果通信时连接成功，双方将交换各自保存的系统中节点的负载、路由等信息。在交换信息时，一个节点A先将保存的所有节点版本发送给对方节点B；节点B将接收到的节点版本与自身保存节点版本相比对，将比A中新的节点信息发送给A，同时告知A需要将哪些节点信息发给它；节点A收到节点B的回复后，更新自身的节点信息，并将B索要的节点信息发给B；节点B接受A发来的节点信息，并更新自身的节点信息。这样，节点A和节点B就完成了节点信息的互换，同时更新各自保存的节点信息。
Dynamo中还通过Gossip来实现错误检测。任何节点向其他节点发起通信后，如果对方没有回应，则认为对方节点失效，并选择别的节点进行通信。发起通信的节点会定期向失效节点发出消息，如果对方有回应，则可以重新建立通信。
为了避免新加入的节点之间不能及时发现其他节点的存在，Dynamo中设置了一些种子节点（Seed Node）。如图3-7所示，种子节点和所有的节点都有联系。当新节点加入时，它扮演一个中介的角色，使新加入节点之间互相感知。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/113_1.jpg)

图3-7 基于Gossip协议的成员检测机制
假如一新节点加入节点总数为N的系统，并以最优的方式进行传播（即每次通信的两个节点都是第一次交换新节点信息），那么将新节点信息传遍整个系统需要的时间复杂度为logn。如图3-8所示，自底向上每一层代表一次随机通信。第一层节点1将信息交换给节点2；第二层节点1和2同时开始随机选择其他节点交换信息，比如节点1向节点3发送信息，节点2向节点4发送信息；以此类推，直到N个节点全部传遍。整个过程形成一个倒的二叉树，树高为logn。显然，当N的值很大时，传播时间就会变得很长，因此，Dynamo中的节点数不能太多。根据Amazon的实际经验，当节点数N在数千时，Dynamo的效率是非常高的；但当节点数N增加到数万后，效率就会急剧下降。为此，Amazon采用了分层Dynamo结构来解决该问题[1，40]。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/114_1.jpg)

图3-8 基于Gossip协议的最优传遍路径

**5. 容错机制:**
Dynamo采用了廉价的服务器作为硬件设施，并将物理节点失效作为常态来处理。Dynamo的容错机制中包括了对临时故障和永久性故障的处理机制。
1）临时故障处理机制
Dynamo中如果某个节点由于机器假死等因素无法与其他节点通信，则会被其他节点认为失效。这种故障是临时性的，被认为失效的节点会在后期的闲聊中被发现并重新使用。为了处理临时失效的节点，Dynamo中采用了一种带有监听的数据回传机制（Hinted Handoff）[1]。假设数据副本的数量为N，当写入某个数据时，如果其preference list中前N个节点中某个节点失效，则会在preference list中第N+1个节点上的临时空间内记录需要在失效节点上写入的数据。同时，第N+1个节点需要记录失效节点的位置，并对失效的节点进行监测。一旦失效的节点重新可用，第N+1个节点会将保存的临时数据回传给该节点，并删除临时空间中的数据。图3-9展示了Dynamo中临时故障的处理机制。当虚拟节点A失效后，会将数据临时存放在节点D的临时空间中，并在节点A重新可用后，由节点D将数据回传给节点A。
2）永久性故障处理机制
在节点失效超过了设定时间后，如果没有发现节点可以重用，则Dynamo会认定该节点出现了永久性故障，例如磁盘损坏等。此时，Dynamo需要从其他数据副本进行数据同步。在同步过程中，为了保障数据传输的有效性，Dynamo采用Merkle哈希树技术[1]来加快检测和减少数据传输量。Merkle哈希树可以为二叉树或多叉树，其中每个叶子节点的值为单个数据文件的哈希值，非叶子节点的值为该节点所有子节点组合后的哈希值。当采用Merkle哈希树检测数据是否一致时，系统会先比较根节点的值，如果值相同则说明所有数据一致，否则需要继续比较，直到哈希值不同的叶子节点。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/115_1.jpg)

图3-9 Dynamo临时故障处理机制

如图3-10所示，Merkle哈希树A和B的根节点值不同。进一步比较其子节点，发现左子节点的值相同，表明左子节点所覆盖的所有叶节点对应的数据文件一致；而右子节点的值不同，则进一步比较其子节点，如此下去，直至发现原来在A中值为11的叶节点在B中变成了17，表明该叶节点对应的数据需要重新同步。通过对每个节点上每个区间的数据构建Merkle哈希树，Dynamo可以快速地进行数据比对，检测数据的一致性，并大大减少了需要传输的数据量，提高了系统效率。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/115_2.jpg)

图3-10 Merkle哈希树

### 3.2 弹性计算云EC2

弹性计算云服务（Elastic Compute Cloud，EC2）是AWS的重要组成部分，用于提供大小可调节的计算容量[13]。它为用户提供了许多非常有价值的特性，包括低成本、灵活性、安全性、易用性和容错性等[8]。借助Amazon EC2，用户可以在不需要硬件投入的情况下，快速开发和部署应用程序，并方便地配置和管理[10]。

#### 3.2.1 EC2的基本架构

图3-11展示了Amazon EC2的基本架构，主要包括了Amazon机器映象、实例、存储模块等组成部分，并能与S3等其他Amazon云计算服务结合使用。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/116_1.jpg)

图3-11 EC2的基本架构

**1. Amazon机器映象（AMI）:**
Amazon机器映像（Amazon Machine Image，AMI）是包含了操作系统、服务器程序、应用程序等软件配置的模板，可以用于启动不同实例，进而像传统的主机一样提供服务[13]。因此，当用户使用EC2服务去创建自己的应用程序时，首先需要构建或获取相应的AMI。Amazon为用户提供了四种获取AMI的途径。用户可以免费使用Amazon提供的公共AMI，或根据自身需要定制一个或多个私有AMI。此外，用户还可以向开发者付费购买AMI，或者使用其他开发者分享的共享AMI。构建好的AMI分为Amazon EBS支持和实例存储支持两类，所启动的实例的根设备分别为Amazon EBS卷和实例存储卷，后者依据Amazon S3中存储的模板而创建。
**2. 实例（Instance）:**
EC2中实例由AMI启动，可以像传统的主机一样提供服务。同一个AMI可以用于创建具有不同计算和存储能力的实例。目前，Amazon提供了多种不同类型的实例，分别在计算、GPU、内存、存储、网络、费用等方面进行了优化[43]。这些实例类型面向了不同的用户需求。例如，构建基因组分析等科学计算应用的用户可以选择计算优化型实例，构建数据仓储应用的用户可以选择存储优化型实例，而构建吞吐量很小的应用的用户可以选择费用很低的微型实例。此外，Amazon还允许用户在应用程序的需求发生变更时，对实例的类型进行调整，从而实现按需付费。
除了可以选择不同的实例类型外，Amazon EC2还为实例提供了许多附加功能，帮助用户更好地部署和管理应用程序[43]。例如，用户可以通过EBS优化来获得专用的吞吐量，借助增强型连网来提供网络传输性能，甚至使用专用硬件把自己的实例与其他用户实例进行物理隔离。
**3. 弹性块存储（EBS）**
除了少数实例类型外，每个实例自身携带一个存储模块（Instance Store），用于临时存储用户数据。但存储模块中的数据仅在实例的生命周期内存在；如果实例出现故障或被终止，数据将会丢失。因此，如果希望存储的数据时间与实例的生命周期无关，可以采用弹性块存储（Elastic Block Store，EBS）或S3（参见3.3节）进行数据存储。
EBS存储卷的设计与物理硬盘相似，其大小由用户设定，目前提供的容量从1GB到1TB不等。同一个实例可以连接多个EBS存储卷，每个EBS存储卷在同一时刻只能连接一个实例。但用户可以将EBS存储卷从所连接的实例断开，并连接到另一个实例上。EBS存储卷适用于数据需要细粒度地频繁访问并持久保存的情形，适合作为文件系统或数据库的主存储。
快照功能是EBS的特色功能之一，用于在S3中存储Amazon EBS卷的时间点副本。快照备份采用了增量备份的方式，仅保存上一次快照后更改的数据块；同样，删除快照时也仅删除该快照专有的数据块。快照包含了从拍摄时间起的所有信息，可以作为创建新的Amazon EBS的起点。

#### 3.2.2 EC2的关键技术

**1. 地理区域和可用区域：**
AWS中采用了两种区域[13]（Zone）：地理区域（Region Zone）和可用区域（Availability Zone）。其中，地理区域是按照实际的地理位置划分的。目前，Amazon在全世界共有10个地理区域，包括：美东（北佛吉尼亚）、美西（俄勒冈）、美西（北加利佛尼亚）、欧洲（爱尔兰）、亚太（新加坡）、亚太（东京）、亚太（悉尼）、南美（圣保罗）、美西服务政府的GovCloud区域和中国（北京）区域。而可用区域的划分则是根据是否有独立的供电系统和冷却系统等，这样某个可用区域的供电或冷却系统错误就不会影响到其他可用区域，通常将每个数据中心看做一个可用区域。
图3-12展示了两者之间的关系。EC2系统中包含多个地理区域，而每个地理区域中又包含多个可用区域。为了确保系统的稳定性，用户最好将自己的多个实例分布在不同的可用区域和地理区域中。这样在某个区域出现问题时可以用别的实例代替，最大限度地保证了用户利益。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/118_1.jpg)

图3-12 EC2中区域间关系

**2. EC2的通信机制：**
在EC2服务中，系统各模块之间及系统和外界之间的信息交互是通过IP地址进行的。EC2中的IP地址包括三大类：公共IP地址[13]（Public IP Address）、私有IP地址[13]（Private IP Address）及弹性IP地址[13]（Elastic IP Address）。EC2的实例一旦被创建就会动态地分配两个IP地址，即公共IP地址和私有IP地址。公共IP地址和私有IP地址之间通过网络地址转换（Network Address Translation，NAT）技术实现相互之间的转换。公共IP地址和特定的实例相对应，在某个实例终结或被弹性IP地址替代之前，公共IP地址会一直存在，实例通过这个公共IP地址和外界进行通信。私有IP地址也和某个特定的实例相对应，它由动态主机配置协议（DHCP）分配产生。
**3. 弹性负载平衡（Elastic Load Balancing）:**
弹性负载平衡功能允许EC2实例自动分发应用流量，从而保证工作负载不会超过现有能力，并且在一定程度上支持容错。弹性负载平衡功能可以识别出应用实例的状态，当一个应用运行不佳时，它会自动将流量路由到状态较好的实例资源上，直到前者恢复正常才会重新分配流量到其实例上。
**4. 监控服务（CloudWatch）:**
Amazon CloudWatch提供了AWS资源的可视化监测功能，包括EC2实例状态、资源利用率、需求状况、CPU利用率、磁盘读取、写入和网络流量等指标。使用CloudWatch时，用户只需要选择EC2实例，设定监视时间，CloudWatch就可以自动收集和存储监测数据。用户可以通过AWS服务管理控制台或命令行工具来维护和处理这些监测数据。
**5. 自动缩放（AutoScaling）：**
自动缩放可以按照用户自定义的条件，自动调整EC2的计算能力。在需求高峰期时，该功能可以确保EC2实例的处理能力无缝增大；在需求下降时，自动缩小EC2实例规模以降低成本。自动缩放功能特别适合周期性变化的应用程序，它由CloudWatch自动启动。
**6. 服务管理控制台（AWS Management Console）：**
服务管理控制台是一种基于Web的控制环境，可用于启动、管理EC2实例和提供各种管理工具和API接口。图3-13展示了各项技术通过互相配合来实现EC2的可扩展性和可靠性。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/119_1.jpg)

图3-13 各关键技术的配合工作图

#### 3.2.3 EC2的安全及容错机制

对网络传输中的数据进行控制的一个非常有效的办法是配置防火墙，但是传统的防火墙的规则是建立在IP地址、子网范围等基础之上的。EC2的特点之一就是允许用户随时更新实例状态，用户可以随时加入或删除实例，实例状态的动态变化方便了用户，但是却给防火墙的配置带来了麻烦。因此，EC2采用了安全组[13]（Security Group）技术。安全组是一组规则，用户利用这些规则来决定哪些网络流量会被实例接受，其他则全部拒绝。当用户的实例被创建时，如果没有指定安全组，则系统自动将该实例分配给一个默认组（Default Group）。默认组只接受组内成员的消息，拒绝其他任何消息。当一个组的规则改变后，改变的规则自动适用于组中所有的成员。
用户在访问EC2时需要使用SSH（Secure Shell）密钥对[13]（Key Pair）来登录服务。图3-14[13]展示了使用SSH访问EC2的流程。SSH是目前对网络上传输的数据进行加密的一种很可靠的协议，当用户创建一个密钥对时，密钥对的名称（Key Pair Name）和公钥（Public Key）会被存储在EC2中。在用户创建新的实例时，EC2会将它保存的信息复制一份放在实例的元数据（Metadata）中，然后用户使用自己保存的私钥（Private Key）就可以安全地登录EC2并使用相关服务。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/120_1.jpg)

图3-14 用户使用密钥对登录服务

在EC2的容错机制中，使用弹性IP地址是非常有效的一种方法。在创建实例时，系统会分配一个公共IP和一个私有IP给实例。用户是通过Internet利用公共IP地址访问实例的，但每次启动实例时这个公共IP地址都会发生变化。而DNS解析器中的IP地址和DNS名称的映射关系的更新大概需要24小时，为了解决这个问题，EC2引入了弹性IP地址的概念。弹性IP地址和用户账号绑定而不是和某个特定的实例绑定，这给系统的容错带来极大的方便，每个账号默认绑定5个弹性IP地址。当系统正在使用的实例出现故障时，用户只需要将弹性IP地址通过网络地址转换NAT转换为新实例所对应的私有IP地址，这样就将弹性IP地址与新的实例关联起来，访问服务时不会感觉到任何差异。这也是前面为什么建议在不同的区域建立实例的原因，当某一区域出现问题时可以直接用其他区域的实例来代替。因为所有区域的实例都出现故障的概率几乎为零，所以通过弹性IP地址改变映射关系总可以保证有实例可用。

### 3.3 简单存储服务S3

简单存储服务（Simple Storage Services，S3）构架在Dynamo之上，用于提供任意类型文件的临时或永久性存储。S3的总体设计目标是可靠、易用及低成本[9]。

#### 3.3.1 S3的基本概念和操作

图3-15展示了S3存储系统的基本结构，其中涉及两个基本概念：桶（Bucket）和对象（Object）。
**1. 桶：**
桶是用于存储对象的容器，其作用类似于文件夹[15]，但桶不可以被嵌套，即在桶中不能创建桶。目前，Amazon限制了每个用户创建桶的数量，但没有限制每个桶中对象的数量。桶的名称要求在整个Amazon S3的服务器中是全局唯一的，以避免在S3中数据共享时出现相互冲突的情况。在对桶命名时，建议采用符合DNS要求的命名规则，以便与CloudFront等其他AWS服务配合使用。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/121_1.jpg)

图3-15 S3的基本结构图

**2.对象：**
对象是S3的基本存储单元，主要由数据和元数据组成[15]。数据可以是任意类型，但大小会受到对象最大容量的限制。元数据是数据内容的附加描述信息，通过名称-值（name-value）集合的形式来定义，可以是系统默认的元数据（System Metadata）或用户指定的自定义元数据（User Metadata）。表3-2展示了S3中系统默认的元数据。
表3-2 S3的系统默认元数据

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/121_2.jpg)

每个对象在所在的桶中有唯一的键（key）[15]。通过将桶名和键相结合的方式，可以标识每个对象。键在对象创建后无法被更改，即重命名对于S3中的对象是无效的。
S3中对象的存储在默认情况下是不进行版本控制的。但S3中提供了版本控制的功能，用于存档早期版本的对象或者防止对象被误删。版本控制只能对于桶内所有的对象启用，而无法具体对某个对象启用版本控制。当对某个桶启用版本控制后，桶内会出现键相同但版本号不同的对象，此时对象需要通过“桶名+键+版本号”的形式来唯一标识。
**3. 基本操作：**
S3中支持对桶和对象的操作，主要包括：Get、Put、List、Delete和Head。表3-3[5]列出了五种操作的主要内容。

表3-3 S3的主要操作

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/122_1.jpg)

#### 3.3.2 S3的数据一致性模型

与其构建的基础Dynamo相同，S3中采用了最终一致性模型。图3-16展示了S3中数据一致性模型的示意图。在数据被充分传播到所有的存放节点之前，服务器返回给用户的仍是原数据，此时用户操作可能会出现如下几种情况[15]。

（1）写入一个新的对象并立即读取它，服务器可能返回“键不存在”。

（2）写入一个新的对象并立即列出桶中已有的对象，该对象可能不会出现在列表中。

（3）用新数据替换现有的对象并立即读取它，服务器可能会返回原有的数据。

（4）删除现有的对象并立即读取它，服务器可能会返回被删除的数据。

（5）删除现有的对象并立即列出桶中的所有对象，服务器可能会列出被删除的对象。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/122_2.jpg)

图3-16 S3的数据最终一致性模型示意图

#### 3.3.3 S3的安全措施

对于用户尤其是商业用户来说，系统的易用性是其考虑的一方面，但最终决定其是否使用S3服务的通常是S3的安全程度。S3向用户提供包括身份认证[15]（Authentication）和访问控制列表[15]（ACL）的双重安全机制。

**1. 身份认证：**

S3中使用基于HMAC-SHA1的数字签名方式来确定用户身份。HMAC-SHA1是一种安全的基于加密Hash函数和共享密钥的消息认证协议，它可以有效地防止数据在传输过程中被截获和篡改，维护了数据的完整性、可靠性和安全性。HMAC-SHA1消息认证机制的成功在于一个加密的Hash函数、一个加密的随机密钥和一个安全的密钥交换机制。在新用户注册时，Amazon会给每个用户分配一个Access Key ID和一个Secret Access Key。Access Key ID是一个20位的由字母和数字组成的串，Secret Access Key是一个40位的字符串。Access Key ID用来确定服务请求的发送者，而Secret Access Key则参与数字签名过程，用来证明用户是发送服务请求的账户的合法拥有者。数字签名具体实现过程如图3-17所示。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/123_1.jpg)

图3-17 S3数字签名具体实现过程

S3用户首先发出服务请求，系统会自动生成一个服务请求字符串。HMAC函数的主要功能是计算用户的服务请求字符串和Secret Access Key生成的数字签名，并将这个签名和服务请求字符串一起传给S3服务器。当服务器接收到信息后会从中分离出用户的Access Key ID，通过查询S3数据库得到用户的Secret Access Key。利用和上面相同的过程生成一个数字签名，然后和用户发送的数字签名做比对，相同则通过验证，反之拒绝。

**2. 访问控制列表：**

访问控制列表（Access Control List，ACL）是S3提供的可供用户自行定义的访问控制策略列表。很多时候用户希望将自己的文件和别人共享但又不想未经授权的用户进入，此时可以根据需要设置合适的访问控制列表。S3的访问控制策略（Access Control Policy，ACP）提供表3-4所列的五种访问权限。

表3-4 S3的访问控制策略

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/124_1.jpg)

需要注意的是桶和对象的ACL是各自独立的，对桶有某种访问权限不代表对桶中的对象也具有相同的权限，也就是说S3的ACL不具有继承性。

S3中有三大类型的授权用户，分别是所有者（Owner）、个人授权用户（User）和组授权用户（Group）[4]。

1）所有者

所有者是桶或对象的创建者，默认具是WRITE_ACP权限。所有者本身也要服从ACL，如果该所有者没有READ_ACP，则无法读取ACL。但是所有者可以通过覆写相应桶或对象的ACP获取想要的权限，从这个意义上来说，所有者默认就是最高权限拥有者。

2）个人授权用户

这包括两种授权方式。一种是通过电子邮件地址授权的用户（User by E-mail），即授权给和某个特定电子邮件地址绑定的AWS用户；另一种是通过用户ID进行授权（User by Canonical Representation），这种方式是直接授权给拥有某个特定AWS ID的用户。后一种方式比较麻烦，因为ID是一个不规则的字符串，用户在授权的过程中容易出错。值得注意的是通过电子邮件地址方式授权的方法最终还是在S3服务器内部转换成相应的用户ID进行授权。

3）组授权用户

同样包括两种方式。一种是AWS用户组（AWS User Group），它将授权分发给所有AWS账户拥有者；另一种是所有用户组（All User Group），这是一种有着很大潜在危险的授权方式，因为它允许匿名访问，所以不建议使用这种方式。

### 3.4 非关系型数据库服务SimpleDB和DynamoDB

与S3不同，非关系型数据库服务主要用于存储结构化的数据，并为这些数据提供查找、删除等基本的数据库功能。AWS中提供的非关系型数据库主要包括SimpleDB和DynamoDB。

#### 3.4.1 非关系型数据库与传统关系数据库的比较

非关系型数据库与传统的关系数据库相比，有以下区别。

**1. 数据模型：**

关系数据库对数据有严格的约束，包括数据之间的关系和数据的完整性。比如，关系数据库中某个属性的数据类型是确定的（如整数、字符串等），数据的范围是确定的（如0～1023等）。而在非关系型数据库的key-value存储形式中，key和value可以使用任意的数据类型。

**2. 数据处理：**

关系数据库满足CAP原则的C和A，在P方面很弱，所以导致其在可扩展性方面面临很多问题。非关系型数据库满足CAP原则的A和P，而在C方面比较弱，所以使得其无法满足ACID要求。

**3. 接口层：**

关系数据库都是以SQL语言对数据进行访问的，提供了强大的查询功能，并便于在各种关系数据库间移植。非关系型数据库对数据的操作大多通过API来实现，支持简单的查询功能，且由于不同数据库之间API的不同而造成移植性较差。

综上所述，关系数据库具有高一致性，在ACID方面很强，移植性很高；但在可扩展性方面能力较弱，只能通过提高服务器的配置来提高处理能力。非关系型数据库具有很高的可扩展性，可以通过增加服务器数量来不断提高存储规模，具有很好的并发处理能力；但由于缺乏数据一致性保证，所以处理事务性问题能力较弱，并且难以处理跨表、跨服务器的查询。

#### 3.4.2 SimpleDB

SimpleDB基本结构图如图3-18所示，包含了域、条目、属性、值等概念。

**1. 域（Domain）：**

域是用于存放具有一定关联关系的数据的容器，其中的数据以UTF-8编码的字符串形式存储[16]。每个用户账户中的域名必须是唯一的，且域名长度为3～255个字符。每个域中数据的大小具有一定的限制，因此通常将不同特征的数据放入不同的域中；对于Web数据等不易划分的数据，可以利用哈希函数将其散列到不同的域中。但域的划分也会为数据操作带来一些限制，例如SimpleDB中的数据库操作以域为基本单位，即所有的查询只能在一个域内进行，而不允许在域间进行操作。因此，是否划分域需要综合多种因素考虑。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/126_1.jpg)

图3-18 SimpleDB的基本结构图

**2. 条目（Item）：**

条目对应着一条记录，通过一系列属性来描述，即条目是属性的集合[16]。在每个域中，条目名必须是唯一的。与关系数据库不同，SimpleDB中不需要事先定义条目的模式，即条目由哪些属性来描述。这使得SimpleDB在操作上具有极大的灵活性，用户可以随时创建、删除以及修改条目的内容。

**3. 属性（Attribute）：**

属性是条目的特征，每个属性都用于对条目某方面特性进行概括性描述[16]。每个条目可以有多个属性。属性的操作相对自由，当某个条目需要新的属性时，只需要将该属性添加进去，而不用考虑该属性是否与域中的其他条目相关。

**4. 值（Value）：**

值用于描述某个条目在某个属性上的具体内容[16]。SimpleDB中允许多值属性，即一个条目的一个属性中可以有多个值。多值属性可以带来很大的便利性。例如，某类商品除颜色外其他参数完全一致，此时可以通过在颜色属性中存放多个值来使用一个条目表示该商品，而不需要像关系数据库中那样建立多条记录。图3-19显示了SimpleDB的树状组织方式，其中可以看出SimpleDB对多值属性的支持。

SimpleDB在使用过程中会有一些限制。例如，SimpleDB中每个属性值的大小不能超过1KB，这个限制使得SimpleDB存储的数据范围极其有限。常用的解决方案是将相对大的数据存储在S3中，在SimpleDB中只保存指向某个特定文件位置的指针。图3-20展示了SimpleDB与其他AWS组件综合使用的方式。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/127_1.jpg)

图3-19 SimpleDB的树状结构图

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/127_1.jpg)

图3-20 AWS服务的综合使用方式

由于SimpleDB中的数据都以字符串形式存储，使得查询操作时采取的是词典顺序，直接使用会出现一些问题。常见的问题和解决方式包括[16]在整数之前补零、对负整数集添加正向偏移量、采用ISO 8601格式对日期进行转换等。

此外，SimpleDB为了提高系统可用性采取了最终一致性数据模型，并为每次操作设定了一个阈值，当操作时间超过该阈值时，系统会向用户返回错误。

#### 3.4.3 DynamoDB

DynamoDB是Amazon在SimpleDB之后提供的非关系型数据库服务。它在设计上既延续了SimpleDB的优点，也解决了SimpleDB中存在的部分问题。

DynamoDB以表为基本单位，表中的条目同样不需要预先定义的模式，即每个条目可以具有不同的属性。与SimpleDB不同，DynamoDB中取消了对表中数据大小的限制，这使得用户可以将表的容量设置成任意需要的大小，并由系统自动分配到多个服务器上。在数据一致性方面，DynamoDB不再固定使用最终一致性数据模型，而是允许用户选择弱一致性或者强一致性。此外，DynamoDB还在硬件上进行了优化，采用固态硬盘作为支撑，并根据用户设定的读/写流量限制预设来确定数据分布的硬盘数量，以确保每次请求的性能都是高效且稳定的。

#### 3.4.4 SimpleDB和DynamoDB的比较

SimpleDB和DynamoDB都是Amazon提供的非关系型数据库服务。SimpleDB中限制了每张表的大小，更适合于小规模负载的工作；但SimpleDB会自动对所有属性进行索引，提供了更加强大的查询功能。与之相比，DynamoDB支持自动将数据和负载分布到多个服务器上，并未限制存储在单个表中数据量的大小，适用于较大规模负载的工作。

### 3.5 关系数据库服务RDS

非关系数据库在处理ACID类问题时存在一些先天性的不足，为了满足相关应用的需求，Amazon提供了相关数据库服务（Relational Database Service，RDS）。

#### 3.5.1 RDS的基本原理

Amazon RDS[42]将MySQL数据库移植到集群中，在一定的范围内解决了关系数据库的可扩展性问题。

MySQL集群方式采用了Share-Nothing架构，如图3-21所示。每台数据库服务器都是完全独立的计算机系统，通过网络相连，不共享任何资源。这是一个具有较高可扩展性的架构，当数据库处理能力不足时，可以通过增加服务器数量来提高处理能力，同时多个服务器也增加了数据库并发访问的能力。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/128_1.jpg)

图3-21 Share-Nothing架构

集群MySQL通过表单划分（Sharding）的方式将一张大表划分为若干个小表，分别存储在不同的数据库服务器上，这样就从逻辑上保证了数据库的可扩展性。但是表单的划分没有固定的方式，主要根据业务的需要进行针对性的划分，这就对数据库的管理人员提出了非常高的要求，如果划分得不科学，则查询经常会跨表单和服务器，性能就会严重下降。

集群MySQL通过主从备份和读副本技术提高可靠性和数据处理能力，如图3-22所示。Master A为主数据库，Master B为从数据库，组成主从备份。如果Master B检测到Master A瘫痪，则立刻接替Master A的位置，成为主服务器，并会重新创建一台从服务器。在数据库升级时，先对从数据库进行升级，然后将从数据库转变为主数据库，再对新的从数据库进行升级，这样就可以实现数据库的实时升级，保证业务的连续性；为了提高数据库的并发处理能力，集群MySQL设置了若干个读副本（Slave），顾名思义，读副本中的数据只能读，不能写，写操作只能由主数据库来完成。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/129_1.jpg)

图3-22 集群MySQL

#### 3.5.2 RDS的使用

从用户和开发者的角度来看，RDS和一个远程MySQL关系数据库没什么两样。Amazon将RDS中的MySQL服务器实例称做DB Instance，通过基于Web的API进行创建和管理，其余的操作可以通过标准的MySQL通信协议完成。创建DB Instance时需要指定一些属性来确定数据库实例的行为和能力，例如Class属性决定了所创建的DB Instance可用的内存和处理能力。Amazon以ECU（Elastic Compute Unit）作为其计算能力单位（1个ECU差不多相当于1个1.0GHz 2007 Xeon处理器），用户可以创建拥有1.7GB内存和1 ECU的小型DB Instance或拥有68GB内存和26 ECU的超级大型（Quadruple Extra Large）DB Instance。创建DB Instance时还需要定义可用的存储，存储范围为5GB到1024GB，RDS数据库中表最大可以达到1TB。

可以通过两种工具对RDS进行操作：命令行工具和兼容的MySQL客户端程序。命令行工具是Amazon提供的Java应用套装，负责处理DB Instance的管理，比如创建、参数调整、删除等，可以从Amazon网站下载。MySQL客户端是可以与MySQL服务器进行通信的应用程序，比如MySQL Administrator客户端。

### 3.6 简单队列服务SQS

要想构建一个灵活且可扩展的系统，低耦合度是很有必要的。因为只有系统各个组件之间的关联度尽可能低，才可以根据系统需要随时从系统中增加或者删除某些组件。但松散的耦合度也带来了组件之间的通信问题，如何实现安全、高效通信是设计一个低耦合度的分布式系统所必须考虑的问题。简单队列服务（Simple Queue Service，SQS）是Amazon为了解决其云计算平台之间不同组件的通信而专门设计开发的。

#### 3.6.1 SQS的基本模型

图3-23展示了SQS的基本模型。SQS由三个基本部分组成：系统组件（Component）、队列（Queue）和消息（Message）[17]。系统组件是SQS的服务对象，而SQS则是组件之间沟通的桥梁。组件在这里有双重角色，它既可以是消息的发送者，也可以是消息的接收者。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/130_1.jpg)

图3-23 SQS基本模型

在SQS中，消息和队列是最重要的两个概念。消息是发送者创建的具有一定格式的文本数据，接收对象可以是一个或多个组件。消息的大小是有限制的，目前Amazon规定每条消息不得超过8KB，但是消息的数量并未做限制。队列是存放消息的容器，类似于S3中的桶，队列的数目也是任意的，创建队列时用户必须给其指定一个在SQS账户内唯一的名称。当需要定位某个队列时采用URL的方式进行访问，URL是系统自动给创建的队列分配的。队列在传递消息时会尽可能实现“先进先出”，但无法保证先进入的消息一定会最先被投递给指定的接收者。不过SQS允许用户在消息中添加有关的序列数据，对于数据发送顺序要求比较高的用户可以在发送消息之前向其中加入相关信息。与队列相比，消息涉及的内容更多，需要考虑的问题更复杂。下面就消息的内容进行分析。

#### 3.6.2 SQS的消息

**1. 消息的格式：**

消息由以下四部分组成[11]。

（1）消息ID（Message ID）：由系统返回给用户，用来标识队列中的不同消息。

（2）接收句柄（Receipt Handle）：当从队列中接收消息时就会从消息那里得到一个接收句柄，这个句柄可以用来对消息进行删除等操作。

（3）消息体（Body）：消息的正文部分，需要注意的是消息存放的是文本数据并且不能是URL编码方式。

（4）消息体MD5摘要（MD5 of Body）：消息体字符串的MD5校验和。

**2. 消息取样：**

队列中的消息是被冗余存储的，同一个消息会存放在系统的多个服务器上。其目的是保证系统的高可用性，但这会给用户查询队列中的消息带来麻烦。为了解决该问题，SQS采用了基于加权随机分布（Weighted Random Distribution）的消息取样[17]，当用户发出查询队列中消息的命令后，系统在所有的服务器上使用基于加权随机分布算法随机地选出部分服务器，然后返回这些服务器上保存的所查询的队列消息副本。图3-24展示了SQS中消息取样的示意图。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/131_1.jpg)

图3-24 基于加权随机分布的消息取样

当消息数量较少时，SQS进行消息取样时可能会出现返回结果不准确的现象。如图3-24中，队列中实际包含了A、B、C、D四个消息，但返回给用户的只有A、B、C三个消息。但由于消息采样具有随机性，只要用户一直查询下去，总会查询到所有的消息。

**3. 消息的可见性超时值及生命周期：**

在SQS中，消息是否被接受是由用户自己确认的。当用户执行删除操作后，系统就会认为用户已经准确地接收到消息，将队列中的消息彻底删除。如果用户未接收到数据或接收到数据并没有执行删除操作，SQS将在队列中保留该消息。为了保证其他组件不会看见用户的消息，SQS会将该消息阻塞，也就相当于给消息加了一把锁。但是这把锁并不会一直锁住消息，因为系统保留消息的目的是给用户重传数据。为此SQS引入了一个可见性超时值[11，17]（Visibility Timeout）。可见性表明该消息可以被所有的组件查看，可见性超时值相当于一个计时器，在设定好的时间内，发给用户的消息对于其他所有的组件是不可见的。如果在计时器到时之前用户一直未执行删除操作，则SQS会将该消息的状态变成可见并给用户重传这个消息。可见性超时值可以由用户自行设置，用户可以根据自己操作的需要改变这个值，经验表明太长或太短的超时值都是不合适的。除了在计时器开始计时前改变设置，在计时器计时的过程中还可以对计时器进行两种操作：扩展（Extend）和终止（Terminate）。扩展操作就是将计时器按照新设定的值重新计时，终止就是将当前的计时过程终止，直接将消息由不可见变为可见。这两个操作的设置都只是临时性设置，不会被系统保存。消息从产生并发送至队列一直到其从队列中被删除的全过程称为消息的生命周期（Life Cycle）。如果消息在队列中存放的时间超过4天，SQS也会自动将其删除。图3-25是消息的可见性超时值和生命周期的示意图。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/132_1.jpg)

图3-25 消息可见性超时值及生命周期

### 3.7 内容推送服务CloudFront

CloudFront是基于Amazon云计算平台实现的内容分发网络（Content Delivery Network，CDN）。借助Amazon部署在世界各地的边缘节点，用户可以快速、高效地对由CloudFront提供服务的网站进行访问。

#### 3.7.1 CDN

传统的网络服务模式中，用户和内容提供商位于服务的两端，网络服务提供商将两者联系起来。在这种情况下，网络服务提供商仅仅起“桥梁”作用。图3-26是传统的用户访问网站的模式。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/133_1.jpg)

图3-26 传统用户访问网站的模式

用户在发出服务请求后，需要经过DNS服务器进行域名解析后得到所访问网站的真实IP，然后利用该IP访问网站。在这种模式中，世界各地的访问者都必须直接和网站服务器连接才可以访问相关内容，存在明显的缺陷。首先，网站服务器可以容纳的访问量是有限的，一旦发生突发事件，例如率先发布了某个重大消息或遭受到DDOS（分布式拒绝服务攻击），网站的流量会在短时间内急剧上升，带来的必然结果就是访问速度下降，更有甚者直接导致网站瘫痪。其次，这种模式中没有考虑访问者的地域问题，会造成与网站服务器处于不同地域的用户访问速度很慢甚至无法访问。最后，使用不同网络服务提供商服务的用户之间的互访速度也会受到限制。

为了解决这个弊端，CDN技术通过将网站内容发布到靠近用户的边缘节点，使不同地域的用户在访问相同网页时可以就近获取。这样既可以减轻源服务器的负担，也可以减少整个网络中流量分布不均的情况，进而改善整个网络性能。所谓的边缘节点是CDN服务提供商经过精心挑选的距离用户非常近的服务器节点，仅“一跳”（Single Hop）之遥。用户在访问时就无须再经过多个路由器，访问时间大大缩减。CDN是通过在现有的网络上增加一层网络架构来实现的。图3-27是使用了CDN后用户访问网站的基本流程图。

如图3-27所示，DNS在对域名进行解析时不再向用户返回网站服务器的IP，而是返回了由智能CDN负载均衡系统选定的某个边缘节点的IP。用户利用这个IP来访问边缘节点，然后该节点通过其内部DNS解析出真实的网站IP并发出请求来获取用户所需的页面，请求成功后向用户显示该页面并加以保存，以便用户再次访问时可以直接读取。这种访问模式的好处主要有以下几点。

（1）将网站的服务流量以比较均匀的方式分散到边缘节点中，减轻了网站源服务器的负担。

（2）由于边缘节点与访问者的地理位置较近，访问速度快。

（3）智能DNS负载均衡系统和各个边缘节点之间始终保持着通信联系，可以确保分配给用户的边缘节点始终可用且在允许的流量范围之内。

![img](https://raw.githubusercontent.com/les-chien/MyGallery/CloudComputing/pictures/134_1.jpg)

图3-27 加入CDN后用户访问流程

CDN的实现需要多种网络技术的支持，主要包括以下几种。

（1）负载均衡技术。负载均衡就是将流量均匀地分发到可以完成相同功能的若干个服务器上，在减轻服务器压力的同时也避免了单一网络通道的流量拥堵。

（2）分布式存储。在使用CDN服务之后，网站的内容不再是单一地被保存在源服务器上，多个边缘节点中都可能保存相应的副本。如何对网页内容进行分发以及如何保证边缘节点内容的时效性都是需要考虑的问题。

（3）缓存技术。缓存技术通过将内容存储在本地或者网络服务提供商的服务器上来改善用户的响应时间。

目前国内一些大的门户网站像新浪、网易等都已经采用了CDN，用户无论在何地访问这些网站可能都感觉不到网络拥堵的情况。但对一些经济实力有限的中小企业来说，资金的限制使他们无法大规模地使用普通的CDN服务，CloudFront的推出无疑给这些企业带来了便利。下面就简单介绍CloudFront这种云端的CDN。

#### 3.7.2 CloudFront

CloudFront正是通过Amazon设在全球的边缘节点来实现CDN的，但是较普通的CDN而言，它的优势无疑是巨大的。首先，CloudFront的收费方式和Amazon的其他云计算收费方式一样是按用户实际使用的服务来收费，这尤其适合那些资金缺乏的中小企业。其次，CloudFront的使用非常简单，只要配合S3再加上几个简单的设置就可以完成CDN的部署。下面先介绍CloudFront中的几个基本概念。

**1. 对象（Object）：**

对象[21]就是希望利用CloudFront进行分发的任意一个文件，但该文件首先须满足两个条件：一个是必须存储在S3中，另一个是它必须被设置为公开可读（Publicly Readable）。一般来说，通过CloudFront分发网页中的静态内容比较合适。

**2. 源服务器（Origin）：**
